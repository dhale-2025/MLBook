[
  {
    "objectID": "neural-network-Rversion.html",
    "href": "neural-network-Rversion.html",
    "title": "9  Neural Networks in R",
    "section": "",
    "text": "9.1 Introduction\nThis chapter introduces neural networks in R, progressing from feed-forward to recurrent (RNN) and convolutional (CNN) architectures.\nIt uses the Ames housing dataset, a sine wave sequence, and the MNIST digit dataset to demonstrate increasingly complex network structures.\nEach example illustrates a different ANN architecture.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks in R</span>"
    ]
  },
  {
    "objectID": "neural-network-Rversion.html#cross-validation-tuning",
    "href": "neural-network-Rversion.html#cross-validation-tuning",
    "title": "9  Neural Networks in R",
    "section": "11.1 1.2 Cross-Validation Tuning",
    "text": "11.1 1.2 Cross-Validation Tuning\nWe tune two important parameters: - size — number of neurons in the hidden layer\n- decay — regularization strength (prevents overfitting)\ncaret::train() automates this search using k-fold cross-validation.\n\n\nCode\ntune_grid &lt;- expand.grid(size = c(3,5,7,9), decay = c(0,0.1,0.5,1))\nctrl &lt;- trainControl(method = \"cv\", number = 5)\n\nset.seed(123)\nnn.tuned &lt;- train(\n  x = x_train, y = as.numeric(y_train_s),\n  method = \"nnet\",\n  tuneGrid = tune_grid,\n  trControl = ctrl,\n  linout = TRUE,\n  trace = FALSE\n)\n\nnn.tuned$bestTune\n\n\n   size decay\n15    9   0.5\n\n\nCode\nplot(nn.tuned)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks in R</span>"
    ]
  },
  {
    "objectID": "neural-network-Rversion.html#evaluate-performance",
    "href": "neural-network-Rversion.html#evaluate-performance",
    "title": "9  Neural Networks in R",
    "section": "11.2 1.3 Evaluate Performance",
    "text": "11.2 1.3 Evaluate Performance\nAfter identifying the best parameters, we retrain the model and evaluate its predictive accuracy.\n\n\nCode\nbest_size  &lt;- nn.tuned$bestTune$size\nbest_decay &lt;- nn.tuned$bestTune$decay\n\nset.seed(123)\nnn.final &lt;- nnet(\n  y ~ ., \n  data = train_df, \n  size = best_size, \n  decay = best_decay, \n  linout = TRUE, \n  trace = FALSE\n)\n\npred_std &lt;- predict(nn.final, newdata = as.data.frame(x_test))\npred_nn  &lt;- pred_std * attr(y_train_s, \"scaled:scale\") + attr(y_train_s, \"scaled:center\")\n\nperf &lt;- data.frame(\n  MAE  = mean(abs(y_test - pred_nn)),\n  MAPE = mean(abs((y_test - pred_nn) / y_test)) * 100\n)\nperf\n\n\n       MAE     MAPE\n1 20392.57 12.63864",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks in R</span>"
    ]
  },
  {
    "objectID": "neural-network-Rversion.html#train-and-evaluate-the-cnn",
    "href": "neural-network-Rversion.html#train-and-evaluate-the-cnn",
    "title": "9  Neural Networks in R",
    "section": "14.1 4.1 Train and Evaluate the CNN",
    "text": "14.1 4.1 Train and Evaluate the CNN\n\n\nCode\nmodel_cnn &lt;- keras_model_sequential() |&gt;\n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\", input_shape = c(28,28,1)) |&gt;\n  layer_max_pooling_2d(pool_size = c(2,2)) |&gt;\n  layer_flatten() |&gt;\n  layer_dense(units = 32, activation = \"relu\") |&gt;\n  layer_dense(units = 10, activation = \"softmax\")\n\ncompile(model_cnn, optimizer = \"adam\",\n        loss = \"categorical_crossentropy\",\n        metrics = \"accuracy\")\n\nhistory_cnn &lt;- fit(model_cnn, x_train, y_train_cat,\n                   epochs = 3, batch_size = 64, validation_split = 0.2, verbose = 0)\n\nplot(history_cnn)\n\n\n\n\n\n\n\n\n\nCode\nscore &lt;- evaluate(model_cnn, x_test, y_test_cat)\n\n\n32/32 - 0s - 745us/step - accuracy: 0.9000 - loss: 0.3169\n\n\nCode\nscore\n\n\n$accuracy\n[1] 0.9\n\n$loss\n[1] 0.3169269",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks in R</span>"
    ]
  },
  {
    "objectID": "neural-network-Rversion.html#visualizing-correct-and-incorrect-predictions",
    "href": "neural-network-Rversion.html#visualizing-correct-and-incorrect-predictions",
    "title": "9  Neural Networks in R",
    "section": "14.2 4.2 Visualizing Correct and Incorrect Predictions",
    "text": "14.2 4.2 Visualizing Correct and Incorrect Predictions\nWe visualize examples of digits that the CNN classified correctly and incorrectly.\n\n\nCode\npred_probs &lt;- predict(model_cnn, x_test)\n\n\n32/32 - 0s - 1ms/step\n\n\nCode\npred_classes &lt;- apply(pred_probs, 1, which.max) - 1\n\ncorrect_idx &lt;- which(pred_classes == y_test)[1:6]\nincorrect_idx &lt;- which(pred_classes != y_test)[1:6]\n\npar(mfrow=c(2,6), mar=c(1,1,2,1))\nfor (i in correct_idx) {\n  img &lt;- x_test[i,,,1]\n  image(1:28, 1:28, t(apply(img, 2, rev)), col=gray.colors(255), axes=FALSE,\n        main=paste(\"✓\", y_test[i]))\n}\nfor (i in incorrect_idx) {\n  img &lt;- x_test[i,,,1]\n  image(1:28, 1:28, t(apply(img, 2, rev)), col=gray.colors(255), axes=FALSE,\n        main=paste(\"✗\", y_test[i], \"→\", pred_classes[i]))\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks in R</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html",
    "href": "generalized-additive-models-r.html",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "",
    "text": "4.1 1. Setup and Data\nCode\nset.seed(4321)\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(AmesHousing)   # make_ordinal_ames()\nlibrary(earth)         # MARS\nlibrary(segmented)     # piecewise (segmented) regression\nlibrary(splines)       # regression splines (bs, ns) - kept for reference\nlibrary(mgcv)          # smoothing splines via GAM\nlibrary(caret)         # data splitting / utilities\nlibrary(Metrics)       # rmse, mse helpers (plus we'll compute R2 manually)\ntheme_set(theme_minimal())\nCode\n# Limit to requested columns\ncols &lt;- c(\"Sale_Price\",\"Bedroom_AbvGr\",\"Year_Built\",\"Mo_Sold\",\"Lot_Area\",\"Street\",\n          \"Central_Air\",\"First_Flr_SF\",\"Second_Flr_SF\",\"Full_Bath\",\"Half_Bath\",\n          \"Fireplaces\",\"Garage_Area\",\"Gr_Liv_Area\",\"TotRms_AbvGrd\")\n\names &lt;- make_ordinal_ames() |&gt; \n  dplyr::select(dplyr::all_of(cols)) |&gt; \n  tidyr::drop_na()\n\n# Train/test split\nset.seed(4321)\nidx &lt;- sample.int(nrow(ames), size = floor(0.7*nrow(ames)))\ntrain &lt;- ames[idx,]\ntest  &lt;- ames[-idx,]\n\nglimpse(train)\n\n\nRows: 2,051\nColumns: 15\n$ Sale_Price    &lt;int&gt; 274000, 75200, 329900, 145400, 108000, 184000, 176000, 1…\n$ Bedroom_AbvGr &lt;int&gt; 3, 2, 4, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 3, 2, 3, 3, 2,…\n$ Year_Built    &lt;int&gt; 2001, 1922, 2005, 1926, 1949, 1999, 1962, 1915, 1999, 19…\n$ Mo_Sold       &lt;int&gt; 1, 9, 8, 5, 5, 6, 5, 6, 9, 6, 8, 5, 7, 7, 4, 8, 5, 11, 5…\n$ Lot_Area      &lt;int&gt; 9720, 3672, 11643, 7000, 8777, 5858, 19296, 8094, 3768, …\n$ Street        &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pa…\n$ Central_Air   &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y,…\n$ First_Flr_SF  &lt;int&gt; 1366, 816, 1544, 861, 1126, 1337, 1382, 1048, 713, 792, …\n$ Second_Flr_SF &lt;int&gt; 581, 0, 814, 424, 0, 0, 0, 720, 739, 725, 0, 1151, 695, …\n$ Full_Bath     &lt;int&gt; 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1,…\n$ Half_Bath     &lt;int&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,…\n$ Fireplaces    &lt;int&gt; 1, 0, 1, 0, 0, 1, 1, 0, 0, 2, 0, 1, 2, 1, 0, 1, 0, 0, 1,…\n$ Garage_Area   &lt;dbl&gt; 725, 100, 784, 506, 520, 511, 884, 576, 506, 400, 0, 434…\n$ Gr_Liv_Area   &lt;int&gt; 1947, 816, 2358, 1285, 1126, 1337, 1382, 1768, 1452, 151…\n$ TotRms_AbvGrd &lt;int&gt; 7, 5, 10, 6, 5, 5, 6, 8, 6, 7, 5, 8, 7, 5, 6, 7, 6, 6, 5…\n\n\nCode\nsummary(train$Sale_Price)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12789  130000  161000  180897  215000  755000",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#quick-visual-nonlinearity-is-common",
    "href": "generalized-additive-models-r.html#quick-visual-nonlinearity-is-common",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.2 2. Quick Visual: Nonlinearity is Common",
    "text": "4.2 2. Quick Visual: Nonlinearity is Common\n\n\nCode\nggplot(train, aes(Gr_Liv_Area, Sale_Price)) +\n  geom_point(alpha=.3) +\n  geom_smooth(method=\"loess\", se=FALSE) +\n  labs(title=\"Sale_Price vs Gr_Liv_Area (LOESS smoother)\")\n\n\n\n\n\n\n\n\n\nTakeaway: The relationship bends—nonlinear methods can help.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#piecewise-regression",
    "href": "generalized-additive-models-r.html#piecewise-regression",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.3 3. Piecewise Regression",
    "text": "4.3 3. Piecewise Regression\n\n\nCode\nm_lin &lt;- lm(Sale_Price ~ Gr_Liv_Area, data=train)\nm_seg_init &lt;- lm(Sale_Price ~ Gr_Liv_Area, data=train)\nm_seg &lt;- segmented(m_seg_init, seg.Z = ~ Gr_Liv_Area, psi = list(Gr_Liv_Area = 2000))\nsummary(m_seg)\n\n\n\n    ***Regression Model with Segmented Relationship(s)***\n\nCall: \nsegmented.lm(obj = m_seg_init, seg.Z = ~Gr_Liv_Area, psi = list(Gr_Liv_Area = 2000))\n\nEstimated Break-Point(s):\n                  Est.  St.Err\npsi1.Gr_Liv_Area 2466 332.881\n\nCoefficients of the linear terms:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7901.186   4698.641   1.682   0.0928 .  \nGr_Liv_Area     115.705      3.171  36.488   &lt;2e-16 ***\nU1.Gr_Liv_Area  -23.479     12.436  -1.888       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55110 on 2047 degrees of freedom\nMultiple R-Squared: 0.5124,  Adjusted R-squared: 0.5116 \n\nBoot restarting based on 6 samples. Last fit:\nConvergence attained in 1 iterations (rel. change 0)\n\n\n\n\nCode\nplot(train$Gr_Liv_Area, train$Sale_Price, pch=16, cex=.5,\n     xlab=\"Gr_Liv_Area\", ylab=\"Sale_Price\", main=\"Piecewise fit at estimated knot\")\nplot(m_seg, add=TRUE, col=2, lwd=2)\nabline(v = m_seg$psi[,\"Est.\"], lty=2, col=2)\n\n\n\n\n\n\n\n\n\nInterpretation (piecewise): Two linear slopes before/after a knot. Useful when you expect a threshold effect.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#mars-earth-from-simple-to-interactions",
    "href": "generalized-additive-models-r.html#mars-earth-from-simple-to-interactions",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.4 4. MARS (earth) – From Simple to Interactions",
    "text": "4.4 4. MARS (earth) – From Simple to Interactions\nMARS = Multivariate Adaptive Regression Splines. It builds hinge basis functions like h(x - c) = max(0, x - c) and can interact them.\n- A term such as h(Gr_Liv_Area - 1500) means once Gr_Liv_Area exceeds 1500, the fitted line’s slope can change.\n- An interaction like h(Gr_Liv_Area - 1500) * Central_AirY means the slope change applies when Central_Air == \"Y\".\n\n4.4.1 4A. Univariate MARS (Garage_Area)\n\n\nCode\nmars1 &lt;- earth(Sale_Price ~ Garage_Area, data=train)\nsummary(mars1)\n\n\nCall: earth(formula=Sale_Price~Garage_Area, data=train)\n\n                    coefficients\n(Intercept)           124159.039\nh(286-Garage_Area)       -60.257\nh(Garage_Area-286)       297.277\nh(Garage_Area-521)      -483.642\nh(Garage_Area-576)       733.859\nh(Garage_Area-758)      -356.460\nh(Garage_Area-1043)     -490.873\n\nSelected 7 of 7 terms, and 1 of 1 predictors\nTermination condition: RSq changed by less than 0.001 at 7 terms\nImportance: Garage_Area\nNumber of terms at each degree of interaction: 1 6 (additive model)\nGCV 3427475346    RSS 6.94092e+12    GRSq 0.4492014    RSq 0.4556309\n\n\n\n\nCode\ngrid &lt;- tibble(Garage_Area = seq(min(train$Garage_Area), max(train$Garage_Area), length.out=200))\ngrid$pred &lt;- predict(mars1, newdata=grid)\n\nggplot(train, aes(Garage_Area, Sale_Price)) +\n  geom_point(alpha=.3) +\n  geom_line(data=grid, aes(Garage_Area, pred), color=\"blue\", linewidth=1.2) +\n  labs(title=\"Step 1: MARS with One Predictor (Garage_Area)\",\n       subtitle=\"Piecewise linear fit with automatically chosen knots\",\n       y=\"Predicted Sale_Price\")\n\n\n\n\n\n\n\n\n\n\n\n4.4.2 4B. Additive MARS (degree = 1; no interactions)\n\n\nCode\nmars2 &lt;- earth(Sale_Price ~ Bedroom_AbvGr + Year_Built + Mo_Sold + Lot_Area +\n                 Street + Central_Air + First_Flr_SF + Second_Flr_SF + Full_Bath +\n                 Half_Bath + Fireplaces + Garage_Area + Gr_Liv_Area + TotRms_AbvGrd,\n               data=train, degree=1, nfold=5)\nsummary(mars2)\n\n\nCall: earth(formula=Sale_Price~Bedroom_AbvGr+Year_Built+Mo_Sold+Lot_Ar...),\n            data=train, degree=1, nfold=5)\n\n                      coefficients\n(Intercept)              319493.46\nCentral_AirY              20289.49\nh(4-Bedroom_AbvGr)         9214.66\nh(Bedroom_AbvGr-4)       -23009.05\nh(Year_Built-1977)         1275.57\nh(2004-Year_Built)         -336.64\nh(Year_Built-2004)         5315.57\nh(13869-Lot_Area)            -2.09\nh(Lot_Area-13869)             0.22\nh(First_Flr_SF-1600)        104.91\nh(2402-First_Flr_SF)        -71.56\nh(First_Flr_SF-2402)       -176.61\nh(1523-Second_Flr_SF)       -53.13\nh(Second_Flr_SF-1523)       426.63\nh(Half_Bath-1)           -45378.31\nh(2-Fireplaces)          -14408.56\nh(Fireplaces-2)          -26072.58\nh(Garage_Area-539)          101.97\nh(Garage_Area-1043)        -294.30\nh(Gr_Liv_Area-2049)          65.21\nh(Gr_Liv_Area-3194)        -159.79\n\nSelected 21 of 24 terms, and 10 of 14 predictors\nTermination condition: Reached nk 29\nImportance: First_Flr_SF, Second_Flr_SF, Year_Built, Garage_Area, ...\nNumber of terms at each degree of interaction: 1 20 (additive model)\nGCV 1033819964  RSS 2.036439e+12  GRSq 0.8338641  RSq 0.8402842  CVRSq 0.7978671\n\nNote: the cross-validation sd's below are standard deviations across folds\n\nCross validation:   nterms 21.00 sd 0.71    nvars 9.20 sd 0.84\n\n     CVRSq    sd     MaxErr     sd\n     0.798 0.043    -428941 282429\n\n\n\n\nCode\nev2 &lt;- evimp(mars2); plot(ev2, main=\"Step 2: Variable Importance (degree=1, additive)\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2)); plotmo(mars2, type=\"response\", nresponse=1, do.par=FALSE); par(mfrow=c(1,1))\n\n\n plotmo grid:    Bedroom_AbvGr Year_Built Mo_Sold Lot_Area Street Central_Air\n                             3       1974       6     9480   Pave           Y\n First_Flr_SF Second_Flr_SF Full_Bath Half_Bath Fireplaces Garage_Area\n         1092             0         2         0          1         480\n Gr_Liv_Area TotRms_AbvGrd\n        1442             6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.3 4C. MARS with 2-way Interactions (degree = 2)\n\n\nCode\nmars3 &lt;- earth(Sale_Price ~ Bedroom_AbvGr + Year_Built + Mo_Sold + Lot_Area +\n                 Street + Central_Air + First_Flr_SF + Second_Flr_SF + Full_Bath +\n                 Half_Bath + Fireplaces + Garage_Area + Gr_Liv_Area + TotRms_AbvGrd,\n               data=train, degree=2, nfold=5)\nsummary(mars3)\n\n\nCall: earth(formula=Sale_Price~Bedroom_AbvGr+Year_Built+Mo_Sold+Lot_Ar...),\n            data=train, degree=2, nfold=5)\n\n                                           coefficients\n(Intercept)                                   411949.27\nh(Year_Built-2004)                            169869.37\nh(14803-Lot_Area)                                 -1.75\nh(Lot_Area-14803)                                  0.48\nh(1570-First_Flr_SF)                            -150.79\nh(First_Flr_SF-1570)                             224.90\nh(1523-Second_Flr_SF)                            -86.91\nh(Second_Flr_SF-1523)                            156.02\nh(1-Half_Bath)                                 -7743.19\nh(Half_Bath-1)                                -54819.24\nh(2-Fireplaces)                               -10516.92\nh(1043-Garage_Area)                              -70.97\nh(Garage_Area-1043)                             -141.86\nh(Gr_Liv_Area-3194)                              106.27\nh(3-Bedroom_AbvGr) * h(1523-Second_Flr_SF)         7.01\nh(Bedroom_AbvGr-3) * h(1523-Second_Flr_SF)        -9.61\nh(2004-Year_Built) * h(First_Flr_SF-876)          -1.86\nh(2004-Year_Built) * h(3194-Gr_Liv_Area)          -0.17\nh(Year_Built-2004) * h(Gr_Liv_Area-2320)        -159.80\nh(Year_Built-2004) * h(2320-Gr_Liv_Area)         156.64\nh(Year_Built-2004) * h(3194-Gr_Liv_Area)        -173.05\nh(First_Flr_SF-1778) * h(2-Fireplaces)           -78.19\nh(1043-Garage_Area) * h(2122-Gr_Liv_Area)          0.05\n\nSelected 23 of 29 terms, and 9 of 14 predictors\nTermination condition: Reached nk 29\nImportance: First_Flr_SF, Second_Flr_SF, Year_Built, Gr_Liv_Area, ...\nNumber of terms at each degree of interaction: 1 13 9\nGCV 795873462  RSS 1.544416e+12  GRSq 0.8721024  RSq 0.8788731  CVRSq 0.7824362\n\nNote: the cross-validation sd's below are standard deviations across folds\n\nCross validation:   nterms 22.60 sd 1.82    nvars 9.40 sd 1.14\n\n     CVRSq    sd     MaxErr     sd\n     0.782 0.114    -945502 422348\n\n\n\n\nCode\nev3 &lt;- evimp(mars3); plot(ev3, main=\"Step 3: Variable Importance (degree=2, interactions)\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2)); plotmo(mars3, type=\"response\", nresponse=1, do.par=FALSE); par(mfrow=c(1,1))\n\n\n plotmo grid:    Bedroom_AbvGr Year_Built Mo_Sold Lot_Area Street Central_Air\n                             3       1974       6     9480   Pave           Y\n First_Flr_SF Second_Flr_SF Full_Bath Half_Bath Fireplaces Garage_Area\n         1092             0         2         0          1         480\n Gr_Liv_Area TotRms_AbvGrd\n        1442             6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting MARS terms:\nLook for h() pieces and products. Coefficients on h(x-c) indicate how slope changes after the knot c. Interaction products mean “the slope change depends on another variable”.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#loess-visual-local-regression",
    "href": "generalized-additive-models-r.html#loess-visual-local-regression",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.5 5. LOESS (Visual local regression)",
    "text": "4.5 5. LOESS (Visual local regression)\n\n\nCode\nloess_fit &lt;- loess(Sale_Price ~ Gr_Liv_Area, data=train, span=0.4)\ngrid_lo &lt;- tibble(Gr_Liv_Area = seq(min(train$Gr_Liv_Area), max(train$Gr_Liv_Area), length.out=200))\ngrid_lo$pred &lt;- predict(loess_fit, newdata=grid_lo)\n\nggplot() +\n  geom_point(data=train, aes(Gr_Liv_Area, Sale_Price), alpha=.25) +\n  geom_line(data=grid_lo, aes(Gr_Liv_Area, pred), linewidth=1.2) +\n  labs(title=\"LOESS fit (span=0.4)\", y=\"Predicted Sale_Price\")\n\n\n\n\n\n\n\n\n\nWe keep LOESS as a visualization-oriented smoother (not used in the multivariate comparison below).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#gam-splines-with-mgcv-simple-complex",
    "href": "generalized-additive-models-r.html#gam-splines-with-mgcv-simple-complex",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.6 6. GAM Splines with mgcv – Simple → Complex",
    "text": "4.6 6. GAM Splines with mgcv – Simple → Complex\nGAM smooths are written s(x) and estimated with penalized splines. The edf (effective degrees of freedom) in the summary tells you the smooth’s complexity (larger edf ⇒ wigglier function).\n\n4.6.1 6A. Univariate GAM: Sale_Price ~ s(Garage_Area)\n\n\nCode\ngam1 &lt;- gam(Sale_Price ~ s(Garage_Area, k=9), data=train, method=\"REML\")\nsummary(gam1)  # inspect edf and significance\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nSale_Price ~ s(Garage_Area, k = 9)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   180897       1292     140   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df     F p-value    \ns(Garage_Area) 7.078  7.719 217.1  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =   0.45   Deviance explained = 45.2%\n-REML =  25418  Scale est. = 3.4232e+09  n = 2051\n\n\nCode\nplot(gam1, pages=1, shade=TRUE, scheme=1, scale=0)\n\n\n\n\n\n\n\n\n\n\n\n4.6.2 6B. Two-smooth Additive GAM: s(Garage_Area) + s(Gr_Liv_Area)\n\n\nCode\ngam2 &lt;- gam(Sale_Price ~ s(Garage_Area, k=9) + s(Gr_Liv_Area, k=9), data=train, method=\"REML\")\nsummary(gam2)\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nSale_Price ~ s(Garage_Area, k = 9) + s(Gr_Liv_Area, k = 9)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   180897       1026   176.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df     F p-value    \ns(Garage_Area) 6.446  7.293 103.3  &lt;2e-16 ***\ns(Gr_Liv_Area) 7.585  7.941 152.6  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.653   Deviance explained = 65.5%\n-REML =  24949  Scale est. = 2.1608e+09  n = 2051\n\n\nCode\nplot(gam2, pages=1, shade=TRUE, scheme=1, scale=0)\n\n\n\n\n\n\n\n\n\n\n\n4.6.3 6C. Full Multivariate GAM (selected smooths + factors)\n\n\nCode\ngam3 &lt;- gam(Sale_Price ~ \n              s(Garage_Area, k=9) + \n              s(Gr_Liv_Area, k=9) + \n              s(Year_Built, k=7) +\n              s(Lot_Area, k=7) +\n              Bedroom_AbvGr + Mo_Sold + \n              Street + Central_Air + \n              First_Flr_SF + Second_Flr_SF + \n              Full_Bath + Half_Bath + \n              Fireplaces + TotRms_AbvGrd,\n            data=train, method=\"REML\")\nsummary(gam3)\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nSale_Price ~ s(Garage_Area, k = 9) + s(Gr_Liv_Area, k = 9) + \n    s(Year_Built, k = 7) + s(Lot_Area, k = 7) + Bedroom_AbvGr + \n    Mo_Sold + Street + Central_Air + First_Flr_SF + Second_Flr_SF + \n    Full_Bath + Half_Bath + Fireplaces + TotRms_AbvGrd\n\nParametric coefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -21237.92   30106.84  -0.705   0.4806    \nBedroom_AbvGr -10350.20    1382.38  -7.487 1.05e-13 ***\nMo_Sold         -409.12     270.24  -1.514   0.1302    \nStreetPave     30589.21   13486.61   2.268   0.0234 *  \nCentral_AirY   17189.68    3366.03   5.107 3.59e-07 ***\nFirst_Flr_SF     140.27      17.44   8.044 1.47e-15 ***\nSecond_Flr_SF    101.93      17.36   5.871 5.06e-09 ***\nFull_Bath      -4548.94    2211.69  -2.057   0.0398 *  \nHalf_Bath       2355.98    2175.40   1.083   0.2789    \nFireplaces     13066.61    1371.59   9.527  &lt; 2e-16 ***\nTotRms_AbvGrd  -1640.87     948.67  -1.730   0.0838 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df      F p-value    \ns(Garage_Area) 7.067  7.712  24.96  &lt;2e-16 ***\ns(Gr_Liv_Area) 7.899  7.996  46.44  &lt;2e-16 ***\ns(Year_Built)  5.255  5.764 128.05  &lt;2e-16 ***\ns(Lot_Area)    5.465  5.883  16.44  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.827   Deviance explained =   83%\n-REML =  24167  Scale est. = 1.0786e+09  n = 2051\n\n\nCode\nplot(gam3, pages=1, shade=TRUE, scheme=1, scale=0)\n\n\n\n\n\n\n\n\n\nInterpreting GAM splines:\nA smooth s(x) shows how the expected outcome varies with x holding other terms constant. The edf indicates complexity; wide confidence bands suggest greater uncertainty in those regions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#model-comparison-r²-rmse-mse",
    "href": "generalized-additive-models-r.html#model-comparison-r²-rmse-mse",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.7 7. Model Comparison (R², RMSE, MSE)",
    "text": "4.7 7. Model Comparison (R², RMSE, MSE)\n\n\nCode\n# Helper for R^2 on test\nrsq &lt;- function(y, yhat) {\n  1 - sum((y - yhat)^2) / sum((y - mean(y))^2)\n}\n\n# Predictions\np_piece &lt;- predict(m_seg, newdata=test)\np_mars  &lt;- predict(mars3, newdata=test)   # final MARS with interactions\np_gam   &lt;- predict(gam3, newdata=test)    # full multivariate GAM\n\n# Metrics\ntbl &lt;- tibble(\n  Model = c(\"Piecewise (segmented)\", \"MARS (degree=2)\", \"GAM splines (mgcv full)\"),\n  RMSE  = c(rmse(test$Sale_Price, p_piece),\n            rmse(test$Sale_Price, p_mars),\n            rmse(test$Sale_Price, p_gam)),\n  MSE   = c(mse(test$Sale_Price, p_piece),\n            mse(test$Sale_Price, p_mars),\n            mse(test$Sale_Price, p_gam)),\n  R2    = c(rsq(test$Sale_Price, p_piece),\n            rsq(test$Sale_Price, p_mars),\n            rsq(test$Sale_Price, p_gam))\n) |&gt; arrange(desc(R2))\n\ntbl\n\n\n# A tibble: 3 × 4\n  Model                     RMSE         MSE    R2\n  &lt;chr&gt;                    &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 MARS (degree=2)         33028. 1090840972. 0.839\n2 GAM splines (mgcv full) 36796. 1353918715. 0.800\n3 Piecewise (segmented)   59034. 3484963067. 0.484",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#final-plot-predicted-vs-actual-test-set",
    "href": "generalized-additive-models-r.html#final-plot-predicted-vs-actual-test-set",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.8 8. Final Plot: Predicted vs Actual (Test Set)",
    "text": "4.8 8. Final Plot: Predicted vs Actual (Test Set)\n\n\nCode\nplot_df &lt;- bind_rows(\n  tibble(Model=\"Piecewise (segmented)\", Actual=test$Sale_Price, Pred=p_piece),\n  tibble(Model=\"MARS (degree=2)\", Actual=test$Sale_Price, Pred=p_mars),\n  tibble(Model=\"GAM splines (mgcv full)\", Actual=test$Sale_Price, Pred=p_gam)\n)\n\nggplot(plot_df, aes(Actual, Pred, color=Model)) +\n  geom_point(alpha=.35) +\n  geom_abline(intercept=0, slope=1, linetype=\"dashed\") +\n  labs(title=\"Predicted vs Actual (Test Set)\", y=\"Predicted Sale_Price\") +\n  coord_equal()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#how-to-explain",
    "href": "generalized-additive-models-r.html#how-to-explain",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.9 9. How to Explain",
    "text": "4.9 9. How to Explain\n\nPiecewise: “There’s a threshold where the effect changes.” Simple story, limited flexibility.\n\nMARS: “The model finds breakpoints and sometimes when they matter (interactions).” Explain h(x-c) as extra slope after c.\n\nGAM (splines): “We model smooth curves for key variables; edf shows wiggliness. More edf ⇒ more flexibility.”\n\nTradeoff: More flexibility usually improves error (RMSE/MSE) and R², but reduce interpretability. Validate with cross‑validation and keep the story aligned with business intuition.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "neural-network-Rversion.html#feed-forward-neural-networks-ames-data",
    "href": "neural-network-Rversion.html#feed-forward-neural-networks-ames-data",
    "title": "9  Neural Networks in R",
    "section": "9.2 1. Feed-Forward Neural Networks (Ames Data)",
    "text": "9.2 1. Feed-Forward Neural Networks (Ames Data)\nFeed-forward networks are the simplest type of neural networks. They map input features (like home size, rooms, or year built) to an output (home price) through layers of neurons.\nEach neuron applies a weighted sum and activation function, allowing the network to learn complex relationships between variables.\n\n\nCode\n#Note: This is specific to my environment \nlibrary(reticulate)\nuse_python(\"/Users/donaldhale/r-arm/bin/python\", required = TRUE)\n\nlibrary(keras3)\n\ntf &lt;- reticulate::import(\"tensorflow\", delay_load = FALSE)\ncat(\"TensorFlow version:\", tf$`__version__`, \"\\n\")\n\n\nTensorFlow version: 2.16.1 \n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(nnet)\nlibrary(NeuralNetTools)\nlibrary(AmesHousing)\n\nset.seed(4321)\n\n# Load and sample data\names &lt;- make_ordinal_ames() %&gt;% mutate(id = row_number())\ntrain_idx &lt;- sample(1:nrow(ames), 0.7 * nrow(ames))\ntraining &lt;- ames[train_idx, ]\ntesting  &lt;- ames[-train_idx, ]\n\n# Select a manageable subset of variables\nvars &lt;- c(\"Sale_Price\", \"Bedroom_AbvGr\", \"Year_Built\", \"Mo_Sold\", \"Lot_Area\",\n          \"Street\", \"Central_Air\", \"First_Flr_SF\", \"Second_Flr_SF\",\n          \"Full_Bath\", \"Half_Bath\", \"Fireplaces\", \"Garage_Area\",\n          \"Gr_Liv_Area\", \"TotRms_AbvGrd\")\ntraining &lt;- training[, vars]\ntesting  &lt;- testing[, vars]\n\n# Convert categorical predictors into dummy numeric columns\nx_train &lt;- model.matrix(Sale_Price ~ ., data = training)[, -1]\ny_train &lt;- training$Sale_Price\nx_test  &lt;- model.matrix(Sale_Price ~ ., data = testing)[, -1]\ny_test  &lt;- testing$Sale_Price\n\n# Drop low-variance predictors and scale for stable training\nnzv &lt;- nearZeroVar(x_train)\nif (length(nzv) &gt; 0) {\n  x_train &lt;- x_train[, -nzv, drop = FALSE]\n  x_test  &lt;- x_test[, -nzv, drop = FALSE]\n}\n\nx_train &lt;- scale(x_train)\nx_test  &lt;- scale(x_test, center = attr(x_train, \"scaled:center\"),\n                 scale = attr(x_train, \"scaled:scale\"))\n\ny_train_s &lt;- scale(y_train)\n\n# Combine into a single training dataframe\ntrain_df &lt;- data.frame(y = as.numeric(y_train_s), x_train)\n\nset.seed(123)\nnn.ames &lt;- nnet(y ~ ., data = train_df, size = 5, linout = TRUE, trace = FALSE)\nplotnet(nn.ames)\n\n\n\n\n\n\n\n\n\n\n\n9.2.1 1.2 Cross-Validation Tuning\nWe tune two important parameters: - size — number of neurons in the hidden layer\n- decay — regularization strength (prevents overfitting)\ncaret::train() automates this search using k-fold cross-validation.\n\n\nCode\ntune_grid &lt;- expand.grid(size = c(3,5,7,9), decay = c(0,0.1,0.5,1))\nctrl &lt;- trainControl(method = \"cv\", number = 5)\n\nset.seed(123)\nnn.tuned &lt;- train(\n  x = x_train, y = as.numeric(y_train_s),\n  method = \"nnet\",\n  tuneGrid = tune_grid,\n  trControl = ctrl,\n  linout = TRUE,\n  trace = FALSE\n)\n\nnn.tuned$bestTune\n\n\n   size decay\n15    9   0.5\n\n\nCode\nplot(nn.tuned)\n\n\n\n\n\n\n\n\n\n\n\n\n9.2.2 1.3 Evaluate Performance\nAfter identifying the best parameters, we retrain the model and evaluate its predictive accuracy.\n\n\nCode\nbest_size  &lt;- nn.tuned$bestTune$size\nbest_decay &lt;- nn.tuned$bestTune$decay\n\nset.seed(123)\nnn.final &lt;- nnet(\n  y ~ ., \n  data = train_df, \n  size = best_size, \n  decay = best_decay, \n  linout = TRUE, \n  trace = FALSE\n)\n\npred_std &lt;- predict(nn.final, newdata = as.data.frame(x_test))\npred_nn  &lt;- pred_std * attr(y_train_s, \"scaled:scale\") + attr(y_train_s, \"scaled:center\")\n\nperf &lt;- data.frame(\n  MAE  = mean(abs(y_test - pred_nn)),\n  MAPE = mean(abs((y_test - pred_nn) / y_test)) * 100\n)\nperf\n\n\n       MAE     MAPE\n1 20392.57 12.63864",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks in R</span>"
    ]
  },
  {
    "objectID": "neural-network-Rversion.html#feed-forward-neural-networks-keras-3",
    "href": "neural-network-Rversion.html#feed-forward-neural-networks-keras-3",
    "title": "9  Neural Networks in R",
    "section": "9.3 2. Feed-Forward Neural Networks (Keras 3)",
    "text": "9.3 2. Feed-Forward Neural Networks (Keras 3)\nThe Keras 3 API is a modern deep learning interface. It supports more complex models and GPUs while maintaining a simple syntax.\nThis example shows the same concept as above using a multi-layer perceptron (MLP).\n\n\nCode\n#library(keras3)\nset.seed(123)\n\nmodel_ff &lt;- keras_model_sequential() |&gt;\n  layer_dense(units = 16, activation = \"relu\", input_shape = ncol(x_train)) |&gt;\n  layer_dropout(rate = 0.1) |&gt;\n  layer_dense(units = 1)\n\ncompile(model_ff, optimizer = optimizer_adam(learning_rate = 0.01),\n        loss = \"mse\", metrics = \"mae\")\n\nhistory &lt;- fit(model_ff, x_train, as.numeric(y_train_s),\n               epochs = 40, batch_size = 32, validation_split = 0.2, verbose = 0)\n\nplot(history)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks in R</span>"
    ]
  },
  {
    "objectID": "neural-network-Rversion.html#recurrent-neural-network-rnn",
    "href": "neural-network-Rversion.html#recurrent-neural-network-rnn",
    "title": "9  Neural Networks in R",
    "section": "9.4 3. Recurrent Neural Network (RNN)",
    "text": "9.4 3. Recurrent Neural Network (RNN)\nRNNs are ideal for sequential data such as time series, text, or speech.\nWe use a sine wave as a toy example because its repeating pattern requires the model to “remember” recent values — something feed-forward networks cannot do.\n\n\nCode\nset.seed(42)\nt &lt;- seq(0, 50, by = 0.1)\ny &lt;- sin(t)\nplot(t, y, type='l', main=\"Sine Wave Example\", ylab=\"y\", xlab=\"Time\")\n\n\n\n\n\n\n\n\n\nCode\nwindow_size &lt;- 20\nX &lt;- sapply(1:(length(y) - window_size), function(i) y[i:(i + window_size - 1)])\nX &lt;- t(X)\ny_out &lt;- y[(window_size + 1):length(y)]\nX &lt;- array(X, dim = c(nrow(X), window_size, 1))\n\nsplit &lt;- round(0.8 * nrow(X))\nx_train &lt;- X[1:split,,]; y_train &lt;- y_out[1:split]\nx_test  &lt;- X[(split + 1):nrow(X),,]; y_test  &lt;- y_out[(split + 1):length(y_out)]\n\nmodel_rnn &lt;- keras_model_sequential() |&gt;\n  layer_simple_rnn(units = 16, activation = \"tanh\", input_shape = c(window_size, 1)) |&gt;\n  layer_dense(units = 1)\n\ncompile(model_rnn, loss = \"mse\", optimizer = \"adam\")\n\nhistory_rnn &lt;- fit(model_rnn, x_train, y_train, epochs = 30, batch_size = 16,\n                   validation_split = 0.2, verbose = 0)\n\nplot(history_rnn)\n\n\n\n\n\n\n\n\n\nCode\npreds &lt;- predict(model_rnn, x_test)\n\n\n3/3 - 0s - 11ms/step\n\n\nCode\nplot(y_test, type='l', main=\"RNN Sine Prediction\", col='blue', ylab=\"y\")\nlines(preds, col='red')\nlegend(\"bottomleft\", legend=c(\"Actual\",\"Predicted\"), col=c(\"blue\",\"red\"), lty=1, bty=\"n\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks in R</span>"
    ]
  },
  {
    "objectID": "neural-network-Rversion.html#convolutional-neural-network-cnn",
    "href": "neural-network-Rversion.html#convolutional-neural-network-cnn",
    "title": "9  Neural Networks in R",
    "section": "9.5 4. Convolutional Neural Network (CNN)",
    "text": "9.5 4. Convolutional Neural Network (CNN)\nCNNs are built for spatial pattern recognition — ideal for images.\nThe MNIST dataset contains 70,000 grayscale images of handwritten digits (0–9), each 28×28 pixels.\nThese lines prepare the data for use in a CNN.\n\n\nCode\n#library(keras3)\n\n# Load and preview the MNIST data\nmnist &lt;- dataset_mnist()\n\n# We only use a subset (5k train, 1k test) for quick demonstration\nx_train &lt;- mnist$train$x[1:5000,,]\ny_train &lt;- mnist$train$y[1:5000]\nx_test  &lt;- mnist$test$x[1:1000,,]\ny_test  &lt;- mnist$test$y[1:1000]\n\n# Normalize pixel values: 0-255 -&gt; 0-1\nx_train &lt;- x_train / 255\nx_test  &lt;- x_test / 255\n\n# Reshape to [samples, height, width, channels]\n# MNIST images are grayscale, so channels = 1\nx_train &lt;- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))\nx_test  &lt;- array_reshape(x_test,  c(nrow(x_test),  28, 28, 1))\n\n# Convert labels to categorical one-hot vectors (10 classes for digits 0-9)\ny_train_cat &lt;- to_categorical(y_train, 10)\ny_test_cat  &lt;- to_categorical(y_test, 10)\n\n\n\n9.5.1 Explanation of the MNIST Preprocessing\n\nLoad the built-in dataset from Keras.\n\nSubsample for faster training (5,000 train / 1,000 test).\n\nNormalize pixel intensities to the 0–1 range.\n\nReshape to the 4D structure required by CNNs: [samples, height, width, channels].\n\nOne-hot encode labels, converting a digit like 3 into a vector [0,0,0,1,0,0,0,0,0,0].\n\nThese steps turn raw pixel data into numeric tensors suitable for learning.\n\n\n\n9.5.2 4.1 Train and Evaluate the CNN\n\n\nCode\nmodel_cnn &lt;- keras_model_sequential() |&gt;\n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\", input_shape = c(28,28,1)) |&gt;\n  layer_max_pooling_2d(pool_size = c(2,2)) |&gt;\n  layer_flatten() |&gt;\n  layer_dense(units = 32, activation = \"relu\") |&gt;\n  layer_dense(units = 10, activation = \"softmax\")\n\ncompile(model_cnn, optimizer = \"adam\",\n        loss = \"categorical_crossentropy\",\n        metrics = \"accuracy\")\n\nhistory_cnn &lt;- fit(model_cnn, x_train, y_train_cat,\n                   epochs = 3, batch_size = 64, validation_split = 0.2, verbose = 0)\n\nplot(history_cnn)\n\n\n\n\n\n\n\n\n\nCode\nscore &lt;- evaluate(model_cnn, x_test, y_test_cat)\n\n\n32/32 - 0s - 745us/step - accuracy: 0.9000 - loss: 0.3169\n\n\nCode\nscore\n\n\n$accuracy\n[1] 0.9\n\n$loss\n[1] 0.3169269\n\n\n\n\n\n9.5.3 4.2 Visualizing Correct and Incorrect Predictions\nWe visualize examples of digits that the CNN classified correctly and incorrectly.\n\n\nCode\npred_probs &lt;- predict(model_cnn, x_test)\n\n\n32/32 - 0s - 1ms/step\n\n\nCode\npred_classes &lt;- apply(pred_probs, 1, which.max) - 1\n\ncorrect_idx &lt;- which(pred_classes == y_test)[1:6]\nincorrect_idx &lt;- which(pred_classes != y_test)[1:6]\n\npar(mfrow=c(2,6), mar=c(1,1,2,1))\nfor (i in correct_idx) {\n  img &lt;- x_test[i,,,1]\n  image(1:28, 1:28, t(apply(img, 2, rev)), col=gray.colors(255), axes=FALSE,\n        main=paste(\"✓\", y_test[i]))\n}\nfor (i in incorrect_idx) {\n  img &lt;- x_test[i,,,1]\n  image(1:28, 1:28, t(apply(img, 2, rev)), col=gray.colors(255), axes=FALSE,\n        main=paste(\"✗\", y_test[i], \"→\", pred_classes[i]))\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks in R</span>"
    ]
  },
  {
    "objectID": "neural-network-Rversion.html#summary",
    "href": "neural-network-Rversion.html#summary",
    "title": "9  Neural Networks in R",
    "section": "9.6 Summary",
    "text": "9.6 Summary\n\nFeed-forward networks: handle static, tabular data (e.g., housing prices).\n\nRNNs: handle sequential data with temporal dependencies (e.g., sine wave, stock prices, text, timer series).\n\nCNNs: handle image and spatial data by learning patterns like edges and textures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks in R</span>"
    ]
  }
]