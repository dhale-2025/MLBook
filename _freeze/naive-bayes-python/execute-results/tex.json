{
  "hash": "f487c256cb1eec68513cf2252d0efcf0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Naive Bayes Classification - Python Version\"\nformat:\n  html:\n    toc: true\n    toc-depth: 4\n    code-fold: true\njupyter: python3\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n## 1. Overview and Learning Objectives\n\nIn this Python chapter, we will:\n\n1. Load the **Ames Housing** dataset from OpenML.\n2. Create a **High vs Low** price classification target.\n3. Fit **Gaussian Naive Bayes**.\n4. Produce predictions and class probabilities.\n5. Compute a **correct ROC curve and AUC**.\n6. Mirror the structure of the R version.\n\n---\n\n## 2. Setup and Data Preparation\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_openml\n\n# Load Ames Housing dataset\names = fetch_openml(name=\"house_prices\", as_frame=True)\ndf = ames.frame.copy()\n\ndf.rename(columns={\"SalePrice\": \"Sale_Price\"}, inplace=True)\n\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>Sale_Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 81 columns</p>\n</div>\n```\n:::\n:::\n\n\n---\n\n## 2.1 Select Variables of Interest\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\npredictors = [\n    \"Sale_Price\",\n    \"BedroomAbvGr\",\n    \"YearBuilt\",\n    \"MoSold\",\n    \"LotArea\",\n    \"Street\",\n    \"CentralAir\",\n    \"1stFlrSF\",\n    \"2ndFlrSF\",\n    \"FullBath\",\n    \"HalfBath\",\n    \"Fireplaces\",\n    \"GarageArea\",\n    \"GrLivArea\",\n    \"TotRmsAbvGrd\"\n]\n\ndf = df[predictors].dropna()\n```\n:::\n\n\n---\n\n## 2.2 Create a Binary Target Variable\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nmedian_price = df[\"Sale_Price\"].median()\n\ndf[\"Price_High\"] = np.where(df[\"Sale_Price\"] > median_price, \"High\", \"Low\")\ndf[\"Price_High\"] = df[\"Price_High\"].astype(\"category\")\n\ndf[\"Price_High\"].value_counts(normalize=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nPrice_High\nLow     0.50137\nHigh    0.49863\nName: proportion, dtype: float64\n```\n:::\n:::\n\n\n---\n\n## 2.3 Train/Test Split\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nX = df.drop(columns=[\"Sale_Price\", \"Price_High\"])\ny = df[\"Price_High\"]\n\n# Encode categorical predictors\nX = pd.get_dummies(X, drop_first=True)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=4321, stratify=y\n)\n\nX_train.shape, X_test.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n((1022, 14), (438, 14))\n```\n:::\n:::\n\n\n---\n\n## 3. Gaussian Naive Bayes Model\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nGaussianNB()\n```\n:::\n:::\n\n\n---\n\n## 4. Predictions\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ny_pred = gnb.predict(X_test)\n\n# Extract the probability of the TRUE positive class (\"High\")\npos_index = np.where(gnb.classes_ == \"High\")[0][0]\ny_prob_high = gnb.predict_proba(X_test)[:, pos_index]\n```\n:::\n\n\n---\n\n## 5. Confusion Matrix & Classification Report\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ncm = confusion_matrix(y_test, y_pred)\ncm\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([[187,  31],\n       [ 20, 200]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](naive-bayes-python_files/figure-pdf/cell-9-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n        High       0.90      0.86      0.88       218\n         Low       0.87      0.91      0.89       220\n\n    accuracy                           0.88       438\n   macro avg       0.88      0.88      0.88       438\nweighted avg       0.88      0.88      0.88       438\n\n```\n:::\n:::\n\n\n---\n\n## 6. ROC Curve (Corrected) and AUC\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Convert y_test to binary (1 = High, 0 = Low)\ny_test_binary = (y_test == \"High\").astype(int)\n\n# Compute ROC using correct positive class probability\nfpr, tpr, thresholds = roc_curve(y_test_binary, y_prob_high)\nroc_auc = auc(fpr, tpr)\n\nroc_auc\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nnp.float64(0.9547122602168474)\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nplt.figure(figsize=(6,5))\nplt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\", linewidth=3)\nplt.plot([0,1], [0,1], \"k--\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve - Naive Bayes (Corrected)\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](naive-bayes-python_files/figure-pdf/cell-12-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n---\n\n## 7. Interpretation Notes\n\n### Naive Bayes Assumptions\n- Predictors are independent given the class.\n- Numeric features follow Gaussian distributions.\n- One-hot encoded categorical variables behave like Bernoulli variables.\n\n### Strengths\n- Fast and simple.\n- Works well as a baseline.\n- Highly interpretable.\n\n### Weaknesses\n- Independence assumption rarely holds.\n- Gaussian assumption is an approximation.\n\n---\n\n## 8. Summary\n\nThis chapter:\n\n- Loaded the Ames Housing dataset.\n- Created a binary classification target.\n- Fit Gaussian Naive Bayes.\n- Correctly computed ROC & AUC.\n- Provided fully aligned interpretation with the R version.\n\n",
    "supporting": [
      "naive-bayes-python_files/figure-pdf"
    ],
    "filters": []
  }
}