{
  "hash": "d1c7fb5486cfe09d6b03f565c6865ca5",
  "result": {
    "engine": "knitr",
    "markdown": "\n---\ntitle: \"Tree Based Models - R Version\"\nformat:\n  html:\n    toc: true\n    number-sections: true\n    code-fold: true\n  pdf:\n    number-sections: true\n    toc: true\n---\n\n> Comparison: **Decision Tree**, **Random Forest**, and **XGBoost** on the Ames dataset. Includes random-variable baselines, variable importance, partial dependence plots.\n\n## Visual Roadmap\n\n```{mermaid}\nflowchart LR\n    A[\"Data (Ames)\"]\n    DT[\"Decision Tree (Baseline)\"]\n    RF[\"Random Forest (Bagging + Random Features)\"]\n    XGB[\"XGBoost (Boosted Trees + Regularization)\"]\n    Compare[\"Model Comparison\"]\n    \n    A --> DT\n    A --> RF\n    A --> XGB\n    DT --> Compare\n    RF --> Compare\n    XGB --> Compare\n```\n\n## Libraries, Data, Split\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(randomForest)\nlibrary(xgboost)\nlibrary(Ckmeans.1d.dp)\nlibrary(pdp)\nlibrary(AmesHousing)\n\names <- make_ordinal_ames()\names <- ames %>% mutate(id = row_number())\n\nset.seed(4321)\ntraining <- ames %>% sample_frac(0.7)\ntesting <- anti_join(ames, training, by = 'id')\n\ntraining <- training %>% \n  select(Sale_Price,\n         Bedroom_AbvGr, Year_Built, Mo_Sold, Lot_Area, Street, Central_Air,\n         First_Flr_SF, Second_Flr_SF, Full_Bath, Half_Bath, Fireplaces,\n         Garage_Area, Gr_Liv_Area, TotRms_AbvGrd)\n\ntraining.df <- as.data.frame(training)\n```\n:::\n\n\n## Decision Tree (Baseline)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\ntree.ames <- rpart(Sale_Price ~ ., data = training.df, method = \"anova\",\n                   control = rpart.control(cp = 0.005))\n\nrpart.plot(tree.ames, type = 2, fallen.leaves = TRUE, cex = 0.6,\n           main = \"Decision Tree for Ames Housing\")\n```\n\n::: {.cell-output-display}\n![](Tree_Based_R_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntesting_sel <- testing %>% select(names(training.df))\npred_tree <- predict(tree.ames, newdata = testing_sel)\n\ntree_perf <- testing_sel %>% \n  mutate(pred = pred_tree,\n         AE = abs(Sale_Price - pred),\n         APE = 100*abs((Sale_Price - pred)/Sale_Price)) %>%\n  summarise(Model = \"Decision Tree\",\n            RMSE = sqrt(mean((Sale_Price - pred)^2)),\n            MAE = mean(AE),\n            MAPE = mean(APE),\n            R2 = 1 - sum((Sale_Price - pred)^2)/sum((Sale_Price - mean(Sale_Price))^2))\ntree_perf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  Model           RMSE    MAE  MAPE    R2\n  <chr>          <dbl>  <dbl> <dbl> <dbl>\n1 Decision Tree 42938. 28322.  17.5 0.727\n```\n\n\n:::\n:::\n\n\n## Random Forest: Random Variable, PDPs, and Importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add random variable for feature selection sanity check\ntraining.df$random <- rnorm(nrow(training.df))\n\nset.seed(12345)\nrf.ames <- randomForest(Sale_Price ~ ., data = training.df, ntree = 500, mtry = 4, importance = TRUE)\nplot(rf.ames, main = \"Random Forest: Error vs Trees\")\n```\n\n::: {.cell-output-display}\n![](Tree_Based_R_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nvarImpPlot(rf.ames, sort = TRUE, n.var = 15, main = \"Random Forest: Variables vs Random Baseline\")\n```\n\n::: {.cell-output-display}\n![](Tree_Based_R_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code}\nimportance(rf.ames)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 %IncMSE IncNodePurity\nBedroom_AbvGr 16.7830622  2.081292e+11\nYear_Built    59.6827740  2.608576e+12\nMo_Sold        0.6717216  1.820404e+11\nLot_Area      16.2410098  5.818823e+11\nStreet         4.6451725  5.141533e+09\nCentral_Air   15.9709319  1.023533e+11\nFirst_Flr_SF  37.4760827  1.884177e+12\nSecond_Flr_SF 24.6324055  6.159270e+11\nFull_Bath     12.3716046  6.191162e+11\nHalf_Bath     13.6183708  1.153802e+11\nFireplaces    26.1893181  5.796209e+11\nGarage_Area   28.4006898  1.961373e+12\nGr_Liv_Area   35.1279551  2.445543e+12\nTotRms_AbvGrd 13.9865453  4.189078e+11\nrandom         1.2436860  1.937137e+11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Partial dependence for key variables\npartialPlot(rf.ames, training.df, Year_Built, main = \"Partial Dependence: Year_Built (RF)\")\n```\n\n::: {.cell-output-display}\n![](Tree_Based_R_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n\n```{.r .cell-code}\npartialPlot(rf.ames, training.df, Garage_Area, main = \"Partial Dependence: Garage_Area (RF)\")\n```\n\n::: {.cell-output-display}\n![](Tree_Based_R_files/figure-html/unnamed-chunk-4-4.png){width=672}\n:::\n:::\n\n\n### Random Forest Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntesting_sel$random <- rnorm(nrow(testing_sel))\ntesting_sel$pred_rf <- predict(rf.ames, testing_sel)\n\nrf_perf <- testing_sel %>% \n  mutate(AE = abs(Sale_Price - pred_rf),\n         APE = 100*abs((Sale_Price - pred_rf)/Sale_Price)) %>%\n  summarise(Model = \"Random Forest\",\n            RMSE = sqrt(mean((Sale_Price - pred_rf)^2)),\n            MAE = mean(AE),\n            MAPE = mean(APE),\n            R2 = 1 - sum((Sale_Price - pred_rf)^2)/sum((Sale_Price - mean(Sale_Price))^2))\nrf_perf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  Model           RMSE    MAE  MAPE    R2\n  <chr>          <dbl>  <dbl> <dbl> <dbl>\n1 Random Forest 30147. 19223.  11.6 0.866\n```\n\n\n:::\n:::\n\n\n## XGBoost: Random Variable, CV, Tuning, PDPs\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraining$random <- rnorm(nrow(training))\ntrain_x <- model.matrix(Sale_Price ~ ., data = training)[, -1]\ntrain_y <- training$Sale_Price\n\nset.seed(12345)\nxgb.ames <- xgboost(\n  data = train_x, label = train_y, subsample = 0.5, nrounds = 100,\n  objective = \"reg:squarederror\", verbose = 0\n)\n\nxgbcv.ames <- xgb.cv(\n  data = train_x, label = train_y, subsample = 0.5, nrounds = 100,\n  nfold = 10, objective = \"reg:squarederror\", verbose = 0\n)\n\ntune_grid <- expand.grid(\n  nrounds = 24,\n  eta = c(0.1, 0.15, 0.2, 0.25, 0.3),\n  max_depth = c(1:10),\n  gamma = c(0),\n  colsample_bytree = 1,\n  min_child_weight = 1,\n  subsample = c(0.25, 0.5, 0.75, 1)\n)\n\nset.seed(12345)\nxgb.ames.caret <- train(\n  x = train_x, y = train_y,\n  method = \"xgbTree\",\n  tuneGrid = tune_grid,\n  trControl = trainControl(method = 'cv', number = 10)\n)\nplot(xgb.ames.caret)\n```\n\n::: {.cell-output-display}\n![](Tree_Based_R_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Fit tuned model\nxgb.ames <- xgboost(\n  data = train_x, label = train_y, subsample = 1,\n  nrounds = 24, eta = 0.25, max_depth = 5,\n  objective = \"reg:squarederror\", verbose = 0\n)\n\n# Variable importance including random variable\nxgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb.ames))\n```\n\n::: {.cell-output-display}\n![](Tree_Based_R_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Partial dependence plots\ntrain_df <- as.data.frame(train_x)\n\npartial(xgb.ames, pred.var = \"Year_Built\",\n        plot = TRUE, rug = TRUE, alpha = 0.1, plot.engine = \"lattice\",\n        train = train_df, pdp.color = \"red\")\n```\n\n::: {.cell-output-display}\n![](Tree_Based_R_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n\n```{.r .cell-code}\npartial(xgb.ames, pred.var = \"Garage_Area\",\n        plot = TRUE, rug = TRUE, alpha = 0.1, plot.engine = \"lattice\",\n        train = train_df)\n```\n\n::: {.cell-output-display}\n![](Tree_Based_R_files/figure-html/unnamed-chunk-6-4.png){width=672}\n:::\n:::\n\n\n### XGBoost Results (Aligned Columns)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntesting$random <- rnorm(nrow(testing))\ntesting_x <- model.matrix(Sale_Price ~ ., data = testing)[, -1]\n\n# Align test columns to training columns\nmissing_cols <- setdiff(colnames(train_x), colnames(testing_x))\nfor (col in missing_cols) {\n  testing_x <- cbind(testing_x, 0)\n  colnames(testing_x)[ncol(testing_x)] <- col\n}\ntesting_x <- testing_x[, colnames(train_x)]\n\ntesting_sel$pred_xgb <- predict(xgb.ames, testing_x)\n\nxgb_perf <- testing_sel %>% \n  mutate(AE = abs(Sale_Price - pred_xgb),\n         APE = 100*abs((Sale_Price - pred_xgb)/Sale_Price)) %>%\n  summarise(Model = \"XGBoost\",\n            RMSE = sqrt(mean((Sale_Price - pred_xgb)^2)),\n            MAE = mean(AE),\n            MAPE = mean(APE),\n            R2 = 1 - sum((Sale_Price - pred_xgb)^2)/sum((Sale_Price - mean(Sale_Price))^2))\nxgb_perf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  Model     RMSE    MAE  MAPE    R2\n  <chr>    <dbl>  <dbl> <dbl> <dbl>\n1 XGBoost 30355. 19820.  11.8 0.864\n```\n\n\n:::\n:::\n\n\n## Final Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbind_rows(tree_perf, rf_perf, xgb_perf) %>%\n  arrange(RMSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  Model           RMSE    MAE  MAPE    R2\n  <chr>          <dbl>  <dbl> <dbl> <dbl>\n1 Random Forest 30147. 19223.  11.6 0.866\n2 XGBoost       30355. 19820.  11.8 0.864\n3 Decision Tree 42938. 28322.  17.5 0.727\n```\n\n\n:::\n:::\n\n\n> **Summary:** The Decision Tree provides interpretability, Random Forest and XGBoost attempt to further reduce bias and manage variance error through randomization and ensembles of weak learners. The issue is that they become less and less interpretable, so we have to introduce other tools to understand what variables are important and how they are relating to the outcome.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}