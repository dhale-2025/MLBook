{
  "hash": "28f4f785921127b1da9190a4184cbd69",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Model Selection and Regularization - Python Version\"\nformat: html\ntoc: true\ncode-fold: true\ncode-tools: false\n---\n\n## Overview\n\nThis section parallels the R version of Model Selection and Regularization.\\\nWe will:\n\n1.  Load the Ames Housing data\\\n2.  Split into training and test sets\\\n3.  Perform Ridge, Lasso, and Elastic Net regressions with 10-fold CV\\\n4.  Visualize RMSE and coefficient shrinkage patterns\\\n5.  Compare test set performance\n\n------------------------------------------------------------------------\n\n## Step 0: Setup and Data\n\nWe’ll use scikit-learn for modeling.\\\nIf the CSVs `ames_training.csv` and `ames_testing.csv` are not present, you can load Ames Housing via the `fetch_openml` function.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error\n\n# Load Ames dataset directly\names = fetch_openml(name=\"house_prices\", as_frame=True)\ndf = ames.frame\n\n# Subset variables to match the R version\nkeep = [\n    \"SalePrice\",\"BedroomAbvGr\",\"YearBuilt\",\"MoSold\",\"LotArea\",\"Street\",\"CentralAir\",\n    \"1stFlrSF\",\"2ndFlrSF\",\"FullBath\",\"HalfBath\",\"Fireplaces\",\"GarageArea\",\n    \"GrLivArea\",\"TotRmsAbvGrd\"\n]\ndf = df[keep].copy()\n\n# Clean up names to match Python variable rules\ndf.columns = [\"Sale_Price\",\"Bedroom_AbvGr\",\"Year_Built\",\"Mo_Sold\",\"Lot_Area\",\"Street\",\n              \"Central_Air\",\"First_Flr_SF\",\"Second_Flr_SF\",\"Full_Bath\",\"Half_Bath\",\n              \"Fireplaces\",\"Garage_Area\",\"Gr_Liv_Area\",\"TotRms_AbvGrd\"]\n\n# Drop missing rows\ndf = df.dropna()\n\n# Train/test split (70/30)\ntrain, test = train_test_split(df, test_size=0.3, random_state=123)\n\n# Identify predictors and target\ny_train = train[\"Sale_Price\"]\ny_test  = test[\"Sale_Price\"]\nX_train = train.drop(columns=\"Sale_Price\")\nX_test  = test.drop(columns=\"Sale_Price\")\n\ncat_vars = [\"Street\",\"Central_Air\"]\nnum_vars = [c for c in X_train.columns if c not in cat_vars]\n\n# Preprocess: scale numeric, one-hot encode categorical\npreprocessor = ColumnTransformer([\n    (\"num\", StandardScaler(), num_vars),\n    (\"cat\", OneHotEncoder(drop=\"first\"), cat_vars)\n])\n```\n:::\n\n\n------------------------------------------------------------------------\n\n## Step 1: Ridge Regression\n\nWe’ll use cross-validation to tune λ (alpha).\\\nThen we visualize RMSE vs log(lambda) and the number of nonzero coefficients.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nalphas = np.logspace(4, -4, 80)\nridge = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", RidgeCV(alphas=alphas, scoring=\"neg_mean_squared_error\", cv=10))\n])\nridge.fit(X_train, y_train)\n\nridge_best = ridge.named_steps[\"model\"].alpha_\nprint(\"Best Ridge alpha:\", ridge_best)\n\n# Evaluate RMSE on test set\nridge_pred = ridge.predict(X_test)\nridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))\nprint(\"Test RMSE (Ridge):\", ridge_rmse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Ridge alpha: 94.33732216299774\nTest RMSE (Ridge): 37055.84511170759\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Step 2: Lasso Regression\n\nLasso adds variable selection — some coefficients go exactly to zero.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nlasso = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", LassoCV(alphas=alphas, cv=10, max_iter=20000))\n])\nlasso.fit(X_train, y_train)\n\nlasso_best = lasso.named_steps[\"model\"].alpha_\nprint(\"Best Lasso alpha:\", lasso_best)\n\nlasso_pred = lasso.predict(X_test)\nlasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_pred))\nprint(\"Test RMSE (Lasso):\", lasso_rmse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Lasso alpha: 11.568875283162821\nTest RMSE (Lasso): 37074.74978063246\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Step 3: Visualize Coefficient Shrinkage and RMSE\n\nLet’s visualize how RMSE and model sparsity change with λ.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Compute RMSE path manually\ndef cv_rmse(model_class, X, y, alphas):\n    rmse = []\n    for a in alphas:\n        model = Pipeline([\n            (\"prep\", preprocessor),\n            (\"model\", model_class(alpha=a, max_iter=20000))\n        ])\n        scores = cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=KFold(10, shuffle=True, random_state=123))\n        rmse.append(np.sqrt(-scores.mean()))\n    return np.array(rmse)\n\nfrom sklearn.linear_model import Ridge, Lasso\nridge_rmse_path = cv_rmse(Ridge, X_train, y_train, alphas)\nlasso_rmse_path = cv_rmse(Lasso, X_train, y_train, alphas)\n\nplt.figure()\nplt.plot(np.log(alphas), ridge_rmse_path, label=\"Ridge\")\nplt.plot(np.log(alphas), lasso_rmse_path, label=\"Lasso\")\nplt.xlabel(\"log(lambda)\")\nplt.ylabel(\"Cross-validated RMSE\")\nplt.title(\"RMSE vs log(lambda)\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](resampling-model-selection-python_files/figure-pdf/cell-5-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Step 4: Elastic Net\n\nElastic Net blends Ridge and Lasso.\\\nWe’ll vary α (the mix) and visualize the tradeoff.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nl1_ratios = np.linspace(0, 1, 6)\nen_cv = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", ElasticNetCV(l1_ratio=l1_ratios, alphas=alphas, cv=10, max_iter=50000))\n])\nen_cv.fit(X_train, y_train)\n\nprint(\"Best Elastic Net alpha:\", en_cv.named_steps[\"model\"].alpha_)\nprint(\"Best Elastic Net l1_ratio:\", en_cv.named_steps[\"model\"].l1_ratio_)\n\nen_pred = en_cv.predict(X_test)\nen_rmse = np.sqrt(mean_squared_error(y_test, en_pred))\nprint(\"Test RMSE (Elastic Net):\", en_rmse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Elastic Net alpha: 0.55825862688627\nBest Elastic Net l1_ratio: 0.8\nTest RMSE (Elastic Net): 37134.497858434275\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Step 5: Compare Models\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nresults = pd.DataFrame({\n    \"Model\": [\"Ridge\", \"Lasso\", \"Elastic Net\"],\n    \"Test_RMSE\": [ridge_rmse, lasso_rmse, en_rmse]\n})\nprint(results.sort_values(\"Test_RMSE\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Model     Test_RMSE\n0        Ridge  37055.845112\n1        Lasso  37074.749781\n2  Elastic Net  37134.497858\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Summary\n\n-   Ridge stabilizes coefficients by shrinking them.\\\n-   Lasso enforces sparsity by zeroing weak predictors.\\\n-   Elastic Net blends both for balance.\\\n-   Regularization often beats pure stepwise regression on unseen data.\n\n",
    "supporting": [
      "resampling-model-selection-python_files/figure-pdf"
    ],
    "filters": []
  }
}