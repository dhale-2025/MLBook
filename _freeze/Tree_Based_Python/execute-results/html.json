{
  "hash": "706c09eb5299e89d55b422a54140b976",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tree Based Models - Python Version\"\nformat:\n  html:\n    toc: true\n    number-sections: true\n    code-fold: true\n  pdf:\n    number-sections: true\n    toc: true\n---\n\n> Python version of the ensemble workflow: **Decision Tree**, **Random Forest**, and **XGBoost** on the Ames Housing dataset. Includes a random variable baseline, variable importance, partial dependence plots, and RÂ²-based model comparison.\n\n## Visual Roadmap\n\n```{mermaid}\nflowchart LR\n    A[\"Data (Ames)\"]\n    DT[\"Decision Tree (Baseline)\"]\n    RF[\"Random Forest (Bagging + Random Features)\"]\n    XGB[\"XGBoost (Boosted Trees + Regularization)\"]\n    Compare[\"Model Comparison\"]\n    \n    A --> DT\n    A --> RF\n    A --> XGB\n    DT --> Compare\n    RF --> Compare\n    XGB --> Compare\n```\n\n## Setup and Data Preparation\n\n::: {#84a2963e .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom xgboost import XGBRegressor, plot_importance\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Load Ames Housing dataset from OpenML\names = fetch_openml(name=\"house_prices\", as_frame=True)\ndf = ames.frame\n\n# Select subset of features similar to R version\nfeatures = [\n    'BedroomAbvGr', 'YearBuilt', 'MoSold', 'LotArea', 'Street', 'CentralAir',\n    '1stFlrSF', '2ndFlrSF', 'FullBath', 'HalfBath', 'Fireplaces',\n    'GarageArea', 'GrLivArea', 'TotRmsAbvGrd'\n]\ndf = df[features + ['SalePrice']].dropna()\n\n# Encode categorical variables\ndf = pd.get_dummies(df, drop_first=True)\n\n# Train/test split\ntrain, test = train_test_split(df, test_size=0.3, random_state=4321)\n\n# Add random variable\ntrain['random'] = np.random.randn(len(train))\ntest['random'] = np.random.randn(len(test))\n\nX_train = train.drop(columns=['SalePrice'])\ny_train = train['SalePrice']\nX_test = test.drop(columns=['SalePrice'])\ny_test = test['SalePrice']\n```\n:::\n\n\n## Decision Tree (Baseline)\n\n::: {#d29ccd4d .cell execution_count=2}\n``` {.python .cell-code}\ntree = DecisionTreeRegressor(random_state=12345, max_depth=5)\ntree.fit(X_train, y_train)\n\n# Visualize tree structure\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(16, 8))\nplot_tree(tree, feature_names=X_train.columns, filled=True, max_depth=3)\nplt.title(\"Decision Tree for Ames Housing\")\nplt.show()\n\n# Predictions and metrics\npred_tree = tree.predict(X_test)\nrmse_tree = mean_squared_error(y_test, pred_tree)\nmae_tree = mean_absolute_error(y_test, pred_tree)\nmape_tree = np.mean(np.abs((y_test - pred_tree) / y_test)) * 100\nr2_tree = r2_score(y_test, pred_tree)\n\npd.DataFrame([[\"Decision Tree\", rmse_tree, mae_tree, mape_tree, r2_tree]],\n             columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R2\"])\n```\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python_files/figure-html/cell-3-output-1.png){width=1210 height=631}\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Decision Tree</td>\n      <td>3.366549e+09</td>\n      <td>32011.591014</td>\n      <td>17.550378</td>\n      <td>0.528022</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Random Forest: Random Variable, Importance, PDPs\n\n::: {#7be0ddd8 .cell execution_count=3}\n``` {.python .cell-code}\nrf = RandomForestRegressor(n_estimators=500, max_features=4, random_state=12345)\nrf.fit(X_train, y_train)\n\n# Feature importance including random variable\nimportances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nimportances[:15].plot(kind='barh', title='Random Forest: Top Variables (with Random Baseline)', figsize=(8,6))\nplt.show()\n\n# Partial dependence plots\nPartialDependenceDisplay.from_estimator(rf, X_train, ['YearBuilt', 'GarageArea'], kind='average')\nplt.suptitle(\"Partial Dependence (Random Forest)\")\nplt.show()\n\n# Predictions and metrics\npred_rf = rf.predict(X_test)\nrmse_rf = mean_squared_error(y_test, pred_rf)\nmae_rf = mean_absolute_error(y_test, pred_rf)\nmape_rf = np.mean(np.abs((y_test - pred_rf) / y_test)) * 100\nr2_rf = r2_score(y_test, pred_rf)\n\npd.DataFrame([[\"Random Forest\", rmse_rf, mae_rf, mape_rf, r2_rf]],\n             columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R2\"])\n```\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python_files/figure-html/cell-4-output-1.png){width=725 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python_files/figure-html/cell-4-output-2.png){width=619 height=477}\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Random Forest</td>\n      <td>1.484355e+09</td>\n      <td>22357.412411</td>\n      <td>12.267671</td>\n      <td>0.791899</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## XGBoost: Random Variable, Importance, PDPs\n\n::: {#3b80afe3 .cell execution_count=4}\n``` {.python .cell-code}\nxgb = XGBRegressor(\n    n_estimators=100, learning_rate=0.25, max_depth=5, subsample=1,\n    objective=\"reg:squarederror\", random_state=12345, n_jobs=-1\n)\nxgb.fit(X_train, y_train)\n\n# Feature importance including random variable\nplt.figure(figsize=(8,6))\nplot_importance(xgb, max_num_features=15, importance_type='gain', title='XGBoost: Variable Importance (with Random Baseline)')\nplt.show()\n\n# Partial dependence plots\nPartialDependenceDisplay.from_estimator(xgb, X_train, ['YearBuilt', 'GarageArea'], kind='average')\nplt.suptitle(\"Partial Dependence (XGBoost)\")\nplt.show()\n\n# Predictions and metrics\npred_xgb = xgb.predict(X_test)\nrmse_xgb = mean_squared_error(y_test, pred_xgb)\nmae_xgb = mean_absolute_error(y_test, pred_xgb)\nmape_xgb = np.mean(np.abs((y_test - pred_xgb) / y_test)) * 100\nr2_xgb = r2_score(y_test, pred_xgb)\n\npd.DataFrame([[\"XGBoost\", rmse_xgb, mae_xgb, mape_xgb, r2_xgb]],\n             columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R2\"])\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 768x576 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python_files/figure-html/cell-5-output-2.png){width=728 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python_files/figure-html/cell-5-output-3.png){width=619 height=477}\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>XGBoost</td>\n      <td>1.545215e+09</td>\n      <td>23977.53125</td>\n      <td>13.258681</td>\n      <td>0.783366</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Final Model Comparison\n\n::: {#e2aa646f .cell execution_count=5}\n``` {.python .cell-code}\nresults = pd.DataFrame([\n    [\"Decision Tree\", rmse_tree, mae_tree, mape_tree, r2_tree],\n    [\"Random Forest\", rmse_rf, mae_rf, mape_rf, r2_rf],\n    [\"XGBoost\", rmse_xgb, mae_xgb, mape_xgb, r2_xgb]\n], columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R2\"]).sort_values(\"RMSE\")\nresults\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Random Forest</td>\n      <td>1.484355e+09</td>\n      <td>22357.412411</td>\n      <td>12.267671</td>\n      <td>0.791899</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGBoost</td>\n      <td>1.545215e+09</td>\n      <td>23977.531250</td>\n      <td>13.258681</td>\n      <td>0.783366</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Decision Tree</td>\n      <td>3.366549e+09</td>\n      <td>32011.591014</td>\n      <td>17.550378</td>\n      <td>0.528022</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n> ðŸ§© **Summary:** The Decision Tree offers interpretability; Random Forest reduces variance; and XGBoost improves bias and predictive power â€” typically achieving the best RMSE and RÂ².\n\n",
    "supporting": [
      "Tree_Based_Python_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}