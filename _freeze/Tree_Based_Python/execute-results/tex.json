{
  "hash": "706c09eb5299e89d55b422a54140b976",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tree Based Models - Python Version\"\nformat:\n  html:\n    toc: true\n    number-sections: true\n    code-fold: true\n  pdf:\n    number-sections: true\n    toc: true\n---\n\n> Python version of the ensemble workflow: **Decision Tree**, **Random Forest**, and **XGBoost** on the Ames Housing dataset. Includes a random variable baseline, variable importance, partial dependence plots, and RÂ²-based model comparison.\n\n## Visual Roadmap\n\n```{mermaid}\nflowchart LR\n    A[\"Data (Ames)\"]\n    DT[\"Decision Tree (Baseline)\"]\n    RF[\"Random Forest (Bagging + Random Features)\"]\n    XGB[\"XGBoost (Boosted Trees + Regularization)\"]\n    Compare[\"Model Comparison\"]\n    \n    A --> DT\n    A --> RF\n    A --> XGB\n    DT --> Compare\n    RF --> Compare\n    XGB --> Compare\n```\n\n## Setup and Data Preparation\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom xgboost import XGBRegressor, plot_importance\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Load Ames Housing dataset from OpenML\names = fetch_openml(name=\"house_prices\", as_frame=True)\ndf = ames.frame\n\n# Select subset of features similar to R version\nfeatures = [\n    'BedroomAbvGr', 'YearBuilt', 'MoSold', 'LotArea', 'Street', 'CentralAir',\n    '1stFlrSF', '2ndFlrSF', 'FullBath', 'HalfBath', 'Fireplaces',\n    'GarageArea', 'GrLivArea', 'TotRmsAbvGrd'\n]\ndf = df[features + ['SalePrice']].dropna()\n\n# Encode categorical variables\ndf = pd.get_dummies(df, drop_first=True)\n\n# Train/test split\ntrain, test = train_test_split(df, test_size=0.3, random_state=4321)\n\n# Add random variable\ntrain['random'] = np.random.randn(len(train))\ntest['random'] = np.random.randn(len(test))\n\nX_train = train.drop(columns=['SalePrice'])\ny_train = train['SalePrice']\nX_test = test.drop(columns=['SalePrice'])\ny_test = test['SalePrice']\n```\n:::\n\n\n## Decision Tree (Baseline)\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ntree = DecisionTreeRegressor(random_state=12345, max_depth=5)\ntree.fit(X_train, y_train)\n\n# Visualize tree structure\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(16, 8))\nplot_tree(tree, feature_names=X_train.columns, filled=True, max_depth=3)\nplt.title(\"Decision Tree for Ames Housing\")\nplt.show()\n\n# Predictions and metrics\npred_tree = tree.predict(X_test)\nrmse_tree = mean_squared_error(y_test, pred_tree)\nmae_tree = mean_absolute_error(y_test, pred_tree)\nmape_tree = np.mean(np.abs((y_test - pred_tree) / y_test)) * 100\nr2_tree = r2_score(y_test, pred_tree)\n\npd.DataFrame([[\"Decision Tree\", rmse_tree, mae_tree, mape_tree, r2_tree]],\n             columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R2\"])\n```\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python_files/figure-pdf/cell-3-output-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Decision Tree</td>\n      <td>3.489195e+09</td>\n      <td>32677.487894</td>\n      <td>17.895749</td>\n      <td>0.510827</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Random Forest: Random Variable, Importance, PDPs\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nrf = RandomForestRegressor(n_estimators=500, max_features=4, random_state=12345)\nrf.fit(X_train, y_train)\n\n# Feature importance including random variable\nimportances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nimportances[:15].plot(kind='barh', title='Random Forest: Top Variables (with Random Baseline)', figsize=(8,6))\nplt.show()\n\n# Partial dependence plots\nPartialDependenceDisplay.from_estimator(rf, X_train, ['YearBuilt', 'GarageArea'], kind='average')\nplt.suptitle(\"Partial Dependence (Random Forest)\")\nplt.show()\n\n# Predictions and metrics\npred_rf = rf.predict(X_test)\nrmse_rf = mean_squared_error(y_test, pred_rf)\nmae_rf = mean_absolute_error(y_test, pred_rf)\nmape_rf = np.mean(np.abs((y_test - pred_rf) / y_test)) * 100\nr2_rf = r2_score(y_test, pred_rf)\n\npd.DataFrame([[\"Random Forest\", rmse_rf, mae_rf, mape_rf, r2_rf]],\n             columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R2\"])\n```\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python_files/figure-pdf/cell-4-output-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python_files/figure-pdf/cell-4-output-2.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Random Forest</td>\n      <td>1.474364e+09</td>\n      <td>22149.606041</td>\n      <td>12.195379</td>\n      <td>0.793299</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## XGBoost: Random Variable, Importance, PDPs\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nxgb = XGBRegressor(\n    n_estimators=100, learning_rate=0.25, max_depth=5, subsample=1,\n    objective=\"reg:squarederror\", random_state=12345, n_jobs=-1\n)\nxgb.fit(X_train, y_train)\n\n# Feature importance including random variable\nplt.figure(figsize=(8,6))\nplot_importance(xgb, max_num_features=15, importance_type='gain', title='XGBoost: Variable Importance (with Random Baseline)')\nplt.show()\n\n# Partial dependence plots\nPartialDependenceDisplay.from_estimator(xgb, X_train, ['YearBuilt', 'GarageArea'], kind='average')\nplt.suptitle(\"Partial Dependence (XGBoost)\")\nplt.show()\n\n# Predictions and metrics\npred_xgb = xgb.predict(X_test)\nrmse_xgb = mean_squared_error(y_test, pred_xgb)\nmae_xgb = mean_absolute_error(y_test, pred_xgb)\nmape_xgb = np.mean(np.abs((y_test - pred_xgb) / y_test)) * 100\nr2_xgb = r2_score(y_test, pred_xgb)\n\npd.DataFrame([[\"XGBoost\", rmse_xgb, mae_xgb, mape_xgb, r2_xgb]],\n             columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R2\"])\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 2400x1800 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python_files/figure-pdf/cell-5-output-2.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python_files/figure-pdf/cell-5-output-3.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>XGBoost</td>\n      <td>1.590730e+09</td>\n      <td>24369.324219</td>\n      <td>13.379384</td>\n      <td>0.776985</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Final Model Comparison\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nresults = pd.DataFrame([\n    [\"Decision Tree\", rmse_tree, mae_tree, mape_tree, r2_tree],\n    [\"Random Forest\", rmse_rf, mae_rf, mape_rf, r2_rf],\n    [\"XGBoost\", rmse_xgb, mae_xgb, mape_xgb, r2_xgb]\n], columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R2\"]).sort_values(\"RMSE\")\nresults\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Random Forest</td>\n      <td>1.474364e+09</td>\n      <td>22149.606041</td>\n      <td>12.195379</td>\n      <td>0.793299</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGBoost</td>\n      <td>1.590730e+09</td>\n      <td>24369.324219</td>\n      <td>13.379384</td>\n      <td>0.776985</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Decision Tree</td>\n      <td>3.489195e+09</td>\n      <td>32677.487894</td>\n      <td>17.895749</td>\n      <td>0.510827</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n> ðŸ§© **Summary:** The Decision Tree offers interpretability; Random Forest reduces variance; and XGBoost improves bias and predictive power â€” typically achieving the best RMSE and RÂ².\n\n",
    "supporting": [
      "Tree_Based_Python_files/figure-pdf"
    ],
    "filters": []
  }
}