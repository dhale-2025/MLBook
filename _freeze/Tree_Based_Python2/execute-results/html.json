{
  "hash": "d9f8932d6e846d7ef4e33c5aeac994fe",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tree Based Models - Python Version\"\nformat:\n  html:\n    toc: true\n    number-sections: true\n    code-fold: true\n  pdf:\n    number-sections: true\n    toc: true\n---\n\n> Tree Models: **Decision Tree**, **Random Forest**, and **XGBoost** on the Ames Housing dataset. Includes random-variable baselines, variable importance, partial dependence plots, and hyperparameter tuning (random search). \n\n## Visual Roadmap\n\n```{mermaid}\nflowchart LR\n    A[\"Data (Ames)\"]\n    DT[\"Decision Tree (Baseline)\"]\n    RF[\"Random Forest (Bagging + Random Features)\"]\n    XGB[\"XGBoost (Boosted Trees + Regularization)\"]\n    Compare[\"Model Comparison\"]\n    \n    A --> DT\n    A --> RF\n    A --> XGB\n    DT --> Compare\n    RF --> Compare\n    XGB --> Compare\n```\n\n## Setup and Data Preparation\n\n::: {#f3f05d8a .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom xgboost import XGBRegressor, plot_importance\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set random seed for reproducibility\nseed = 12345\nnp.random.seed(seed)\n\n# Load Ames dataset\names = fetch_openml(name=\"house_prices\", as_frame=True)\ndf = ames.frame\n\n# Feature selection matching R version\nfeatures = [\n    'BedroomAbvGr', 'YearBuilt', 'MoSold', 'LotArea', 'Street', 'CentralAir',\n    '1stFlrSF', '2ndFlrSF', 'FullBath', 'HalfBath', 'Fireplaces',\n    'GarageArea', 'GrLivArea', 'TotRmsAbvGrd'\n]\ndf = df[features + ['SalePrice']].dropna()\n\n# Encode categoricals\ndf = pd.get_dummies(df, drop_first=True)\n\n# Train/test split\ntrain, test = train_test_split(df, test_size=0.3, random_state=seed)\n\n# Add random variable for variable-importance baseline\ntrain['random'] = np.random.randn(len(train))\ntest['random'] = np.random.randn(len(test))\n\nX_train = train.drop(columns=['SalePrice'])\ny_train = train['SalePrice']\nX_test = test.drop(columns=['SalePrice'])\ny_test = test['SalePrice']\n```\n:::\n\n\n## Decision Tree (Baseline)\n\n::: {#a22719ea .cell execution_count=3}\n``` {.python .cell-code}\ntree = DecisionTreeRegressor(random_state=seed, max_depth=5)\ntree.fit(X_train, y_train)\n\nplt.figure(figsize=(16,8))\nplot_tree(tree, feature_names=X_train.columns, filled=True, max_depth=3)\nplt.title(\"Decision Tree for Ames Housing\")\nplt.show()\n\n# Metrics\npred_tree = tree.predict(X_test)\nrmse_tree = mean_squared_error(y_test, pred_tree)\nmae_tree = mean_absolute_error(y_test, pred_tree)\nmape_tree = np.mean(np.abs((y_test - pred_tree) / y_test)) * 100\nr2_tree = r2_score(y_test, pred_tree)\n\npd.DataFrame([[\"Decision Tree\", rmse_tree, mae_tree, mape_tree, r2_tree]],\n             columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R2\"])\n```\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python2_files/figure-html/cell-3-output-1.png){width=1210 height=631}\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Decision Tree</td>\n      <td>2.378377e+09</td>\n      <td>28465.348099</td>\n      <td>17.701235</td>\n      <td>0.522975</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Random Forest: Random Variable, Importance, PDPs, and Tuning\n\n::: {#014e457b .cell execution_count=4}\n``` {.python .cell-code}\nrf = RandomForestRegressor(random_state=seed)\n\n# Define parameter grid (similar to R tuning)\nparam_dist_rf = {\n    'n_estimators': [100, 200, 300, 400, 500],\n    'max_features': ['sqrt', 'log2', None, 4, 6, 8],\n    'max_depth': [None, 5, 10, 15, 20],\n    'min_samples_split': [2, 5, 10]\n}\n\ncv = KFold(n_splits=10, shuffle=True, random_state=seed)\n\nrf_random = RandomizedSearchCV(\n    estimator=rf,\n    param_distributions=param_dist_rf,\n    n_iter=20,\n    cv=cv,\n    random_state=seed,\n    n_jobs=-1,\n    scoring='neg_root_mean_squared_error'\n)\n\nrf_random.fit(X_train, y_train)\nrf_best = rf_random.best_estimator_\nprint(\"Best Random Forest Parameters:\", rf_random.best_params_)\n\n# Importance with random variable\nimportances = pd.Series(rf_best.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nimportances[:15].plot(kind='barh', title='Random Forest: Top Variables (with Random Baseline)')\nplt.show()\n\n# Partial dependence plots\nplt.figure()\nPartialDependenceDisplay.from_estimator(rf_best, X_train, ['YearBuilt', 'GarageArea'])\nplt.suptitle(\"Partial Dependence (Random Forest)\")\nplt.show()\n\n# Performance metrics\npred_rf = rf_best.predict(X_test)\nrmse_rf = mean_squared_error(y_test, pred_rf)\nmae_rf = mean_absolute_error(y_test, pred_rf)\nmape_rf = np.mean(np.abs((y_test - pred_rf) / y_test)) * 100\nr2_rf = r2_score(y_test, pred_rf)\n\npd.DataFrame([[\"Random Forest (Tuned)\", rmse_rf, mae_rf, mape_rf, r2_rf]],\n             columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R2\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Random Forest Parameters: {'n_estimators': 500, 'min_samples_split': 2, 'max_features': 4, 'max_depth': 20}\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python2_files/figure-html/cell-4-output-2.png){width=651 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python2_files/figure-html/cell-4-output-4.png){width=619 height=477}\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Random Forest (Tuned)</td>\n      <td>1.093313e+09</td>\n      <td>19889.772151</td>\n      <td>12.832756</td>\n      <td>0.780717</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## XGBoost: Random Variable, Importance, PDPs, and Tuning\n\n::: {#336d7f85 .cell execution_count=5}\n``` {.python .cell-code}\nxgb = XGBRegressor(\n    objective='reg:squarederror',\n    random_state=seed,\n    n_jobs=-1\n)\n\n# Define tuning grid similar to R (eta = learning_rate, subsample, max_depth)\nparam_dist_xgb = {\n    'n_estimators': [24, 50, 100, 200],\n    'learning_rate': [0.1, 0.15, 0.2, 0.25, 0.3],\n    'max_depth': [1, 3, 5, 7, 10],\n    'subsample': [0.25, 0.5, 0.75, 1],\n    'colsample_bytree': [0.5, 0.75, 1]\n}\n\nxgb_random = RandomizedSearchCV(\n    estimator=xgb,\n    param_distributions=param_dist_xgb,\n    n_iter=20,\n    cv=cv,\n    random_state=seed,\n    n_jobs=-1,\n    scoring='neg_root_mean_squared_error'\n)\n\nxgb_random.fit(X_train, y_train)\nxgb_best = xgb_random.best_estimator_\nprint(\"Best XGBoost Parameters:\", xgb_random.best_params_)\n\n# Importance plot with random variable\nplot_importance(xgb_best, max_num_features=15, importance_type='gain')\nplt.title(\"XGBoost: Variable Importance (with Random Baseline)\")\nplt.show()\n\n# Partial dependence plots\nplt.figure()\nPartialDependenceDisplay.from_estimator(xgb_best, X_train, ['YearBuilt', 'GarageArea'])\nplt.suptitle(\"Partial Dependence (XGBoost)\")\nplt.show()\n\n# Metrics\npred_xgb = xgb_best.predict(X_test)\nrmse_xgb = mean_squared_error(y_test, pred_xgb)\nmae_xgb = mean_absolute_error(y_test, pred_xgb)\nmape_xgb = np.mean(np.abs((y_test - pred_xgb) / y_test)) * 100\nr2_xgb = r2_score(y_test, pred_xgb)\n\npd.DataFrame([[\"XGBoost (Tuned)\", rmse_xgb, mae_xgb, mape_xgb, r2_xgb]],\n             columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R2\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest XGBoost Parameters: {'subsample': 0.75, 'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.15, 'colsample_bytree': 0.5}\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python2_files/figure-html/cell-5-output-2.png){width=728 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tree_Based_Python2_files/figure-html/cell-5-output-4.png){width=619 height=477}\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>XGBoost (Tuned)</td>\n      <td>1.133839e+09</td>\n      <td>20819.921875</td>\n      <td>13.330445</td>\n      <td>0.772589</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Final Model Comparison\n\n::: {#2cf660b6 .cell execution_count=6}\n``` {.python .cell-code}\nresults = pd.DataFrame([\n    [\"Decision Tree\", rmse_tree, mae_tree, mape_tree, r2_tree],\n    [\"Random Forest (Tuned)\", rmse_rf, mae_rf, mape_rf, r2_rf],\n    [\"XGBoost (Tuned)\", rmse_xgb, mae_xgb, mape_xgb, r2_xgb]\n], columns=[\"Model\", \"RMSE\", \"MAE\", \"MAPE\", \"R2\"]).sort_values(\"RMSE\")\nresults\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Random Forest (Tuned)</td>\n      <td>1.093313e+09</td>\n      <td>19889.772151</td>\n      <td>12.832756</td>\n      <td>0.780717</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGBoost (Tuned)</td>\n      <td>1.133839e+09</td>\n      <td>20819.921875</td>\n      <td>13.330445</td>\n      <td>0.772589</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Decision Tree</td>\n      <td>2.378377e+09</td>\n      <td>28465.348099</td>\n      <td>17.701235</td>\n      <td>0.522975</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n> **Summary:** The Decision Tree provides interpretability, Random Forest and XGBoost attempt to further reduce bias and manage variance error through randomization and ensembles of weak learners. The issue is that they become less and less interpretable, so we have to introduce other tools to understand what variables are important and how they are relating to the outcome.\n\n",
    "supporting": [
      "Tree_Based_Python2_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}