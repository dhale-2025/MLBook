{
  "hash": "f6d9cf80566311bb77a2f2de4fa6ecd5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"GAM— Piecewise, LOESS, and GAM Splines - Python Version\"\nformat:\n  html:\n    toc: true\n    number-sections: true\n  pdf: default\njupyter: python3\n---\n\n> **Note:** The original MARS section used the `sklearn-contrib-py-earth` package, which is **no longer actively maintained** and may fail to install on modern Python versions.  \n> The MARS code is left **commented out** for reference — you can un-comment it if you successfully install `py-earth` (try `conda install -c conda-forge sklearn-contrib-py-earth`).  \n> Equivalent flexibility can be achieved through **GAM** or **spline-based models**, which are shown below.\n\n## 1. Setup and Data\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport sys\nprint(\"Active Python interpreter:\", sys.executable)\n\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Ridge\nimport inspect\n\n# --- Fix for SciPy >= 1.13 removing .A from sparse matrices ---\nimport scipy.sparse as sp\n\nif not hasattr(sp.spmatrix, \"A\"):\n    def _toarray(self):\n        return self.toarray()\n    sp.spmatrix.A = property(_toarray)\n# -------------------------------------------------------------\n\nnp.random.seed(4321)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nActive Python interpreter: /opt/anaconda3/bin/python\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_openml\n\names = fetch_openml(name=\"house_prices\", as_frame=True).frame\n\nkeep = {\n    \"SalePrice\":\"SalePrice\",\n    \"BedroomAbvGr\":\"Bedroom_AbvGr\",\n    \"YearBuilt\":\"Year_Built\",\n    \"MoSold\":\"Mo_Sold\",\n    \"LotArea\":\"Lot_Area\",\n    \"Street\":\"Street\",\n    \"CentralAir\":\"Central_Air\",\n    \"1stFlrSF\":\"First_Flr_SF\",\n    \"2ndFlrSF\":\"Second_Flr_SF\",\n    \"FullBath\":\"Full_Bath\",\n    \"HalfBath\":\"Half_Bath\",\n    \"Fireplaces\":\"Fireplaces\",\n    \"GarageArea\":\"Garage_Area\",\n    \"GrLivArea\":\"Gr_Liv_Area\",\n    \"TotRmsAbvGrd\":\"TotRms_AbvGrd\"\n}\n\ndf = ames[list(keep.keys())].rename(columns=keep).dropna().copy()\n\nX = df.drop(columns=[\"SalePrice\"])\ny = df[\"SalePrice\"].values\n\nnum_cols = X.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = [c for c in X.columns if c not in num_cols]\n\n# Handle OneHotEncoder version changes and ensure dense output\nif \"sparse_output\" in inspect.signature(OneHotEncoder).parameters:\n    encoder = OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False)\nelse:\n    encoder = OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse=False)\n\npre = ColumnTransformer([\n    (\"num\", StandardScaler(), num_cols),\n    (\"cat\", encoder, cat_cols)\n], sparse_threshold=0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4321)\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SalePrice</th>\n      <th>Bedroom_AbvGr</th>\n      <th>Year_Built</th>\n      <th>Mo_Sold</th>\n      <th>Lot_Area</th>\n      <th>Street</th>\n      <th>Central_Air</th>\n      <th>First_Flr_SF</th>\n      <th>Second_Flr_SF</th>\n      <th>Full_Bath</th>\n      <th>Half_Bath</th>\n      <th>Fireplaces</th>\n      <th>Garage_Area</th>\n      <th>Gr_Liv_Area</th>\n      <th>TotRms_AbvGrd</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>208500</td>\n      <td>3</td>\n      <td>2003</td>\n      <td>2</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>Y</td>\n      <td>856</td>\n      <td>854</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>548</td>\n      <td>1710</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>181500</td>\n      <td>3</td>\n      <td>1976</td>\n      <td>5</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>Y</td>\n      <td>1262</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>460</td>\n      <td>1262</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>223500</td>\n      <td>3</td>\n      <td>2001</td>\n      <td>9</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>Y</td>\n      <td>920</td>\n      <td>866</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>608</td>\n      <td>1786</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>140000</td>\n      <td>3</td>\n      <td>1915</td>\n      <td>2</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>Y</td>\n      <td>961</td>\n      <td>756</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>642</td>\n      <td>1717</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>250000</td>\n      <td>4</td>\n      <td>2000</td>\n      <td>12</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>Y</td>\n      <td>1145</td>\n      <td>1053</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>836</td>\n      <td>2198</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n## 2. Visual Check for Nonlinearity\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport statsmodels.api as sm\nfig = plt.figure()\nplt.scatter(df[\"Gr_Liv_Area\"], df[\"SalePrice\"], s=8, alpha=0.3)\nlow = sm.nonparametric.lowess(df[\"SalePrice\"], df[\"Gr_Liv_Area\"], frac=0.3, return_sorted=True)\nplt.plot(low[:,0], low[:,1], linewidth=2)\nplt.title(\"SalePrice vs Gr_Liv_Area (LOESS smoother)\")\nplt.xlabel(\"Gr_Liv_Area\"); plt.ylabel(\"SalePrice\")\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](generalized-additive-models-python_files/figure-pdf/cell-4-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n---\n\n## 3. Piecewise Regression Example\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nclass PiecewiseLinear:\n    def __init__(self, knot=2000.0):\n        self.knot = knot\n        self.lin = LinearRegression()\n\n    def _transform(self, x):\n        x1 = np.asarray(x).reshape(-1,1)\n        hx = np.maximum(0, x1 - self.knot)\n        return np.hstack([x1, hx])\n\n    def fit(self, x, y):\n        Z = self._transform(x)\n        self.lin.fit(Z, y)\n        return self\n\n    def predict(self, x):\n        Z = self._transform(x)\n        return self.lin.predict(Z)\n\npw = PiecewiseLinear(knot=2000).fit(df[\"Gr_Liv_Area\"].values, df[\"SalePrice\"].values)\n\ngrid = np.linspace(df[\"Gr_Liv_Area\"].min(), df[\"Gr_Liv_Area\"].max(), 200)\npred = pw.predict(grid)\n\nplt.figure()\nplt.scatter(df[\"Gr_Liv_Area\"], df[\"SalePrice\"], s=8, alpha=0.2)\nplt.plot(grid, pred, linewidth=2)\nplt.axvline(2000, linestyle=\"--\")\nplt.title(\"Piecewise Linear Fit at Knot = 2000\")\nplt.xlabel(\"Gr_Liv_Area\"); plt.ylabel(\"SalePrice\")\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](generalized-additive-models-python_files/figure-pdf/cell-5-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n---\n\n## 4. MARS (Commented Out — For Reference Only)\n\n```python\n# The MARS implementation (py-earth) is not actively maintained and may fail to install on modern Python.\n# If you wish to try it, install from conda-forge:\n#     conda install -c conda-forge sklearn-contrib-py-earth\n#\n# Example (commented out):\n#\n# from pyearth import Earth\n#\n# mars = Pipeline([(\"prep\", pre), (\"model\", Earth(max_degree=2))]).fit(X_train, y_train)\n# print(mars.named_steps[\"model\"].summary())\n#\n# xs = np.linspace(X[\"Garage_Area\"].min(), X[\"Garage_Area\"].max(), 200)\n# yhat = mars.predict(pd.DataFrame({\"Garage_Area\": xs}))\n#\n# plt.figure()\n# plt.scatter(X_train[\"Garage_Area\"], y_train, s=8, alpha=0.2)\n# plt.plot(xs, yhat, linewidth=2)\n# plt.title(\"MARS with Garage_Area (Commented Out Example)\")\n# plt.xlabel(\"Garage_Area\"); plt.ylabel(\"SalePrice\")\n# plt.tight_layout()\n```\n\n---\n\n## 5. LOESS Visualization\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfig = plt.figure()\nplt.scatter(df[\"Gr_Liv_Area\"], df[\"SalePrice\"], s=8, alpha=0.2)\nlow = sm.nonparametric.lowess(df[\"SalePrice\"], df[\"Gr_Liv_Area\"], frac=0.4, return_sorted=True)\nplt.plot(low[:,0], low[:,1], linewidth=2)\nplt.title(\"LOESS Smoother (Visual Aid)\")\nplt.xlabel(\"Gr_Liv_Area\"); plt.ylabel(\"SalePrice\")\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](generalized-additive-models-python_files/figure-pdf/cell-6-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n---\n\n## 6. GAM and Spline Approaches (Modern Alternatives)\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom pygam import LinearGAM, s\ngam1 = LinearGAM(s(0)).fit(X_train[[\"Garage_Area\"]].values, y_train)\nprint(gam1.summary())\ngam2 = LinearGAM(s(0) + s(1)).fit(X_train[[\"Garage_Area\",\"Gr_Liv_Area\"]].values, y_train)\nprint(gam2.summary())\nnum_train = X_train[num_cols].values\ngam3 = LinearGAM().fit(num_train, y_train)\nprint(gam3.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                     11.7399\nLink Function:                     IdentityLink Log Likelihood:                                -23263.1874\nNumber of Samples:                         1022 AIC:                                            46551.8545\n                                                AICc:                                           46552.2017\n                                                GCV:                                       3130152370.8794\n                                                Scale:                                     3065497547.8147\n                                                Pseudo R-Squared:                                   0.4883\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                20           11.7         1.11e-16     ***         \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\nNone\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                     20.8593\nLink Function:                     IdentityLink Log Likelihood:                                -22770.8323\nNumber of Samples:                         1022 AIC:                                            45585.3832\n                                                AICc:                                           45586.3834\n                                                GCV:                                       1965630736.4285\n                                                Scale:                                      1893550067.068\n                                                Pseudo R-Squared:                                   0.6868\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                20           12.6         1.11e-16     ***         \ns(1)                              [0.6]                20           8.3          1.11e-16     ***         \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\nNone\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                     89.0836\nLink Function:                     IdentityLink Log Likelihood:                                 -21995.395\nNumber of Samples:                         1022 AIC:                                            44170.9574\n                                                AICc:                                           44188.5855\n                                                GCV:                                       1050008766.0212\n                                                Scale:                                      886662002.9642\n                                                Pseudo R-Squared:                                   0.8633\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                20           8.7          6.58e-07     ***         \ns(1)                              [0.6]                20           12.8         1.11e-16     ***         \ns(2)                              [0.6]                20           11.0         5.15e-01                 \ns(3)                              [0.6]                20           8.4          1.11e-16     ***         \ns(4)                              [0.6]                20           8.6          1.11e-16     ***         \ns(5)                              [0.6]                20           10.1         2.17e-03     **          \ns(6)                              [0.6]                20           3.2          9.76e-02     .           \ns(7)                              [0.6]                20           2.2          4.19e-01                 \ns(8)                              [0.6]                20           2.9          3.16e-10     ***         \ns(9)                              [0.6]                20           9.0          1.11e-16     ***         \ns(10)                             [0.6]                20           5.7          1.11e-16     ***         \ns(11)                             [0.6]                20           6.5          3.78e-01                 \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\nNone\n```\n:::\n:::\n\n\n---\n\n## 7. Regression Spline Example (Fallback)\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom patsy import dmatrix\n\ndesign_tr = dmatrix(\"bs(Garage_Area, df=6) + bs(Gr_Liv_Area, df=6)\", data=X_train, return_type=\"dataframe\")\ndesign_te = dmatrix(\"bs(Garage_Area, df=6) + bs(Gr_Liv_Area, df=6)\", data=X_test, return_type=\"dataframe\")\n\nridge = Ridge(alpha=1.0).fit(design_tr, y_train)\npred_spline = ridge.predict(design_te)\n```\n:::\n\n\n---\n\n## 8. Model Comparison (Piecewise vs GAM vs Spline)\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef rmse(y, yhat):\n    try:\n        return mean_squared_error(y, yhat, squared=False)\n    except TypeError:\n        return np.sqrt(mean_squared_error(y, yhat))\n\ndef mse(y, yhat): return mean_squared_error(y, yhat)\ndef rsq(y, yhat): return r2_score(y, yhat)\n\np_piece = pw.predict(X_test[\"Gr_Liv_Area\"].values)\n\ntry:\n    p_gam = gam3.predict(X_test[num_cols].values)\nexcept Exception:\n    p_gam = np.full_like(y_test, np.nan, dtype=float)\n\np_spline = pred_spline\n\ncmp = pd.DataFrame({\n    \"Model\": [\"Piecewise\", \"GAM (pyGAM)\" if not np.isnan(p_gam).all() else \"GAM (not available)\", \"Regression Spline\"],\n    \"RMSE\": [rmse(y_test, p_piece), rmse(y_test, p_gam) if not np.isnan(p_gam).all() else np.nan, rmse(y_test, p_spline)],\n    \"MSE\": [mse(y_test, p_piece), mse(y_test, p_gam) if not np.isnan(p_gam).all() else np.nan, mse(y_test, p_spline)],\n    \"R2\": [rsq(y_test, p_piece), rsq(y_test, p_gam) if not np.isnan(p_gam).all() else np.nan, rsq(y_test, p_spline)]\n}).sort_values(\"R2\", ascending=False)\n\ncmp\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MSE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>GAM (pyGAM)</td>\n      <td>40805.867162</td>\n      <td>1.665119e+09</td>\n      <td>0.766556</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Piecewise</td>\n      <td>58108.537553</td>\n      <td>3.376602e+09</td>\n      <td>0.526612</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Regression Spline</td>\n      <td>60289.000524</td>\n      <td>3.634764e+09</td>\n      <td>0.490419</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n## 9. Predicted vs Actual Comparison\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nplt.figure()\nplt.scatter(y_test, p_piece, s=8, alpha=0.4, label=\"Piecewise\")\nif not np.isnan(p_gam).all():\n    plt.scatter(y_test, p_gam, s=8, alpha=0.4, label=\"GAM\")\nplt.scatter(y_test, p_spline, s=8, alpha=0.4, label=\"Spline\")\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle=\"--\")\nplt.legend()\nplt.title(\"Predicted vs Actual (Test Set)\")\nplt.xlabel(\"Actual SalePrice\"); plt.ylabel(\"Predicted SalePrice\")\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](generalized-additive-models-python_files/figure-pdf/cell-10-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n---\n\n## 10. Interpretation Summary\n\n- **Piecewise:** adds simple thresholds, easy to explain.  \n- **GAMs:** provide smooth, interpretable nonlinearities.  \n- **Splines:** flexible approximations that behave like local polynomials.  \n- **MARS (optional):** similar conceptually but less maintained in Python; consider using R’s `earth` instead.\n\n",
    "supporting": [
      "generalized-additive-models-python_files/figure-pdf"
    ],
    "filters": []
  }
}