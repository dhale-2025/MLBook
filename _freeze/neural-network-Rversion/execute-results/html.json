{
  "hash": "ac9fabfad10ea34f187f8dd722084e07",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Neural Networks in R\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n  pdf:\n    toc: true\n    documentclass: scrreprt\n    mainfont: \"Helvetica\"\n    fontsize: 11pt\n    geometry:\n      - margin=1in\n    fig-width: 6\n    fig-height: 4\n    code-overflow: wrap\n    code-block-bg: \"#f8f8f8\"\n    code-block-border-left: \"#CCCCCC\"\nexecute:\n  echo: true\n  warning: false\n  message: false\noutput-file: neural-network-Rversion.pdf\n---\n\n## Introduction\n\nThis chapter introduces **neural networks in R**, progressing from feed-forward to recurrent (RNN) and convolutional (CNN) architectures.\\\nIt uses the Ames housing dataset, a sine wave sequence, and the MNIST digit dataset to demonstrate increasingly complex network structures.\n\nEach example illustrates a different ANN architecture.\n\n------------------------------------------------------------------------\n\n## 1. Feed-Forward Neural Networks (Ames Data)\n\nFeed-forward networks are the simplest type of neural networks. They map input features (like home size, rooms, or year built) to an output (home price) through **layers of neurons**.\\\nEach neuron applies a weighted sum and activation function, allowing the network to learn complex relationships between variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Note: This is specific to my environment \nlibrary(reticulate)\nuse_python(\"/Users/donaldhale/r-arm/bin/python\", required = TRUE)\n\nlibrary(keras3)\n\ntf <- reticulate::import(\"tensorflow\", delay_load = FALSE)\ncat(\"TensorFlow version:\", tf$`__version__`, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTensorFlow version: 2.16.1 \n```\n\n\n:::\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(nnet)\nlibrary(NeuralNetTools)\nlibrary(AmesHousing)\n\nset.seed(4321)\n\n# Load and sample data\names <- make_ordinal_ames() %>% mutate(id = row_number())\ntrain_idx <- sample(1:nrow(ames), 0.7 * nrow(ames))\ntraining <- ames[train_idx, ]\ntesting  <- ames[-train_idx, ]\n\n# Select a manageable subset of variables\nvars <- c(\"Sale_Price\", \"Bedroom_AbvGr\", \"Year_Built\", \"Mo_Sold\", \"Lot_Area\",\n          \"Street\", \"Central_Air\", \"First_Flr_SF\", \"Second_Flr_SF\",\n          \"Full_Bath\", \"Half_Bath\", \"Fireplaces\", \"Garage_Area\",\n          \"Gr_Liv_Area\", \"TotRms_AbvGrd\")\ntraining <- training[, vars]\ntesting  <- testing[, vars]\n\n# Convert categorical predictors into dummy numeric columns\nx_train <- model.matrix(Sale_Price ~ ., data = training)[, -1]\ny_train <- training$Sale_Price\nx_test  <- model.matrix(Sale_Price ~ ., data = testing)[, -1]\ny_test  <- testing$Sale_Price\n\n# Drop low-variance predictors and scale for stable training\nnzv <- nearZeroVar(x_train)\nif (length(nzv) > 0) {\n  x_train <- x_train[, -nzv, drop = FALSE]\n  x_test  <- x_test[, -nzv, drop = FALSE]\n}\n\nx_train <- scale(x_train)\nx_test  <- scale(x_test, center = attr(x_train, \"scaled:center\"),\n                 scale = attr(x_train, \"scaled:scale\"))\n\ny_train_s <- scale(y_train)\n\n# Combine into a single training dataframe\ntrain_df <- data.frame(y = as.numeric(y_train_s), x_train)\n\nset.seed(123)\nnn.ames <- nnet(y ~ ., data = train_df, size = 5, linout = TRUE, trace = FALSE)\nplotnet(nn.ames)\n```\n\n::: {.cell-output-display}\n![](neural-network-Rversion_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### 1.2 Cross-Validation Tuning\n\nWe tune two important parameters: - **`size`** — number of neurons in the hidden layer\\\n- **`decay`** — regularization strength (prevents overfitting)\n\n`caret::train()` automates this search using k-fold cross-validation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_grid <- expand.grid(size = c(3,5,7,9), decay = c(0,0.1,0.5,1))\nctrl <- trainControl(method = \"cv\", number = 5)\n\nset.seed(123)\nnn.tuned <- train(\n  x = x_train, y = as.numeric(y_train_s),\n  method = \"nnet\",\n  tuneGrid = tune_grid,\n  trControl = ctrl,\n  linout = TRUE,\n  trace = FALSE\n)\n\nnn.tuned$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   size decay\n15    9   0.5\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(nn.tuned)\n```\n\n::: {.cell-output-display}\n![](neural-network-Rversion_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### 1.3 Evaluate Performance\n\nAfter identifying the best parameters, we retrain the model and evaluate its predictive accuracy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_size  <- nn.tuned$bestTune$size\nbest_decay <- nn.tuned$bestTune$decay\n\nset.seed(123)\nnn.final <- nnet(\n  y ~ ., \n  data = train_df, \n  size = best_size, \n  decay = best_decay, \n  linout = TRUE, \n  trace = FALSE\n)\n\npred_std <- predict(nn.final, newdata = as.data.frame(x_test))\npred_nn  <- pred_std * attr(y_train_s, \"scaled:scale\") + attr(y_train_s, \"scaled:center\")\n\nperf <- data.frame(\n  MAE  = mean(abs(y_test - pred_nn)),\n  MAPE = mean(abs((y_test - pred_nn) / y_test)) * 100\n)\nperf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       MAE     MAPE\n1 20392.57 12.63864\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 2. Feed-Forward Neural Networks (Keras 3)\n\nThe **Keras 3** API is a modern deep learning interface. It supports more complex models and GPUs while maintaining a simple syntax.\\\nThis example shows the same concept as above using a multi-layer perceptron (MLP).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(keras3)\nset.seed(123)\n\nmodel_ff <- keras_model_sequential() |>\n  layer_dense(units = 16, activation = \"relu\", input_shape = ncol(x_train)) |>\n  layer_dropout(rate = 0.1) |>\n  layer_dense(units = 1)\n\ncompile(model_ff, optimizer = optimizer_adam(learning_rate = 0.01),\n        loss = \"mse\", metrics = \"mae\")\n\nhistory <- fit(model_ff, x_train, as.numeric(y_train_s),\n               epochs = 40, batch_size = 32, validation_split = 0.2, verbose = 0)\n\nplot(history)\n```\n\n::: {.cell-output-display}\n![](neural-network-Rversion_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 3. Recurrent Neural Network (RNN)\n\nRNNs are ideal for **sequential data** such as time series, text, or speech.\\\nWe use a sine wave as a toy example because its repeating pattern requires the model to “remember” recent values — something feed-forward networks cannot do.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nt <- seq(0, 50, by = 0.1)\ny <- sin(t)\nplot(t, y, type='l', main=\"Sine Wave Example\", ylab=\"y\", xlab=\"Time\")\n```\n\n::: {.cell-output-display}\n![](neural-network-Rversion_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nwindow_size <- 20\nX <- sapply(1:(length(y) - window_size), function(i) y[i:(i + window_size - 1)])\nX <- t(X)\ny_out <- y[(window_size + 1):length(y)]\nX <- array(X, dim = c(nrow(X), window_size, 1))\n\nsplit <- round(0.8 * nrow(X))\nx_train <- X[1:split,,]; y_train <- y_out[1:split]\nx_test  <- X[(split + 1):nrow(X),,]; y_test  <- y_out[(split + 1):length(y_out)]\n\nmodel_rnn <- keras_model_sequential() |>\n  layer_simple_rnn(units = 16, activation = \"tanh\", input_shape = c(window_size, 1)) |>\n  layer_dense(units = 1)\n\ncompile(model_rnn, loss = \"mse\", optimizer = \"adam\")\n\nhistory_rnn <- fit(model_rnn, x_train, y_train, epochs = 30, batch_size = 16,\n                   validation_split = 0.2, verbose = 0)\n\nplot(history_rnn)\n```\n\n::: {.cell-output-display}\n![](neural-network-Rversion_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\npreds <- predict(model_rnn, x_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3/3 - 0s - 11ms/step\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(y_test, type='l', main=\"RNN Sine Prediction\", col='blue', ylab=\"y\")\nlines(preds, col='red')\nlegend(\"bottomleft\", legend=c(\"Actual\",\"Predicted\"), col=c(\"blue\",\"red\"), lty=1, bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](neural-network-Rversion_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 4. Convolutional Neural Network (CNN)\n\nCNNs are built for **spatial pattern recognition** — ideal for images.\\\nThe **MNIST** dataset contains 70,000 grayscale images of handwritten digits (0–9), each 28×28 pixels.\\\nThese lines prepare the data for use in a CNN.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(keras3)\n\n# Load and preview the MNIST data\nmnist <- dataset_mnist()\n\n# We only use a subset (5k train, 1k test) for quick demonstration\nx_train <- mnist$train$x[1:5000,,]\ny_train <- mnist$train$y[1:5000]\nx_test  <- mnist$test$x[1:1000,,]\ny_test  <- mnist$test$y[1:1000]\n\n# Normalize pixel values: 0-255 -> 0-1\nx_train <- x_train / 255\nx_test  <- x_test / 255\n\n# Reshape to [samples, height, width, channels]\n# MNIST images are grayscale, so channels = 1\nx_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))\nx_test  <- array_reshape(x_test,  c(nrow(x_test),  28, 28, 1))\n\n# Convert labels to categorical one-hot vectors (10 classes for digits 0-9)\ny_train_cat <- to_categorical(y_train, 10)\ny_test_cat  <- to_categorical(y_test, 10)\n```\n:::\n\n\n### Explanation of the MNIST Preprocessing\n\n1.  **Load** the built-in dataset from Keras.\\\n2.  **Subsample** for faster training (5,000 train / 1,000 test).\\\n3.  **Normalize** pixel intensities to the 0–1 range.\\\n4.  **Reshape** to the 4D structure required by CNNs: `[samples, height, width, channels]`.\\\n5.  **One-hot encode** labels, converting a digit like `3` into a vector `[0,0,0,1,0,0,0,0,0,0]`.\n\nThese steps turn raw pixel data into numeric tensors suitable for learning.\n\n------------------------------------------------------------------------\n\n### 4.1 Train and Evaluate the CNN\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_cnn <- keras_model_sequential() |>\n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\", input_shape = c(28,28,1)) |>\n  layer_max_pooling_2d(pool_size = c(2,2)) |>\n  layer_flatten() |>\n  layer_dense(units = 32, activation = \"relu\") |>\n  layer_dense(units = 10, activation = \"softmax\")\n\ncompile(model_cnn, optimizer = \"adam\",\n        loss = \"categorical_crossentropy\",\n        metrics = \"accuracy\")\n\nhistory_cnn <- fit(model_cnn, x_train, y_train_cat,\n                   epochs = 3, batch_size = 64, validation_split = 0.2, verbose = 0)\n\nplot(history_cnn)\n```\n\n::: {.cell-output-display}\n![](neural-network-Rversion_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nscore <- evaluate(model_cnn, x_test, y_test_cat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n32/32 - 0s - 745us/step - accuracy: 0.9000 - loss: 0.3169\n```\n\n\n:::\n\n```{.r .cell-code}\nscore\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$accuracy\n[1] 0.9\n\n$loss\n[1] 0.3169269\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### 4.2 Visualizing Correct and Incorrect Predictions\n\nWe visualize examples of digits that the CNN classified correctly and incorrectly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_probs <- predict(model_cnn, x_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n32/32 - 0s - 1ms/step\n```\n\n\n:::\n\n```{.r .cell-code}\npred_classes <- apply(pred_probs, 1, which.max) - 1\n\ncorrect_idx <- which(pred_classes == y_test)[1:6]\nincorrect_idx <- which(pred_classes != y_test)[1:6]\n\npar(mfrow=c(2,6), mar=c(1,1,2,1))\nfor (i in correct_idx) {\n  img <- x_test[i,,,1]\n  image(1:28, 1:28, t(apply(img, 2, rev)), col=gray.colors(255), axes=FALSE,\n        main=paste(\"✓\", y_test[i]))\n}\nfor (i in incorrect_idx) {\n  img <- x_test[i,,,1]\n  image(1:28, 1:28, t(apply(img, 2, rev)), col=gray.colors(255), axes=FALSE,\n        main=paste(\"✗\", y_test[i], \"→\", pred_classes[i]))\n}\n```\n\n::: {.cell-output-display}\n![](neural-network-Rversion_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Summary\n\n-   **Feed-forward networks**: handle static, tabular data (e.g., housing prices).\\\n-   **RNNs**: handle sequential data with temporal dependencies (e.g., sine wave, stock prices, text, timer series).\\\n-   **CNNs**: handle image and spatial data by learning patterns like edges and textures.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}