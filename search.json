[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning with R and Python",
    "section": "",
    "text": "About\nThis book is a companion to the Machine Learning Class. It will be a repository of R and Python Code. The chapters will be updated as we progress through the class.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Introduction to Machine Learning\nMachine Learning (ML) is a branch of artificial intelligence (AI) that focuses on building systems that learn patterns from data and make predictions or decisions without being explicitly programmed to perform specific tasks. Instead of writing rules by hand, a machine learning algorithm “learns” from examples in the data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#what-is-machine-learning",
    "href": "introduction.html#what-is-machine-learning",
    "title": "1  Introduction",
    "section": "1.2 What is Machine Learning?",
    "text": "1.2 What is Machine Learning?\nFormally, machine learning is the process of using data to train a model to make accurate predictions or decisions. The model identifies patterns in the training data and generalizes these patterns to new, unseen data.\nML can be thought of as:\n\nPredictive modeling – learning a function that maps inputs (features) to outputs (targets).\nAutomated decision-making – systems that improve their performance over time without explicit reprogramming.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#major-types-of-machine-learning",
    "href": "introduction.html#major-types-of-machine-learning",
    "title": "1  Introduction",
    "section": "1.3 Major Types of Machine Learning",
    "text": "1.3 Major Types of Machine Learning\nMachine learning is commonly categorized into several types based on the nature of the task and availability of labeled data.\n\n1.3.1 1. Supervised Learning\n\nUses labeled data, meaning that each training example has an input and a known output.\nThe goal is to learn a function that maps inputs to outputs accurately.\nTypical tasks:\n\nRegression: Predict a continuous variable (e.g., house prices).\nClassification: Predict a categorical variable (e.g., spam vs. non-spam email).\n\n\n\n\n1.3.2 2. Unsupervised Learning\n\nWorks with unlabeled data, where the output is unknown.\nThe goal is to discover patterns or structure in the data.\nTypical tasks:\n\nClustering: Group similar observations together (e.g., customer segmentation).\nDimensionality reduction: Reduce the number of features while retaining important information (e.g., PCA).\n\n\n\n\n1.3.3 3. Semi-Supervised Learning\n\nCombines a small amount of labeled data with a large amount of unlabeled data.\nUseful when labeling data is expensive or time-consuming.\n\n\n\n1.3.4 4. Reinforcement Learning\n\nFocuses on learning through trial and error.\nAn agent learns to take actions in an environment to maximize cumulative reward.\nExamples: Robotics, game AI, recommendation systems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#objectives-of-machine-learning",
    "href": "introduction.html#objectives-of-machine-learning",
    "title": "1  Introduction",
    "section": "1.4 Objectives of Machine Learning",
    "text": "1.4 Objectives of Machine Learning\nThe main objective of machine learning is to build models that generalize well from historical data to make accurate predictions or decisions on new, unseen data. Key considerations include:\n\nAccuracy – How well does the model predict outcomes?\nGeneralization – Does the model perform well on unseen data, or is it overfitting the training data?\nInterpretability – Can humans understand how the model makes predictions?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#bias-variance-tradeoff",
    "href": "introduction.html#bias-variance-tradeoff",
    "title": "1  Introduction",
    "section": "1.5 Bias-Variance Tradeoff",
    "text": "1.5 Bias-Variance Tradeoff\nA fundamental concept in machine learning is the bias-variance tradeoff, which explains the balance between:\n\nBias: Error due to overly simplistic models that underfit the data.\nVariance: Error due to overly complex models that overfit the training data.\n\nThe goal is to find a model that minimizes the total prediction error:\n[ = ^2 + + ]\n\nUnderfitting → High bias, low variance\n\nOverfitting → Low bias, high variance\n\nThe optimal model balances bias and variance for the best predictive performance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#the-machine-learning-process",
    "href": "introduction.html#the-machine-learning-process",
    "title": "1  Introduction",
    "section": "1.6 The Machine Learning Process",
    "text": "1.6 The Machine Learning Process\nA typical workflow in machine learning consists of the following steps:\n\nData Collection\n\nGather data from experiments, databases, or external sources.\n\nData Preprocessing\n\nHandle missing values, outliers, and feature engineering.\n\nModel Selection\n\nChoose an appropriate algorithm (e.g., linear regression, random forest, neural networks).\n\nModel Training\n\nFit the model to the training data.\n\nModel Evaluation\n\nAssess performance using metrics such as RMSE, accuracy, precision, recall, or AUC.\n\nModel Tuning\n\nAdjust hyperparameters to improve performance.\n\nModel Deployment\n\nUse the trained model to make predictions on new data.\n\nMonitoring and Maintenance\n\nContinuously track model performance and retrain if necessary.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#summary",
    "href": "introduction.html#summary",
    "title": "1  Introduction",
    "section": "1.7 Summary",
    "text": "1.7 Summary\nMachine learning allows us to build predictive models that learn from data. Understanding the different types of ML, their objectives, and the tradeoffs between bias and variance is critical to applying ML effectively. The remainder of this book explores practical algorithms, their implementation in R and Python, and strategies for model selection and interpretation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html",
    "href": "resampling-model-selection.html",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "",
    "text": "2.1 Overview\nIn this section, we’ll use the Ames Housing dataset to demonstrate model selection and regularization.\nWe’ll cover:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#overview",
    "href": "resampling-model-selection.html#overview",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "",
    "text": "Splitting data into training and testing sets\n\nPerforming stepwise regression with cross-validation\n\nRunning Ridge and Lasso regression, and visualizing how λ affects RMSE and model complexity\n\nExploring Elastic Net, varying α to balance Ridge and Lasso\n\nComparing all models on the test set",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#step-0-setup-and-data-preparation",
    "href": "resampling-model-selection.html#step-0-setup-and-data-preparation",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "2.2 Step 0: Setup and Data Preparation",
    "text": "2.2 Step 0: Setup and Data Preparation\nWe’ll use a reduced set of variables for speed and clarity.\n\n\nCode\n# Suppress messages and warnings globally\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(AmesHousing)\nlibrary(GGally)\n\nset.seed(123)\n\n# Load data and split into 70% training, 30% testing\names &lt;- make_ordinal_ames() %&gt;% mutate(id = row_number())\ntrain &lt;- ames %&gt;% sample_frac(0.7)\ntest  &lt;- anti_join(ames, train, by = \"id\")\n\n# Select a manageable subset of predictors\nkeep &lt;- c(\"Sale_Price\", \"Bedroom_AbvGr\", \"Year_Built\", \"Mo_Sold\", \"Lot_Area\",\n          \"Street\", \"Central_Air\", \"First_Flr_SF\", \"Second_Flr_SF\", \"Full_Bath\",\n          \"Half_Bath\", \"Fireplaces\", \"Garage_Area\", \"Gr_Liv_Area\", \"TotRms_AbvGrd\")\n\ntrain &lt;- train %&gt;% select(all_of(keep))\ntest  &lt;- test  %&gt;% select(all_of(keep))\n\n# Convert categorical variables to factors\ntrain &lt;- train %&gt;% mutate(across(c(Street, Central_Air), as.factor))\ntest  &lt;- test %&gt;% mutate(across(c(Street, Central_Air), as.factor))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#step-1-stepwise-regression-with-10-fold-cross-validation",
    "href": "resampling-model-selection.html#step-1-stepwise-regression-with-10-fold-cross-validation",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "2.3 Step 1: Stepwise Regression with 10-Fold Cross Validation",
    "text": "2.3 Step 1: Stepwise Regression with 10-Fold Cross Validation\nWe’ll use backward stepwise regression as a traditional benchmark.\n\n\nCode\nctrl &lt;- trainControl(method = \"cv\", number = 10)\n\nstep_model &lt;- train(\n  Sale_Price ~ ., data = train,\n  method = \"lmStepAIC\",\n  trControl = ctrl,\n  trace = FALSE,\n  direction = \"backward\"\n)\n\n# Evaluate on the test set\nstep_pred &lt;- predict(step_model, newdata = test)\nstep_perf &lt;- postResample(step_pred, test$Sale_Price)\nstep_perf\n\n\n        RMSE     Rsquared          MAE \n3.820776e+04 7.670269e-01 2.613098e+04 \n\n\nCode\n# Visualize predicted vs observed\nggplot(data.frame(obs = test$Sale_Price, pred = step_pred), aes(obs, pred)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Stepwise Regression: Observed vs Predicted\",\n       x = \"Observed Price\", y = \"Predicted Price\")\n\n\n\n\n\n\n\n\n\nNote:\nStepwise regression is intuitive and fast, but it can be unstable.\nStepwise regression (forward, backward, or both) selects predictors one at a time based on how much they improve a criterion (like AIC or adjusted R²).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#step-2-ridge-and-lasso-regression",
    "href": "resampling-model-selection.html#step-2-ridge-and-lasso-regression",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "2.4 Step 2: Ridge and Lasso Regression",
    "text": "2.4 Step 2: Ridge and Lasso Regression\nRidge and Lasso both shrink coefficients, but in different ways: - Ridge shrinks all coefficients toward zero. - Lasso can set some coefficients exactly to zero, performing variable selection.\n\n\nCode\nx_train &lt;- model.matrix(Sale_Price ~ ., train)[, -1]\ny_train &lt;- train$Sale_Price\nx_test  &lt;- model.matrix(Sale_Price ~ ., test)[, -1]\ny_test  &lt;- test$Sale_Price\n\nset.seed(123)\ncv_ridge &lt;- cv.glmnet(x_train, y_train, alpha = 0, nfolds = 10)\ncv_lasso &lt;- cv.glmnet(x_train, y_train, alpha = 1, nfolds = 10)\n\n\n\n2.4.1 Visualizing RMSE and Model Complexity\n\n\nCode\nbuild_path_df &lt;- function(cvfit, label) {\n  fit &lt;- cvfit$glmnet.fit\n  tibble(\n    lambda = fit$lambda,\n    log_lambda = log(fit$lambda),\n    RMSE = sqrt(cvfit$cvm),\n    nonzero = colSums(abs(fit$beta) &gt; 0),\n    Model = label\n  )\n}\n\nridge_df &lt;- build_path_df(cv_ridge, \"Ridge\")\nlasso_df &lt;- build_path_df(cv_lasso, \"Lasso\")\ndf &lt;- bind_rows(ridge_df, lasso_df)\n\nggplot(df, aes(log_lambda, RMSE, color = Model)) +\n  geom_line(size = 1) +\n  labs(title = \"RMSE vs log(lambda)\", y = \"Cross-validated RMSE\", x = \"log(lambda)\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(log_lambda, nonzero, color = Model)) +\n  geom_line(size = 1) +\n  labs(title = \"Number of Nonzero Coefficients vs log(lambda)\",\n       y = \"Number of Nonzero Coefficients\", x = \"log(lambda)\")\n\n\n\n\n\n\n\n\n\nNote:\n- Increasing λ increases regularization.\n- Ridge never eliminates variables, Lasso can.\n- There’s a sweet spot where RMSE is minimized.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#step-3-elastic-net-balancing-ridge-and-lasso",
    "href": "resampling-model-selection.html#step-3-elastic-net-balancing-ridge-and-lasso",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "2.5 Step 3: Elastic Net (Balancing Ridge and Lasso)",
    "text": "2.5 Step 3: Elastic Net (Balancing Ridge and Lasso)\nElastic Net introduces α to control the mix between Ridge (α = 0) and Lasso (α = 1).\n\n\nCode\nalpha_grid &lt;- seq(0, 1, by = 0.25)\n\nelastic_results &lt;- map_df(alpha_grid, function(a) {\n  cv_fit &lt;- cv.glmnet(x_train, y_train, alpha = a, nfolds = 10)\n  tibble(\n    alpha = a,\n    best_lambda = cv_fit$lambda.min,\n    best_RMSE = sqrt(min(cv_fit$cvm)),\n    nonzero = sum(abs(coef(cv_fit, s = \"lambda.min\")[-1]) &gt; 0)\n  )\n})\n\nelastic_results\n\n\n# A tibble: 5 × 4\n  alpha best_lambda best_RMSE nonzero\n  &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;int&gt;\n1  0          5603.    42664.      14\n2  0.25       1224.    42675.      14\n3  0.5        1552.    42952.      11\n4  0.75        650.    42612.      11\n5  1          1126.    43048.      10\n\n\n\n2.5.1 Visualizing α Effects\n\n\nCode\nggplot(elastic_results, aes(alpha, best_RMSE)) +\n  geom_line() + geom_point(size = 2) +\n  labs(title = \"Elastic Net: Best RMSE vs Alpha\",\n       y = \"Best Cross-validated RMSE\", x = \"Alpha\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(elastic_results, aes(alpha, nonzero)) +\n  geom_line() + geom_point(size = 2) +\n  labs(title = \"Elastic Net: Model Size vs Alpha\",\n       y = \"Number of Nonzero Coefficients\", x = \"Alpha\")\n\n\n\n\n\n\n\n\n\nNote:\nElastic Net combines the strengths of Ridge and Lasso: - Ridge for stability. - Lasso for sparsity. - Often performs best at intermediate α values.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#step-4-comparing-all-models-on-the-test-set",
    "href": "resampling-model-selection.html#step-4-comparing-all-models-on-the-test-set",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "2.6 Step 4: Comparing All Models on the Test Set",
    "text": "2.6 Step 4: Comparing All Models on the Test Set\nWe’ll compare Stepwise, Ridge, Lasso, and the best Elastic Net.\n\n\nCode\nridge_pred &lt;- predict(cv_ridge, newx = x_test, s = \"lambda.min\")\nlasso_pred &lt;- predict(cv_lasso, newx = x_test, s = \"lambda.min\")\n\nbest_alpha &lt;- elastic_results %&gt;% arrange(best_RMSE) %&gt;% slice(1) %&gt;% pull(alpha)\ncv_en_best &lt;- cv.glmnet(x_train, y_train, alpha = best_alpha, nfolds = 10)\nelastic_pred &lt;- predict(cv_en_best, newx = x_test, s = \"lambda.min\")\n\ncompare_tbl &lt;- bind_rows(\n  Stepwise = as_tibble_row(postResample(step_pred, test$Sale_Price)),\n  Ridge = as_tibble_row(postResample(ridge_pred, y_test)),\n  Lasso = as_tibble_row(postResample(lasso_pred, y_test)),\n  Elastic = as_tibble_row(postResample(elastic_pred, y_test)),\n  .id = \"Model\"\n)\n\ncolnames(compare_tbl) &lt;- c(\"Model\", \"RMSE\", \"Rsquared\", \"MAE\")\ncompare_tbl &lt;- compare_tbl %&gt;% arrange(RMSE)\ncompare_tbl\n\n\n# A tibble: 4 × 4\n  Model      RMSE Rsquared    MAE\n  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Stepwise 38208.    0.767 26131.\n2 Lasso    38297.    0.767 26045.\n3 Elastic  38301.    0.767 26039.\n4 Ridge    38412.    0.766 26022.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#key-takeaways",
    "href": "resampling-model-selection.html#key-takeaways",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "2.7 Key Takeaways",
    "text": "2.7 Key Takeaways\n\nStepwise selection is simple but can lead to unstable models.\n\nRidge adds stability via coefficient shrinkage.\n\nLasso enforces sparsity by removing weak predictors.\n\nElastic Net balances both effects.\n\nRegularization often produces models that generalize better than purely stepwise ones.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html",
    "href": "resampling-model-selection-python.html",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "",
    "text": "3.1 Overview\nThis section parallels the R version of Model Selection and Regularization.\nWe will:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#overview",
    "href": "resampling-model-selection-python.html#overview",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "",
    "text": "Load the Ames Housing data\n\nSplit into training and test sets\n\nPerform Ridge, Lasso, and Elastic Net regressions with 10-fold CV\n\nVisualize RMSE and coefficient shrinkage patterns\n\nCompare test set performance",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#step-0-setup-and-data",
    "href": "resampling-model-selection-python.html#step-0-setup-and-data",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.2 Step 0: Setup and Data",
    "text": "3.2 Step 0: Setup and Data\nWe’ll use scikit-learn for modeling.\nIf the CSVs ames_training.csv and ames_testing.csv are not present, you can load Ames Housing via the fetch_openml function.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error\n\n# Load Ames dataset directly\names = fetch_openml(name=\"house_prices\", as_frame=True)\ndf = ames.frame\n\n# Subset variables to match the R version\nkeep = [\n    \"SalePrice\",\"BedroomAbvGr\",\"YearBuilt\",\"MoSold\",\"LotArea\",\"Street\",\"CentralAir\",\n    \"1stFlrSF\",\"2ndFlrSF\",\"FullBath\",\"HalfBath\",\"Fireplaces\",\"GarageArea\",\n    \"GrLivArea\",\"TotRmsAbvGrd\"\n]\ndf = df[keep].copy()\n\n# Clean up names to match Python variable rules\ndf.columns = [\"Sale_Price\",\"Bedroom_AbvGr\",\"Year_Built\",\"Mo_Sold\",\"Lot_Area\",\"Street\",\n              \"Central_Air\",\"First_Flr_SF\",\"Second_Flr_SF\",\"Full_Bath\",\"Half_Bath\",\n              \"Fireplaces\",\"Garage_Area\",\"Gr_Liv_Area\",\"TotRms_AbvGrd\"]\n\n# Drop missing rows\ndf = df.dropna()\n\n# Train/test split (70/30)\ntrain, test = train_test_split(df, test_size=0.3, random_state=123)\n\n# Identify predictors and target\ny_train = train[\"Sale_Price\"]\ny_test  = test[\"Sale_Price\"]\nX_train = train.drop(columns=\"Sale_Price\")\nX_test  = test.drop(columns=\"Sale_Price\")\n\ncat_vars = [\"Street\",\"Central_Air\"]\nnum_vars = [c for c in X_train.columns if c not in cat_vars]\n\n# Preprocess: scale numeric, one-hot encode categorical\npreprocessor = ColumnTransformer([\n    (\"num\", StandardScaler(), num_vars),\n    (\"cat\", OneHotEncoder(drop=\"first\"), cat_vars)\n])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#step-1-ridge-regression",
    "href": "resampling-model-selection-python.html#step-1-ridge-regression",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.3 Step 1: Ridge Regression",
    "text": "3.3 Step 1: Ridge Regression\nWe’ll use cross-validation to tune λ (alpha).\nThen we visualize RMSE vs log(lambda) and the number of nonzero coefficients.\n\n\nCode\nalphas = np.logspace(4, -4, 80)\nridge = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", RidgeCV(alphas=alphas, scoring=\"neg_mean_squared_error\", cv=10))\n])\nridge.fit(X_train, y_train)\n\nridge_best = ridge.named_steps[\"model\"].alpha_\nprint(\"Best Ridge alpha:\", ridge_best)\n\n# Evaluate RMSE on test set\nridge_pred = ridge.predict(X_test)\nridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))\nprint(\"Test RMSE (Ridge):\", ridge_rmse)\n\n\nBest Ridge alpha: 94.33732216299774\nTest RMSE (Ridge): 37055.84511170759",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#step-2-lasso-regression",
    "href": "resampling-model-selection-python.html#step-2-lasso-regression",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.4 Step 2: Lasso Regression",
    "text": "3.4 Step 2: Lasso Regression\nLasso adds variable selection — some coefficients go exactly to zero.\n\n\nCode\nlasso = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", LassoCV(alphas=alphas, cv=10, max_iter=20000))\n])\nlasso.fit(X_train, y_train)\n\nlasso_best = lasso.named_steps[\"model\"].alpha_\nprint(\"Best Lasso alpha:\", lasso_best)\n\nlasso_pred = lasso.predict(X_test)\nlasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_pred))\nprint(\"Test RMSE (Lasso):\", lasso_rmse)\n\n\nBest Lasso alpha: 11.568875283162821\nTest RMSE (Lasso): 37074.74978063246",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#step-3-visualize-coefficient-shrinkage-and-rmse",
    "href": "resampling-model-selection-python.html#step-3-visualize-coefficient-shrinkage-and-rmse",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.5 Step 3: Visualize Coefficient Shrinkage and RMSE",
    "text": "3.5 Step 3: Visualize Coefficient Shrinkage and RMSE\nLet’s visualize how RMSE and model sparsity change with λ.\n\n\nCode\n# Compute RMSE path manually\ndef cv_rmse(model_class, X, y, alphas):\n    rmse = []\n    for a in alphas:\n        model = Pipeline([\n            (\"prep\", preprocessor),\n            (\"model\", model_class(alpha=a, max_iter=20000))\n        ])\n        scores = cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=KFold(10, shuffle=True, random_state=123))\n        rmse.append(np.sqrt(-scores.mean()))\n    return np.array(rmse)\n\nfrom sklearn.linear_model import Ridge, Lasso\nridge_rmse_path = cv_rmse(Ridge, X_train, y_train, alphas)\nlasso_rmse_path = cv_rmse(Lasso, X_train, y_train, alphas)\n\nplt.figure()\nplt.plot(np.log(alphas), ridge_rmse_path, label=\"Ridge\")\nplt.plot(np.log(alphas), lasso_rmse_path, label=\"Lasso\")\nplt.xlabel(\"log(lambda)\")\nplt.ylabel(\"Cross-validated RMSE\")\nplt.title(\"RMSE vs log(lambda)\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#step-4-elastic-net",
    "href": "resampling-model-selection-python.html#step-4-elastic-net",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.6 Step 4: Elastic Net",
    "text": "3.6 Step 4: Elastic Net\nElastic Net blends Ridge and Lasso.\nWe’ll vary α (the mix) and visualize the tradeoff.\n\n\nCode\nl1_ratios = np.linspace(0, 1, 6)\nen_cv = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", ElasticNetCV(l1_ratio=l1_ratios, alphas=alphas, cv=10, max_iter=50000))\n])\nen_cv.fit(X_train, y_train)\n\nprint(\"Best Elastic Net alpha:\", en_cv.named_steps[\"model\"].alpha_)\nprint(\"Best Elastic Net l1_ratio:\", en_cv.named_steps[\"model\"].l1_ratio_)\n\nen_pred = en_cv.predict(X_test)\nen_rmse = np.sqrt(mean_squared_error(y_test, en_pred))\nprint(\"Test RMSE (Elastic Net):\", en_rmse)\n\n\nBest Elastic Net alpha: 0.55825862688627\nBest Elastic Net l1_ratio: 0.8\nTest RMSE (Elastic Net): 37134.497858434275",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#step-5-compare-models",
    "href": "resampling-model-selection-python.html#step-5-compare-models",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.7 Step 5: Compare Models",
    "text": "3.7 Step 5: Compare Models\n\n\nCode\nresults = pd.DataFrame({\n    \"Model\": [\"Ridge\", \"Lasso\", \"Elastic Net\"],\n    \"Test_RMSE\": [ridge_rmse, lasso_rmse, en_rmse]\n})\nprint(results.sort_values(\"Test_RMSE\"))\n\n\n         Model     Test_RMSE\n0        Ridge  37055.845112\n1        Lasso  37074.749781\n2  Elastic Net  37134.497858",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#summary",
    "href": "resampling-model-selection-python.html#summary",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.8 Summary",
    "text": "3.8 Summary\n\nRidge stabilizes coefficients by shrinking them.\n\nLasso enforces sparsity by zeroing weak predictors.\n\nElastic Net blends both for balance.\n\nRegularization often beats pure stepwise regression on unseen data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  }
]