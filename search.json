[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning with R and Python",
    "section": "",
    "text": "About\nThis book is a companion to the Machine Learning Class. It will be a repository of R and Python Code. The chapters will be updated as we progress through the class.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Introduction to Machine Learning\nMachine Learning (ML) is a branch of artificial intelligence (AI) that focuses on building systems that learn patterns from data and make predictions or decisions without being explicitly programmed to perform specific tasks. Instead of writing rules by hand, a machine learning algorithm “learns” from examples in the data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#what-is-machine-learning",
    "href": "introduction.html#what-is-machine-learning",
    "title": "1  Introduction",
    "section": "1.2 What is Machine Learning?",
    "text": "1.2 What is Machine Learning?\nFormally, machine learning is the process of using data to train a model to make accurate predictions or decisions. The model identifies patterns in the training data and generalizes these patterns to new, unseen data.\nML can be thought of as:\n\nPredictive modeling – learning a function that maps inputs (features) to outputs (targets).\nAutomated decision-making – systems that improve their performance over time without explicit reprogramming.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#major-types-of-machine-learning",
    "href": "introduction.html#major-types-of-machine-learning",
    "title": "1  Introduction",
    "section": "1.3 Major Types of Machine Learning",
    "text": "1.3 Major Types of Machine Learning\nMachine learning is commonly categorized into several types based on the nature of the task and availability of labeled data.\n\n1.3.1 1. Supervised Learning\n\nUses labeled data, meaning that each training example has an input and a known output.\nThe goal is to learn a function that maps inputs to outputs accurately.\nTypical tasks:\n\nRegression: Predict a continuous variable (e.g., house prices).\nClassification: Predict a categorical variable (e.g., spam vs. non-spam email).\n\n\n\n\n1.3.2 2. Unsupervised Learning\n\nWorks with unlabeled data, where the output is unknown.\nThe goal is to discover patterns or structure in the data.\nTypical tasks:\n\nClustering: Group similar observations together (e.g., customer segmentation).\nDimensionality reduction: Reduce the number of features while retaining important information (e.g., PCA).\n\n\n\n\n1.3.3 3. Semi-Supervised Learning\n\nCombines a small amount of labeled data with a large amount of unlabeled data.\nUseful when labeling data is expensive or time-consuming.\n\n\n\n1.3.4 4. Reinforcement Learning\n\nFocuses on learning through trial and error.\nAn agent learns to take actions in an environment to maximize cumulative reward.\nExamples: Robotics, game AI, recommendation systems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#objectives-of-machine-learning",
    "href": "introduction.html#objectives-of-machine-learning",
    "title": "1  Introduction",
    "section": "1.4 Objectives of Machine Learning",
    "text": "1.4 Objectives of Machine Learning\nThe main objective of machine learning is to build models that generalize well from historical data to make accurate predictions or decisions on new, unseen data. Key considerations include:\n\nAccuracy – How well does the model predict outcomes?\nGeneralization – Does the model perform well on unseen data, or is it overfitting the training data?\nInterpretability – Can humans understand how the model makes predictions?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#bias-variance-tradeoff",
    "href": "introduction.html#bias-variance-tradeoff",
    "title": "1  Introduction",
    "section": "1.5 Bias-Variance Tradeoff",
    "text": "1.5 Bias-Variance Tradeoff\nA fundamental concept in machine learning is the bias-variance tradeoff, which explains the balance between:\n\nBias: Error due to overly simplistic models that underfit the data.\nVariance: Error due to overly complex models that overfit the training data.\n\nThe goal is to find a model that minimizes the total prediction error:\n[ = ^2 + + ]\n\nUnderfitting → High bias, low variance\n\nOverfitting → Low bias, high variance\n\nThe optimal model balances bias and variance for the best predictive performance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#the-machine-learning-process",
    "href": "introduction.html#the-machine-learning-process",
    "title": "1  Introduction",
    "section": "1.6 The Machine Learning Process",
    "text": "1.6 The Machine Learning Process\nA typical workflow in machine learning consists of the following steps:\n\nData Collection\n\nGather data from experiments, databases, or external sources.\n\nData Preprocessing\n\nHandle missing values, outliers, and feature engineering.\n\nModel Selection\n\nChoose an appropriate algorithm (e.g., linear regression, random forest, neural networks).\n\nModel Training\n\nFit the model to the training data.\n\nModel Evaluation\n\nAssess performance using metrics such as RMSE, accuracy, precision, recall, or AUC.\n\nModel Tuning\n\nAdjust hyperparameters to improve performance.\n\nModel Deployment\n\nUse the trained model to make predictions on new data.\n\nMonitoring and Maintenance\n\nContinuously track model performance and retrain if necessary.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#summary",
    "href": "introduction.html#summary",
    "title": "1  Introduction",
    "section": "1.7 Summary",
    "text": "1.7 Summary\nMachine learning allows us to build predictive models that learn from data. Understanding the different types of ML, their objectives, and the tradeoffs between bias and variance is critical to applying ML effectively. The remainder of this book explores practical algorithms, their implementation in R and Python, and strategies for model selection and interpretation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html",
    "href": "resampling-model-selection.html",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "",
    "text": "2.1 Overview\nIn this section, we’ll use the Ames Housing dataset to demonstrate model selection and regularization.\nWe’ll cover:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#overview",
    "href": "resampling-model-selection.html#overview",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "",
    "text": "Splitting data into training and testing sets\n\nPerforming stepwise regression with cross-validation\n\nRunning Ridge and Lasso regression, and visualizing how λ affects RMSE and model complexity\n\nExploring Elastic Net, varying α to balance Ridge and Lasso\n\nComparing all models on the test set",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#step-0-setup-and-data-preparation",
    "href": "resampling-model-selection.html#step-0-setup-and-data-preparation",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "2.2 Step 0: Setup and Data Preparation",
    "text": "2.2 Step 0: Setup and Data Preparation\nWe’ll use a reduced set of variables for speed and clarity.\n\n\nCode\n# Suppress messages and warnings globally\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(AmesHousing)\nlibrary(GGally)\n\nset.seed(123)\n\n# Load data and split into 70% training, 30% testing\names &lt;- make_ordinal_ames() %&gt;% mutate(id = row_number())\ntrain &lt;- ames %&gt;% sample_frac(0.7)\ntest  &lt;- anti_join(ames, train, by = \"id\")\n\n# Select a manageable subset of predictors\nkeep &lt;- c(\"Sale_Price\", \"Bedroom_AbvGr\", \"Year_Built\", \"Mo_Sold\", \"Lot_Area\",\n          \"Street\", \"Central_Air\", \"First_Flr_SF\", \"Second_Flr_SF\", \"Full_Bath\",\n          \"Half_Bath\", \"Fireplaces\", \"Garage_Area\", \"Gr_Liv_Area\", \"TotRms_AbvGrd\")\n\ntrain &lt;- train %&gt;% select(all_of(keep))\ntest  &lt;- test  %&gt;% select(all_of(keep))\n\n# Convert categorical variables to factors\ntrain &lt;- train %&gt;% mutate(across(c(Street, Central_Air), as.factor))\ntest  &lt;- test %&gt;% mutate(across(c(Street, Central_Air), as.factor))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#step-1-stepwise-regression-with-10-fold-cross-validation",
    "href": "resampling-model-selection.html#step-1-stepwise-regression-with-10-fold-cross-validation",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "2.3 Step 1: Stepwise Regression with 10-Fold Cross Validation",
    "text": "2.3 Step 1: Stepwise Regression with 10-Fold Cross Validation\nWe’ll use backward stepwise regression as a traditional benchmark.\n\n\nCode\nctrl &lt;- trainControl(method = \"cv\", number = 10)\n\nstep_model &lt;- train(\n  Sale_Price ~ ., data = train,\n  method = \"lmStepAIC\",\n  trControl = ctrl,\n  trace = FALSE,\n  direction = \"backward\"\n)\n\n# Evaluate on the test set\nstep_pred &lt;- predict(step_model, newdata = test)\nstep_perf &lt;- postResample(step_pred, test$Sale_Price)\nstep_perf\n\n\n        RMSE     Rsquared          MAE \n3.820776e+04 7.670269e-01 2.613098e+04 \n\n\nCode\n# Visualize predicted vs observed\nggplot(data.frame(obs = test$Sale_Price, pred = step_pred), aes(obs, pred)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Stepwise Regression: Observed vs Predicted\",\n       x = \"Observed Price\", y = \"Predicted Price\")\n\n\n\n\n\n\n\n\n\nNote:\nStepwise regression is intuitive and fast, but it can be unstable.\nStepwise regression (forward, backward, or both) selects predictors one at a time based on how much they improve a criterion (like AIC or adjusted R²).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#step-2-ridge-and-lasso-regression",
    "href": "resampling-model-selection.html#step-2-ridge-and-lasso-regression",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "2.4 Step 2: Ridge and Lasso Regression",
    "text": "2.4 Step 2: Ridge and Lasso Regression\nRidge and Lasso both shrink coefficients, but in different ways: - Ridge shrinks all coefficients toward zero. - Lasso can set some coefficients exactly to zero, performing variable selection.\n\n\nCode\nx_train &lt;- model.matrix(Sale_Price ~ ., train)[, -1]\ny_train &lt;- train$Sale_Price\nx_test  &lt;- model.matrix(Sale_Price ~ ., test)[, -1]\ny_test  &lt;- test$Sale_Price\n\nset.seed(123)\ncv_ridge &lt;- cv.glmnet(x_train, y_train, alpha = 0, nfolds = 10)\ncv_lasso &lt;- cv.glmnet(x_train, y_train, alpha = 1, nfolds = 10)\n\n\n\n2.4.1 Visualizing RMSE and Model Complexity\n\n\nCode\nbuild_path_df &lt;- function(cvfit, label) {\n  fit &lt;- cvfit$glmnet.fit\n  tibble(\n    lambda = fit$lambda,\n    log_lambda = log(fit$lambda),\n    RMSE = sqrt(cvfit$cvm),\n    nonzero = colSums(abs(fit$beta) &gt; 0),\n    Model = label\n  )\n}\n\nridge_df &lt;- build_path_df(cv_ridge, \"Ridge\")\nlasso_df &lt;- build_path_df(cv_lasso, \"Lasso\")\ndf &lt;- bind_rows(ridge_df, lasso_df)\n\nggplot(df, aes(log_lambda, RMSE, color = Model)) +\n  geom_line(size = 1) +\n  labs(title = \"RMSE vs log(lambda)\", y = \"Cross-validated RMSE\", x = \"log(lambda)\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(log_lambda, nonzero, color = Model)) +\n  geom_line(size = 1) +\n  labs(title = \"Number of Nonzero Coefficients vs log(lambda)\",\n       y = \"Number of Nonzero Coefficients\", x = \"log(lambda)\")\n\n\n\n\n\n\n\n\n\nNote:\n- Increasing λ increases regularization.\n- Ridge never eliminates variables, Lasso can.\n- There’s a sweet spot where RMSE is minimized.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#step-3-elastic-net-balancing-ridge-and-lasso",
    "href": "resampling-model-selection.html#step-3-elastic-net-balancing-ridge-and-lasso",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "2.5 Step 3: Elastic Net (Balancing Ridge and Lasso)",
    "text": "2.5 Step 3: Elastic Net (Balancing Ridge and Lasso)\nElastic Net introduces α to control the mix between Ridge (α = 0) and Lasso (α = 1).\n\n\nCode\nalpha_grid &lt;- seq(0, 1, by = 0.25)\n\nelastic_results &lt;- map_df(alpha_grid, function(a) {\n  cv_fit &lt;- cv.glmnet(x_train, y_train, alpha = a, nfolds = 10)\n  tibble(\n    alpha = a,\n    best_lambda = cv_fit$lambda.min,\n    best_RMSE = sqrt(min(cv_fit$cvm)),\n    nonzero = sum(abs(coef(cv_fit, s = \"lambda.min\")[-1]) &gt; 0)\n  )\n})\n\nelastic_results\n\n\n# A tibble: 5 × 4\n  alpha best_lambda best_RMSE nonzero\n  &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;int&gt;\n1  0          5603.    42664.      14\n2  0.25       1224.    42675.      14\n3  0.5        1552.    42952.      11\n4  0.75        650.    42612.      11\n5  1          1126.    43048.      10\n\n\n\n2.5.1 Visualizing α Effects\n\n\nCode\nggplot(elastic_results, aes(alpha, best_RMSE)) +\n  geom_line() + geom_point(size = 2) +\n  labs(title = \"Elastic Net: Best RMSE vs Alpha\",\n       y = \"Best Cross-validated RMSE\", x = \"Alpha\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(elastic_results, aes(alpha, nonzero)) +\n  geom_line() + geom_point(size = 2) +\n  labs(title = \"Elastic Net: Model Size vs Alpha\",\n       y = \"Number of Nonzero Coefficients\", x = \"Alpha\")\n\n\n\n\n\n\n\n\n\nNote:\nElastic Net combines the strengths of Ridge and Lasso: - Ridge for stability. - Lasso for sparsity. - Often performs best at intermediate α values.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#step-4-comparing-all-models-on-the-test-set",
    "href": "resampling-model-selection.html#step-4-comparing-all-models-on-the-test-set",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "2.6 Step 4: Comparing All Models on the Test Set",
    "text": "2.6 Step 4: Comparing All Models on the Test Set\nWe’ll compare Stepwise, Ridge, Lasso, and the best Elastic Net.\n\n\nCode\nridge_pred &lt;- predict(cv_ridge, newx = x_test, s = \"lambda.min\")\nlasso_pred &lt;- predict(cv_lasso, newx = x_test, s = \"lambda.min\")\n\nbest_alpha &lt;- elastic_results %&gt;% arrange(best_RMSE) %&gt;% slice(1) %&gt;% pull(alpha)\ncv_en_best &lt;- cv.glmnet(x_train, y_train, alpha = best_alpha, nfolds = 10)\nelastic_pred &lt;- predict(cv_en_best, newx = x_test, s = \"lambda.min\")\n\n\n\ncompare_tbl &lt;- bind_rows(\n  Stepwise = as_tibble_row(postResample(step_pred, test$Sale_Price)),\n  Ridge = as_tibble_row(postResample(ridge_pred, y_test)),\n  Lasso = as_tibble_row(postResample(lasso_pred, y_test)),\n  Elastic = as_tibble_row(postResample(elastic_pred, y_test)),\n  .id = \"Model\"\n)\n\ncolnames(compare_tbl) &lt;- c(\"Model\", \"RMSE\", \"Rsquared\", \"MAE\")\ncompare_tbl &lt;- compare_tbl %&gt;% arrange(RMSE)\ncompare_tbl\n\n\n# A tibble: 4 × 4\n  Model      RMSE Rsquared    MAE\n  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Stepwise 38208.    0.767 26131.\n2 Lasso    38297.    0.767 26045.\n3 Elastic  38301.    0.767 26039.\n4 Ridge    38412.    0.766 26022.\n\n\n\n\nCode\n# ------------------------------------------------------------\n\n# Display the best cross-validated Lasso, Ridge, and Elastic Net models\n\n\n# ------------------------------------------------------------\n\nlibrary(glmnet)\nlibrary(knitr)\n\n# Helper function to summarize a fitted cv.glmnet model\n\nsummarize_glmnet &lt;- function(cvfit, X_train, y_train, X_test, y_test, model_name) {\n  cat(\"\\n============================================\\n\")\n  cat(\"Model:\", model_name, \"\\n\")\n  cat(\"============================================\\n\")\n  \n  # Extract best lambda\n  best_lambda &lt;- cvfit$lambda.min\n  cat(\"Best Lambda (lambda.min):\", round(best_lambda, 6), \"\\n\")\n  \n  # Coefficients (non-zero)\n  coefs &lt;- as.matrix(coef(cvfit, s = \"lambda.min\"))\n  nonzero &lt;- coefs[coefs != 0, , drop = FALSE]\n  \n  # Print nonzero coefficients\n  cat(\"\\nNonzero Coefficients:\\n\")\n  print(knitr::kable(as.data.frame(nonzero), digits = 4))\n  \n  # Predictions and performance\n  pred_train &lt;- predict(cvfit, newx = X_train, s = \"lambda.min\")\n  pred_test  &lt;- predict(cvfit, newx = X_test,  s = \"lambda.min\")\n  \n  rmse &lt;- function(y, yhat) sqrt(mean((y - yhat)^2))\n  rsq  &lt;- function(y, yhat) 1 - sum((y - yhat)^2) / sum((y - mean(y))^2)\n  \n  train_rmse &lt;- rmse(y_train, pred_train)\n  test_rmse  &lt;- rmse(y_test, pred_test)\n  test_r2    &lt;- rsq(y_test, pred_test)\n  \n  # Return one-row summary\n  data.frame(\n    Model = model_name,\n    Lambda = round(best_lambda, 6),\n    Train_RMSE = round(train_rmse, 2),\n    Test_RMSE = round(test_rmse, 2),\n    Test_R2 = round(test_r2, 3),\n    Nonzero_Coeff = nrow(nonzero)\n  )\n}\n\n# Summarize and display all three models\nresults &lt;- rbind(\n  summarize_glmnet(cv_lasso, x_train, y_train, x_test, y_test, \"Lasso\"),\n  summarize_glmnet(cv_ridge, x_train, y_train, x_test, y_test, \"Ridge\"),\n  summarize_glmnet(cv_en_best, x_train, y_train, x_test, y_test, \"Elastic Net\")\n)\n\n\n\n============================================\nModel: Lasso \n============================================\nBest Lambda (lambda.min): 444.0297 \n\nNonzero Coefficients:\n\n\n|              |    lambda.min|\n|:-------------|-------------:|\n|(Intercept)   | -1536712.4502|\n|Bedroom_AbvGr |   -12871.7248|\n|Year_Built    |      786.0097|\n|Lot_Area      |        0.3320|\n|StreetPave    |    14782.6929|\n|Central_AirY  |     3463.4742|\n|First_Flr_SF  |       22.9872|\n|Half_Bath     |    -1509.7005|\n|Fireplaces    |    11647.3233|\n|Garage_Area   |       62.2549|\n|Gr_Liv_Area   |       74.6167|\n|TotRms_AbvGrd |     1301.9029|\n\n============================================\nModel: Ridge \n============================================\nBest Lambda (lambda.min): 5603.019 \n\nNonzero Coefficients:\n\n\n|              |    lambda.min|\n|:-------------|-------------:|\n|(Intercept)   | -1398127.9251|\n|Bedroom_AbvGr |   -13051.9562|\n|Year_Built    |      706.9534|\n|Mo_Sold       |      105.9352|\n|Lot_Area      |        0.4001|\n|StreetPave    |    22908.4375|\n|Central_AirY  |     6951.0203|\n|First_Flr_SF  |       45.3168|\n|Second_Flr_SF |       21.9030|\n|Full_Bath     |     4203.4612|\n|Half_Bath     |      -26.1950|\n|Fireplaces    |    12666.1199|\n|Garage_Area   |       64.5695|\n|Gr_Liv_Area   |       42.3845|\n|TotRms_AbvGrd |     3067.4622|\n\n============================================\nModel: Elastic Net \n============================================\nBest Lambda (lambda.min): 592.0396 \n\nNonzero Coefficients:\n\n\n|              |    lambda.min|\n|:-------------|-------------:|\n|(Intercept)   | -1532932.2050|\n|Bedroom_AbvGr |   -12808.7530|\n|Year_Built    |      783.9250|\n|Lot_Area      |        0.3319|\n|StreetPave    |    14821.6671|\n|Central_AirY  |     3497.9772|\n|First_Flr_SF  |       23.2544|\n|Half_Bath     |    -1319.3051|\n|Fireplaces    |    11686.0135|\n|Garage_Area   |       62.4230|\n|Gr_Liv_Area   |       73.9778|\n|TotRms_AbvGrd |     1388.0576|\n\n\nCode\n# Combined performance table\ncat(\"\\n\\n***Model Performance Comparison***\\n\")\n\n\n\n\n***Model Performance Comparison***\n\n\nCode\nkable(results, caption = \"Performance of Cross-Validated Regularized Models\")\n\n\n\nPerformance of Cross-Validated Regularized Models\n\n\nModel\nLambda\nTrain_RMSE\nTest_RMSE\nTest_R2\nNonzero_Coeff\n\n\n\n\nLasso\n444.0297\n41745.42\n38296.94\n0.764\n12\n\n\nRidge\n5603.0194\n41830.59\n38411.78\n0.763\n15\n\n\nElastic Net\n592.0396\n41748.11\n38300.92\n0.764\n12",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection.html#key-takeaways",
    "href": "resampling-model-selection.html#key-takeaways",
    "title": "2  Model Selection and Regularization - R Version",
    "section": "2.7 Key Takeaways",
    "text": "2.7 Key Takeaways\n\nStepwise selection is simple but can lead to unstable models.\n\nRidge adds stability via coefficient shrinkage.\n\nLasso enforces sparsity by removing weak predictors.\n\nElastic Net balances both effects.\n\nRegularization often produces models that generalize better than purely stepwise ones.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Selection and Regularization - R Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html",
    "href": "resampling-model-selection-python.html",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "",
    "text": "3.1 Overview\nThis section parallels the R version of Model Selection and Regularization.\nWe will:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#overview",
    "href": "resampling-model-selection-python.html#overview",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "",
    "text": "Load the Ames Housing data\n\nSplit into training and test sets\n\nPerform Ridge, Lasso, and Elastic Net regressions with 10-fold CV\n\nVisualize RMSE and coefficient shrinkage patterns\n\nCompare test set performance",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#step-0-setup-and-data",
    "href": "resampling-model-selection-python.html#step-0-setup-and-data",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.2 Step 0: Setup and Data",
    "text": "3.2 Step 0: Setup and Data\nWe’ll use scikit-learn for modeling.\nIf the CSVs ames_training.csv and ames_testing.csv are not present, you can load Ames Housing via the fetch_openml function.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error\n\n# Load Ames dataset directly\names = fetch_openml(name=\"house_prices\", as_frame=True)\ndf = ames.frame\n\n# Subset variables to match the R version\nkeep = [\n    \"SalePrice\",\"BedroomAbvGr\",\"YearBuilt\",\"MoSold\",\"LotArea\",\"Street\",\"CentralAir\",\n    \"1stFlrSF\",\"2ndFlrSF\",\"FullBath\",\"HalfBath\",\"Fireplaces\",\"GarageArea\",\n    \"GrLivArea\",\"TotRmsAbvGrd\"\n]\ndf = df[keep].copy()\n\n# Clean up names to match Python variable rules\ndf.columns = [\"Sale_Price\",\"Bedroom_AbvGr\",\"Year_Built\",\"Mo_Sold\",\"Lot_Area\",\"Street\",\n              \"Central_Air\",\"First_Flr_SF\",\"Second_Flr_SF\",\"Full_Bath\",\"Half_Bath\",\n              \"Fireplaces\",\"Garage_Area\",\"Gr_Liv_Area\",\"TotRms_AbvGrd\"]\n\n# Drop missing rows\ndf = df.dropna()\n\n# Train/test split (70/30)\ntrain, test = train_test_split(df, test_size=0.3, random_state=123)\n\n# Identify predictors and target\ny_train = train[\"Sale_Price\"]\ny_test  = test[\"Sale_Price\"]\nX_train = train.drop(columns=\"Sale_Price\")\nX_test  = test.drop(columns=\"Sale_Price\")\n\ncat_vars = [\"Street\",\"Central_Air\"]\nnum_vars = [c for c in X_train.columns if c not in cat_vars]\n\n# Preprocess: scale numeric, one-hot encode categorical\npreprocessor = ColumnTransformer([\n    (\"num\", StandardScaler(), num_vars),\n    (\"cat\", OneHotEncoder(drop=\"first\"), cat_vars)\n])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#step-1-ridge-regression",
    "href": "resampling-model-selection-python.html#step-1-ridge-regression",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.3 Step 1: Ridge Regression",
    "text": "3.3 Step 1: Ridge Regression\nWe’ll use cross-validation to tune λ (alpha).\nThen we visualize RMSE vs log(lambda) and the number of nonzero coefficients.\n\n\nCode\nalphas = np.logspace(4, -4, 80)\nridge = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", RidgeCV(alphas=alphas, scoring=\"neg_mean_squared_error\", cv=10))\n])\nridge.fit(X_train, y_train)\n\nridge_best = ridge.named_steps[\"model\"].alpha_\nprint(\"Best Ridge alpha:\", ridge_best)\n\n# Evaluate RMSE on test set\nridge_pred = ridge.predict(X_test)\nridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))\nprint(\"Test RMSE (Ridge):\", ridge_rmse)\n\n\nBest Ridge alpha: 94.33732216299774\nTest RMSE (Ridge): 37055.84511170759",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#step-2-lasso-regression",
    "href": "resampling-model-selection-python.html#step-2-lasso-regression",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.4 Step 2: Lasso Regression",
    "text": "3.4 Step 2: Lasso Regression\nLasso adds variable selection — some coefficients go exactly to zero.\n\n\nCode\nlasso = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", LassoCV(alphas=alphas, cv=10, max_iter=20000))\n])\nlasso.fit(X_train, y_train)\n\nlasso_best = lasso.named_steps[\"model\"].alpha_\nprint(\"Best Lasso alpha:\", lasso_best)\n\nlasso_pred = lasso.predict(X_test)\nlasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_pred))\nprint(\"Test RMSE (Lasso):\", lasso_rmse)\n\n\nBest Lasso alpha: 11.568875283162821\nTest RMSE (Lasso): 37074.74978063246",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#step-3-visualize-coefficient-shrinkage-and-rmse",
    "href": "resampling-model-selection-python.html#step-3-visualize-coefficient-shrinkage-and-rmse",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.5 Step 3: Visualize Coefficient Shrinkage and RMSE",
    "text": "3.5 Step 3: Visualize Coefficient Shrinkage and RMSE\nLet’s visualize how RMSE and model sparsity change with λ.\n\n\nCode\n# Compute RMSE path manually\ndef cv_rmse(model_class, X, y, alphas):\n    rmse = []\n    for a in alphas:\n        model = Pipeline([\n            (\"prep\", preprocessor),\n            (\"model\", model_class(alpha=a, max_iter=20000))\n        ])\n        scores = cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=KFold(10, shuffle=True, random_state=123))\n        rmse.append(np.sqrt(-scores.mean()))\n    return np.array(rmse)\n\nfrom sklearn.linear_model import Ridge, Lasso\nridge_rmse_path = cv_rmse(Ridge, X_train, y_train, alphas)\nlasso_rmse_path = cv_rmse(Lasso, X_train, y_train, alphas)\n\nplt.figure()\nplt.plot(np.log(alphas), ridge_rmse_path, label=\"Ridge\")\nplt.plot(np.log(alphas), lasso_rmse_path, label=\"Lasso\")\nplt.xlabel(\"log(lambda)\")\nplt.ylabel(\"Cross-validated RMSE\")\nplt.title(\"RMSE vs log(lambda)\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#step-4-elastic-net",
    "href": "resampling-model-selection-python.html#step-4-elastic-net",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.6 Step 4: Elastic Net",
    "text": "3.6 Step 4: Elastic Net\nElastic Net blends Ridge and Lasso.\nWe’ll vary α (the mix) and visualize the tradeoff.\n\n\nCode\nl1_ratios = np.linspace(0, 1, 6)\nen_cv = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", ElasticNetCV(l1_ratio=l1_ratios, alphas=alphas, cv=10, max_iter=50000))\n])\nen_cv.fit(X_train, y_train)\n\nprint(\"Best Elastic Net alpha:\", en_cv.named_steps[\"model\"].alpha_)\nprint(\"Best Elastic Net l1_ratio:\", en_cv.named_steps[\"model\"].l1_ratio_)\n\nen_pred = en_cv.predict(X_test)\nen_rmse = np.sqrt(mean_squared_error(y_test, en_pred))\nprint(\"Test RMSE (Elastic Net):\", en_rmse)\n\n\nBest Elastic Net alpha: 0.55825862688627\nBest Elastic Net l1_ratio: 0.8\nTest RMSE (Elastic Net): 37134.497858434275",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#step-5-compare-models",
    "href": "resampling-model-selection-python.html#step-5-compare-models",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.7 Step 5: Compare Models",
    "text": "3.7 Step 5: Compare Models\n\n\nCode\nresults = pd.DataFrame({\n    \"Model\": [\"Ridge\", \"Lasso\", \"Elastic Net\"],\n    \"Test_RMSE\": [ridge_rmse, lasso_rmse, en_rmse]\n})\nprint(results.sort_values(\"Test_RMSE\"))\n\n\n         Model     Test_RMSE\n0        Ridge  37055.845112\n1        Lasso  37074.749781\n2  Elastic Net  37134.497858",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "resampling-model-selection-python.html#summary",
    "href": "resampling-model-selection-python.html#summary",
    "title": "3  Model Selection and Regularization - Python Version",
    "section": "3.8 Summary",
    "text": "3.8 Summary\n\nRidge stabilizes coefficients by shrinking them.\n\nLasso enforces sparsity by zeroing weak predictors.\n\nElastic Net blends both for balance.\n\nRegularization often beats pure stepwise regression on unseen data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Selection and Regularization - Python Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html",
    "href": "generalized-additive-models-r.html",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "",
    "text": "4.1 1. Setup and Data\nCode\nset.seed(4321)\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(AmesHousing)   # make_ordinal_ames()\nlibrary(earth)         # MARS\nlibrary(segmented)     # piecewise (segmented) regression\nlibrary(splines)       # regression splines (bs, ns) - kept for reference\nlibrary(mgcv)          # smoothing splines via GAM\nlibrary(caret)         # data splitting / utilities\nlibrary(Metrics)       # rmse, mse helpers (plus we'll compute R2 manually)\ntheme_set(theme_minimal())\nCode\n# Limit to requested columns\ncols &lt;- c(\"Sale_Price\",\"Bedroom_AbvGr\",\"Year_Built\",\"Mo_Sold\",\"Lot_Area\",\"Street\",\n          \"Central_Air\",\"First_Flr_SF\",\"Second_Flr_SF\",\"Full_Bath\",\"Half_Bath\",\n          \"Fireplaces\",\"Garage_Area\",\"Gr_Liv_Area\",\"TotRms_AbvGrd\")\n\names &lt;- make_ordinal_ames() |&gt; \n  dplyr::select(dplyr::all_of(cols)) |&gt; \n  tidyr::drop_na()\n\n# Train/test split\nset.seed(4321)\nidx &lt;- sample.int(nrow(ames), size = floor(0.7*nrow(ames)))\ntrain &lt;- ames[idx,]\ntest  &lt;- ames[-idx,]\n\nglimpse(train)\n\n\nRows: 2,051\nColumns: 15\n$ Sale_Price    &lt;int&gt; 274000, 75200, 329900, 145400, 108000, 184000, 176000, 1…\n$ Bedroom_AbvGr &lt;int&gt; 3, 2, 4, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 3, 2, 3, 3, 2,…\n$ Year_Built    &lt;int&gt; 2001, 1922, 2005, 1926, 1949, 1999, 1962, 1915, 1999, 19…\n$ Mo_Sold       &lt;int&gt; 1, 9, 8, 5, 5, 6, 5, 6, 9, 6, 8, 5, 7, 7, 4, 8, 5, 11, 5…\n$ Lot_Area      &lt;int&gt; 9720, 3672, 11643, 7000, 8777, 5858, 19296, 8094, 3768, …\n$ Street        &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pa…\n$ Central_Air   &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y,…\n$ First_Flr_SF  &lt;int&gt; 1366, 816, 1544, 861, 1126, 1337, 1382, 1048, 713, 792, …\n$ Second_Flr_SF &lt;int&gt; 581, 0, 814, 424, 0, 0, 0, 720, 739, 725, 0, 1151, 695, …\n$ Full_Bath     &lt;int&gt; 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1,…\n$ Half_Bath     &lt;int&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,…\n$ Fireplaces    &lt;int&gt; 1, 0, 1, 0, 0, 1, 1, 0, 0, 2, 0, 1, 2, 1, 0, 1, 0, 0, 1,…\n$ Garage_Area   &lt;dbl&gt; 725, 100, 784, 506, 520, 511, 884, 576, 506, 400, 0, 434…\n$ Gr_Liv_Area   &lt;int&gt; 1947, 816, 2358, 1285, 1126, 1337, 1382, 1768, 1452, 151…\n$ TotRms_AbvGrd &lt;int&gt; 7, 5, 10, 6, 5, 5, 6, 8, 6, 7, 5, 8, 7, 5, 6, 7, 6, 6, 5…\n\n\nCode\nsummary(train$Sale_Price)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12789  130000  161000  180897  215000  755000",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#quick-visual-nonlinearity-is-common",
    "href": "generalized-additive-models-r.html#quick-visual-nonlinearity-is-common",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.2 2. Quick Visual: Nonlinearity is Common",
    "text": "4.2 2. Quick Visual: Nonlinearity is Common\n\n\nCode\nggplot(train, aes(Gr_Liv_Area, Sale_Price)) +\n  geom_point(alpha=.3) +\n  geom_smooth(method=\"loess\", se=FALSE) +\n  labs(title=\"Sale_Price vs Gr_Liv_Area (LOESS smoother)\")\n\n\n\n\n\n\n\n\n\nTakeaway: The relationship bends—nonlinear methods can help.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#piecewise-regression",
    "href": "generalized-additive-models-r.html#piecewise-regression",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.3 3. Piecewise Regression",
    "text": "4.3 3. Piecewise Regression\n\n\nCode\nm_lin &lt;- lm(Sale_Price ~ Gr_Liv_Area, data=train)\nm_seg_init &lt;- lm(Sale_Price ~ Gr_Liv_Area, data=train)\nm_seg &lt;- segmented(m_seg_init, seg.Z = ~ Gr_Liv_Area, psi = list(Gr_Liv_Area = 2000))\nsummary(m_seg)\n\n\n\n    ***Regression Model with Segmented Relationship(s)***\n\nCall: \nsegmented.lm(obj = m_seg_init, seg.Z = ~Gr_Liv_Area, psi = list(Gr_Liv_Area = 2000))\n\nEstimated Break-Point(s):\n                  Est.  St.Err\npsi1.Gr_Liv_Area 2466 332.881\n\nCoefficients of the linear terms:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7901.186   4698.641   1.682   0.0928 .  \nGr_Liv_Area     115.705      3.171  36.488   &lt;2e-16 ***\nU1.Gr_Liv_Area  -23.479     12.436  -1.888       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55110 on 2047 degrees of freedom\nMultiple R-Squared: 0.5124,  Adjusted R-squared: 0.5116 \n\nBoot restarting based on 6 samples. Last fit:\nConvergence attained in 1 iterations (rel. change 0)\n\n\n\n\nCode\nplot(train$Gr_Liv_Area, train$Sale_Price, pch=16, cex=.5,\n     xlab=\"Gr_Liv_Area\", ylab=\"Sale_Price\", main=\"Piecewise fit at estimated knot\")\nplot(m_seg, add=TRUE, col=2, lwd=2)\nabline(v = m_seg$psi[,\"Est.\"], lty=2, col=2)\n\n\n\n\n\n\n\n\n\nInterpretation (piecewise): Two linear slopes before/after a knot. Useful when you expect a threshold effect.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#mars-earth-from-simple-to-interactions",
    "href": "generalized-additive-models-r.html#mars-earth-from-simple-to-interactions",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.4 4. MARS (earth) – From Simple to Interactions",
    "text": "4.4 4. MARS (earth) – From Simple to Interactions\nMARS = Multivariate Adaptive Regression Splines. It builds hinge basis functions like h(x - c) = max(0, x - c) and can interact them.\n- A term such as h(Gr_Liv_Area - 1500) means once Gr_Liv_Area exceeds 1500, the fitted line’s slope can change.\n- An interaction like h(Gr_Liv_Area - 1500) * Central_AirY means the slope change applies when Central_Air == \"Y\".\n\n4.4.1 4A. Univariate MARS (Garage_Area)\n\n\nCode\nmars1 &lt;- earth(Sale_Price ~ Garage_Area, data=train)\nsummary(mars1)\n\n\nCall: earth(formula=Sale_Price~Garage_Area, data=train)\n\n                    coefficients\n(Intercept)           124159.039\nh(286-Garage_Area)       -60.257\nh(Garage_Area-286)       297.277\nh(Garage_Area-521)      -483.642\nh(Garage_Area-576)       733.859\nh(Garage_Area-758)      -356.460\nh(Garage_Area-1043)     -490.873\n\nSelected 7 of 7 terms, and 1 of 1 predictors\nTermination condition: RSq changed by less than 0.001 at 7 terms\nImportance: Garage_Area\nNumber of terms at each degree of interaction: 1 6 (additive model)\nGCV 3427475346    RSS 6.94092e+12    GRSq 0.4492014    RSq 0.4556309\n\n\n\n\nCode\ngrid &lt;- tibble(Garage_Area = seq(min(train$Garage_Area), max(train$Garage_Area), length.out=200))\ngrid$pred &lt;- predict(mars1, newdata=grid)\n\nggplot(train, aes(Garage_Area, Sale_Price)) +\n  geom_point(alpha=.3) +\n  geom_line(data=grid, aes(Garage_Area, pred), color=\"blue\", linewidth=1.2) +\n  labs(title=\"Step 1: MARS with One Predictor (Garage_Area)\",\n       subtitle=\"Piecewise linear fit with automatically chosen knots\",\n       y=\"Predicted Sale_Price\")\n\n\n\n\n\n\n\n\n\n\n\n4.4.2 4B. Additive MARS (degree = 1; no interactions)\n\n\nCode\nmars2 &lt;- earth(Sale_Price ~ Bedroom_AbvGr + Year_Built + Mo_Sold + Lot_Area +\n                 Street + Central_Air + First_Flr_SF + Second_Flr_SF + Full_Bath +\n                 Half_Bath + Fireplaces + Garage_Area + Gr_Liv_Area + TotRms_AbvGrd,\n               data=train, degree=1, nfold=5)\nsummary(mars2)\n\n\nCall: earth(formula=Sale_Price~Bedroom_AbvGr+Year_Built+Mo_Sold+Lot_Ar...),\n            data=train, degree=1, nfold=5)\n\n                      coefficients\n(Intercept)              319493.46\nCentral_AirY              20289.49\nh(4-Bedroom_AbvGr)         9214.66\nh(Bedroom_AbvGr-4)       -23009.05\nh(Year_Built-1977)         1275.57\nh(2004-Year_Built)         -336.64\nh(Year_Built-2004)         5315.57\nh(13869-Lot_Area)            -2.09\nh(Lot_Area-13869)             0.22\nh(First_Flr_SF-1600)        104.91\nh(2402-First_Flr_SF)        -71.56\nh(First_Flr_SF-2402)       -176.61\nh(1523-Second_Flr_SF)       -53.13\nh(Second_Flr_SF-1523)       426.63\nh(Half_Bath-1)           -45378.31\nh(2-Fireplaces)          -14408.56\nh(Fireplaces-2)          -26072.58\nh(Garage_Area-539)          101.97\nh(Garage_Area-1043)        -294.30\nh(Gr_Liv_Area-2049)          65.21\nh(Gr_Liv_Area-3194)        -159.79\n\nSelected 21 of 24 terms, and 10 of 14 predictors\nTermination condition: Reached nk 29\nImportance: First_Flr_SF, Second_Flr_SF, Year_Built, Garage_Area, ...\nNumber of terms at each degree of interaction: 1 20 (additive model)\nGCV 1033819964  RSS 2.036439e+12  GRSq 0.8338641  RSq 0.8402842  CVRSq 0.7978671\n\nNote: the cross-validation sd's below are standard deviations across folds\n\nCross validation:   nterms 21.00 sd 0.71    nvars 9.20 sd 0.84\n\n     CVRSq    sd     MaxErr     sd\n     0.798 0.043    -428941 282429\n\n\n\n\nCode\nev2 &lt;- evimp(mars2); plot(ev2, main=\"Step 2: Variable Importance (degree=1, additive)\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2)); plotmo(mars2, type=\"response\", nresponse=1, do.par=FALSE); par(mfrow=c(1,1))\n\n\n plotmo grid:    Bedroom_AbvGr Year_Built Mo_Sold Lot_Area Street Central_Air\n                             3       1974       6     9480   Pave           Y\n First_Flr_SF Second_Flr_SF Full_Bath Half_Bath Fireplaces Garage_Area\n         1092             0         2         0          1         480\n Gr_Liv_Area TotRms_AbvGrd\n        1442             6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.3 4C. MARS with 2-way Interactions (degree = 2)\n\n\nCode\nmars3 &lt;- earth(Sale_Price ~ Bedroom_AbvGr + Year_Built + Mo_Sold + Lot_Area +\n                 Street + Central_Air + First_Flr_SF + Second_Flr_SF + Full_Bath +\n                 Half_Bath + Fireplaces + Garage_Area + Gr_Liv_Area + TotRms_AbvGrd,\n               data=train, degree=2, nfold=5)\nsummary(mars3)\n\n\nCall: earth(formula=Sale_Price~Bedroom_AbvGr+Year_Built+Mo_Sold+Lot_Ar...),\n            data=train, degree=2, nfold=5)\n\n                                           coefficients\n(Intercept)                                   411949.27\nh(Year_Built-2004)                            169869.37\nh(14803-Lot_Area)                                 -1.75\nh(Lot_Area-14803)                                  0.48\nh(1570-First_Flr_SF)                            -150.79\nh(First_Flr_SF-1570)                             224.90\nh(1523-Second_Flr_SF)                            -86.91\nh(Second_Flr_SF-1523)                            156.02\nh(1-Half_Bath)                                 -7743.19\nh(Half_Bath-1)                                -54819.24\nh(2-Fireplaces)                               -10516.92\nh(1043-Garage_Area)                              -70.97\nh(Garage_Area-1043)                             -141.86\nh(Gr_Liv_Area-3194)                              106.27\nh(3-Bedroom_AbvGr) * h(1523-Second_Flr_SF)         7.01\nh(Bedroom_AbvGr-3) * h(1523-Second_Flr_SF)        -9.61\nh(2004-Year_Built) * h(First_Flr_SF-876)          -1.86\nh(2004-Year_Built) * h(3194-Gr_Liv_Area)          -0.17\nh(Year_Built-2004) * h(Gr_Liv_Area-2320)        -159.80\nh(Year_Built-2004) * h(2320-Gr_Liv_Area)         156.64\nh(Year_Built-2004) * h(3194-Gr_Liv_Area)        -173.05\nh(First_Flr_SF-1778) * h(2-Fireplaces)           -78.19\nh(1043-Garage_Area) * h(2122-Gr_Liv_Area)          0.05\n\nSelected 23 of 29 terms, and 9 of 14 predictors\nTermination condition: Reached nk 29\nImportance: First_Flr_SF, Second_Flr_SF, Year_Built, Gr_Liv_Area, ...\nNumber of terms at each degree of interaction: 1 13 9\nGCV 795873462  RSS 1.544416e+12  GRSq 0.8721024  RSq 0.8788731  CVRSq 0.7824362\n\nNote: the cross-validation sd's below are standard deviations across folds\n\nCross validation:   nterms 22.60 sd 1.82    nvars 9.40 sd 1.14\n\n     CVRSq    sd     MaxErr     sd\n     0.782 0.114    -945502 422348\n\n\n\n\nCode\nev3 &lt;- evimp(mars3); plot(ev3, main=\"Step 3: Variable Importance (degree=2, interactions)\")\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2)); plotmo(mars3, type=\"response\", nresponse=1, do.par=FALSE); par(mfrow=c(1,1))\n\n\n plotmo grid:    Bedroom_AbvGr Year_Built Mo_Sold Lot_Area Street Central_Air\n                             3       1974       6     9480   Pave           Y\n First_Flr_SF Second_Flr_SF Full_Bath Half_Bath Fireplaces Garage_Area\n         1092             0         2         0          1         480\n Gr_Liv_Area TotRms_AbvGrd\n        1442             6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting MARS terms:\nLook for h() pieces and products. Coefficients on h(x-c) indicate how slope changes after the knot c. Interaction products mean “the slope change depends on another variable”.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#loess-visual-local-regression",
    "href": "generalized-additive-models-r.html#loess-visual-local-regression",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.5 5. LOESS (Visual local regression)",
    "text": "4.5 5. LOESS (Visual local regression)\n\n\nCode\nloess_fit &lt;- loess(Sale_Price ~ Gr_Liv_Area, data=train, span=0.4)\ngrid_lo &lt;- tibble(Gr_Liv_Area = seq(min(train$Gr_Liv_Area), max(train$Gr_Liv_Area), length.out=200))\ngrid_lo$pred &lt;- predict(loess_fit, newdata=grid_lo)\n\nggplot() +\n  geom_point(data=train, aes(Gr_Liv_Area, Sale_Price), alpha=.25) +\n  geom_line(data=grid_lo, aes(Gr_Liv_Area, pred), linewidth=1.2) +\n  labs(title=\"LOESS fit (span=0.4)\", y=\"Predicted Sale_Price\")\n\n\n\n\n\n\n\n\n\nWe keep LOESS as a visualization-oriented smoother (not used in the multivariate comparison below).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#gam-splines-with-mgcv-simple-complex",
    "href": "generalized-additive-models-r.html#gam-splines-with-mgcv-simple-complex",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.6 6. GAM Splines with mgcv – Simple → Complex",
    "text": "4.6 6. GAM Splines with mgcv – Simple → Complex\nGAM smooths are written s(x) and estimated with penalized splines. The edf (effective degrees of freedom) in the summary tells you the smooth’s complexity (larger edf ⇒ wigglier function).\n\n4.6.1 6A. Univariate GAM: Sale_Price ~ s(Garage_Area)\n\n\nCode\ngam1 &lt;- gam(Sale_Price ~ s(Garage_Area, k=9), data=train, method=\"REML\")\nsummary(gam1)  # inspect edf and significance\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nSale_Price ~ s(Garage_Area, k = 9)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   180897       1292     140   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df     F p-value    \ns(Garage_Area) 7.078  7.719 217.1  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =   0.45   Deviance explained = 45.2%\n-REML =  25418  Scale est. = 3.4232e+09  n = 2051\n\n\nCode\nplot(gam1, pages=1, shade=TRUE, scheme=1, scale=0)\n\n\n\n\n\n\n\n\n\n\n\n4.6.2 6B. Two-smooth Additive GAM: s(Garage_Area) + s(Gr_Liv_Area)\n\n\nCode\ngam2 &lt;- gam(Sale_Price ~ s(Garage_Area, k=9) + s(Gr_Liv_Area, k=9), data=train, method=\"REML\")\nsummary(gam2)\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nSale_Price ~ s(Garage_Area, k = 9) + s(Gr_Liv_Area, k = 9)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   180897       1026   176.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df     F p-value    \ns(Garage_Area) 6.446  7.293 103.3  &lt;2e-16 ***\ns(Gr_Liv_Area) 7.585  7.941 152.6  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.653   Deviance explained = 65.5%\n-REML =  24949  Scale est. = 2.1608e+09  n = 2051\n\n\nCode\nplot(gam2, pages=1, shade=TRUE, scheme=1, scale=0)\n\n\n\n\n\n\n\n\n\n\n\n4.6.3 6C. Full Multivariate GAM (selected smooths + factors)\n\n\nCode\ngam3 &lt;- gam(Sale_Price ~ \n              s(Garage_Area, k=9) + \n              s(Gr_Liv_Area, k=9) + \n              s(Year_Built, k=7) +\n              s(Lot_Area, k=7) +\n              Bedroom_AbvGr + Mo_Sold + \n              Street + Central_Air + \n              First_Flr_SF + Second_Flr_SF + \n              Full_Bath + Half_Bath + \n              Fireplaces + TotRms_AbvGrd,\n            data=train, method=\"REML\")\nsummary(gam3)\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nSale_Price ~ s(Garage_Area, k = 9) + s(Gr_Liv_Area, k = 9) + \n    s(Year_Built, k = 7) + s(Lot_Area, k = 7) + Bedroom_AbvGr + \n    Mo_Sold + Street + Central_Air + First_Flr_SF + Second_Flr_SF + \n    Full_Bath + Half_Bath + Fireplaces + TotRms_AbvGrd\n\nParametric coefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -21237.92   30106.84  -0.705   0.4806    \nBedroom_AbvGr -10350.20    1382.38  -7.487 1.05e-13 ***\nMo_Sold         -409.12     270.24  -1.514   0.1302    \nStreetPave     30589.21   13486.61   2.268   0.0234 *  \nCentral_AirY   17189.68    3366.03   5.107 3.59e-07 ***\nFirst_Flr_SF     140.27      17.44   8.044 1.47e-15 ***\nSecond_Flr_SF    101.93      17.36   5.871 5.06e-09 ***\nFull_Bath      -4548.94    2211.69  -2.057   0.0398 *  \nHalf_Bath       2355.98    2175.40   1.083   0.2789    \nFireplaces     13066.61    1371.59   9.527  &lt; 2e-16 ***\nTotRms_AbvGrd  -1640.87     948.67  -1.730   0.0838 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df      F p-value    \ns(Garage_Area) 7.067  7.712  24.96  &lt;2e-16 ***\ns(Gr_Liv_Area) 7.899  7.996  46.44  &lt;2e-16 ***\ns(Year_Built)  5.255  5.764 128.05  &lt;2e-16 ***\ns(Lot_Area)    5.465  5.883  16.44  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.827   Deviance explained =   83%\n-REML =  24167  Scale est. = 1.0786e+09  n = 2051\n\n\nCode\nplot(gam3, pages=1, shade=TRUE, scheme=1, scale=0)\n\n\n\n\n\n\n\n\n\nInterpreting GAM splines:\nA smooth s(x) shows how the expected outcome varies with x holding other terms constant. The edf indicates complexity; wide confidence bands suggest greater uncertainty in those regions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#model-comparison-r²-rmse-mse",
    "href": "generalized-additive-models-r.html#model-comparison-r²-rmse-mse",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.7 7. Model Comparison (R², RMSE, MSE)",
    "text": "4.7 7. Model Comparison (R², RMSE, MSE)\n\n\nCode\n# Helper for R^2 on test\nrsq &lt;- function(y, yhat) {\n  1 - sum((y - yhat)^2) / sum((y - mean(y))^2)\n}\n\n# Predictions\np_piece &lt;- predict(m_seg, newdata=test)\np_mars  &lt;- predict(mars3, newdata=test)   # final MARS with interactions\np_gam   &lt;- predict(gam3, newdata=test)    # full multivariate GAM\n\n# Metrics\ntbl &lt;- tibble(\n  Model = c(\"Piecewise (segmented)\", \"MARS (degree=2)\", \"GAM splines (mgcv full)\"),\n  RMSE  = c(rmse(test$Sale_Price, p_piece),\n            rmse(test$Sale_Price, p_mars),\n            rmse(test$Sale_Price, p_gam)),\n  MSE   = c(mse(test$Sale_Price, p_piece),\n            mse(test$Sale_Price, p_mars),\n            mse(test$Sale_Price, p_gam)),\n  R2    = c(rsq(test$Sale_Price, p_piece),\n            rsq(test$Sale_Price, p_mars),\n            rsq(test$Sale_Price, p_gam))\n) |&gt; arrange(desc(R2))\n\ntbl\n\n\n# A tibble: 3 × 4\n  Model                     RMSE         MSE    R2\n  &lt;chr&gt;                    &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 MARS (degree=2)         33028. 1090840972. 0.839\n2 GAM splines (mgcv full) 36796. 1353918715. 0.800\n3 Piecewise (segmented)   59034. 3484963067. 0.484",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#final-plot-predicted-vs-actual-test-set",
    "href": "generalized-additive-models-r.html#final-plot-predicted-vs-actual-test-set",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.8 8. Final Plot: Predicted vs Actual (Test Set)",
    "text": "4.8 8. Final Plot: Predicted vs Actual (Test Set)\n\n\nCode\nplot_df &lt;- bind_rows(\n  tibble(Model=\"Piecewise (segmented)\", Actual=test$Sale_Price, Pred=p_piece),\n  tibble(Model=\"MARS (degree=2)\", Actual=test$Sale_Price, Pred=p_mars),\n  tibble(Model=\"GAM splines (mgcv full)\", Actual=test$Sale_Price, Pred=p_gam)\n)\n\nggplot(plot_df, aes(Actual, Pred, color=Model)) +\n  geom_point(alpha=.35) +\n  geom_abline(intercept=0, slope=1, linetype=\"dashed\") +\n  labs(title=\"Predicted vs Actual (Test Set)\", y=\"Predicted Sale_Price\") +\n  coord_equal()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-r.html#how-to-explain",
    "href": "generalized-additive-models-r.html#how-to-explain",
    "title": "4  GAM – Piecewise, MARS, LOESS, and Splines - R Version",
    "section": "4.9 9. How to Explain",
    "text": "4.9 9. How to Explain\n\nPiecewise: “There’s a threshold where the effect changes.” Simple story, limited flexibility.\n\nMARS: “The model finds breakpoints and sometimes when they matter (interactions).” Explain h(x-c) as extra slope after c.\n\nGAM (splines): “We model smooth curves for key variables; edf shows wiggliness. More edf ⇒ more flexibility.”\n\nTradeoff: More flexibility usually improves error (RMSE/MSE) and R², but reduce interpretability. Validate with cross‑validation and keep the story aligned with business intuition.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GAM – Piecewise, MARS, LOESS, and Splines - R Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-python.html",
    "href": "generalized-additive-models-python.html",
    "title": "5  GAM— Piecewise, LOESS, and GAM Splines - Python Version",
    "section": "",
    "text": "5.1 1. Setup and Data\nCode\nimport sys\nprint(\"Active Python interpreter:\", sys.executable)\n\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Ridge\nimport inspect\n\n# --- Fix for SciPy &gt;= 1.13 removing .A from sparse matrices ---\nimport scipy.sparse as sp\n\nif not hasattr(sp.spmatrix, \"A\"):\n    def _toarray(self):\n        return self.toarray()\n    sp.spmatrix.A = property(_toarray)\n# -------------------------------------------------------------\n\nnp.random.seed(4321)\n\n\nActive Python interpreter: /opt/anaconda3/bin/python\nCode\nfrom sklearn.datasets import fetch_openml\n\names = fetch_openml(name=\"house_prices\", as_frame=True).frame\n\nkeep = {\n    \"SalePrice\":\"SalePrice\",\n    \"BedroomAbvGr\":\"Bedroom_AbvGr\",\n    \"YearBuilt\":\"Year_Built\",\n    \"MoSold\":\"Mo_Sold\",\n    \"LotArea\":\"Lot_Area\",\n    \"Street\":\"Street\",\n    \"CentralAir\":\"Central_Air\",\n    \"1stFlrSF\":\"First_Flr_SF\",\n    \"2ndFlrSF\":\"Second_Flr_SF\",\n    \"FullBath\":\"Full_Bath\",\n    \"HalfBath\":\"Half_Bath\",\n    \"Fireplaces\":\"Fireplaces\",\n    \"GarageArea\":\"Garage_Area\",\n    \"GrLivArea\":\"Gr_Liv_Area\",\n    \"TotRmsAbvGrd\":\"TotRms_AbvGrd\"\n}\n\ndf = ames[list(keep.keys())].rename(columns=keep).dropna().copy()\n\nX = df.drop(columns=[\"SalePrice\"])\ny = df[\"SalePrice\"].values\n\nnum_cols = X.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = [c for c in X.columns if c not in num_cols]\n\n# Handle OneHotEncoder version changes and ensure dense output\nif \"sparse_output\" in inspect.signature(OneHotEncoder).parameters:\n    encoder = OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False)\nelse:\n    encoder = OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse=False)\n\npre = ColumnTransformer([\n    (\"num\", StandardScaler(), num_cols),\n    (\"cat\", encoder, cat_cols)\n], sparse_threshold=0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4321)\ndf.head()\n\n\n\n\n\n\n\n\n\nSalePrice\nBedroom_AbvGr\nYear_Built\nMo_Sold\nLot_Area\nStreet\nCentral_Air\nFirst_Flr_SF\nSecond_Flr_SF\nFull_Bath\nHalf_Bath\nFireplaces\nGarage_Area\nGr_Liv_Area\nTotRms_AbvGrd\n\n\n\n\n0\n208500\n3\n2003\n2\n8450\nPave\nY\n856\n854\n2\n1\n0\n548\n1710\n8\n\n\n1\n181500\n3\n1976\n5\n9600\nPave\nY\n1262\n0\n2\n0\n1\n460\n1262\n6\n\n\n2\n223500\n3\n2001\n9\n11250\nPave\nY\n920\n866\n2\n1\n1\n608\n1786\n6\n\n\n3\n140000\n3\n1915\n2\n9550\nPave\nY\n961\n756\n1\n0\n1\n642\n1717\n7\n\n\n4\n250000\n4\n2000\n12\n14260\nPave\nY\n1145\n1053\n2\n1\n1\n836\n2198\n9",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GAM— Piecewise, LOESS, and GAM Splines - Python Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-python.html#visual-check-for-nonlinearity",
    "href": "generalized-additive-models-python.html#visual-check-for-nonlinearity",
    "title": "5  GAM— Piecewise, LOESS, and GAM Splines - Python Version",
    "section": "5.2 2. Visual Check for Nonlinearity",
    "text": "5.2 2. Visual Check for Nonlinearity\n\n\nCode\nimport statsmodels.api as sm\nfig = plt.figure()\nplt.scatter(df[\"Gr_Liv_Area\"], df[\"SalePrice\"], s=8, alpha=0.3)\nlow = sm.nonparametric.lowess(df[\"SalePrice\"], df[\"Gr_Liv_Area\"], frac=0.3, return_sorted=True)\nplt.plot(low[:,0], low[:,1], linewidth=2)\nplt.title(\"SalePrice vs Gr_Liv_Area (LOESS smoother)\")\nplt.xlabel(\"Gr_Liv_Area\"); plt.ylabel(\"SalePrice\")\nplt.tight_layout()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GAM— Piecewise, LOESS, and GAM Splines - Python Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-python.html#piecewise-regression-example",
    "href": "generalized-additive-models-python.html#piecewise-regression-example",
    "title": "5  GAM— Piecewise, LOESS, and GAM Splines - Python Version",
    "section": "5.3 3. Piecewise Regression Example",
    "text": "5.3 3. Piecewise Regression Example\n\n\nCode\nclass PiecewiseLinear:\n    def __init__(self, knot=2000.0):\n        self.knot = knot\n        self.lin = LinearRegression()\n\n    def _transform(self, x):\n        x1 = np.asarray(x).reshape(-1,1)\n        hx = np.maximum(0, x1 - self.knot)\n        return np.hstack([x1, hx])\n\n    def fit(self, x, y):\n        Z = self._transform(x)\n        self.lin.fit(Z, y)\n        return self\n\n    def predict(self, x):\n        Z = self._transform(x)\n        return self.lin.predict(Z)\n\npw = PiecewiseLinear(knot=2000).fit(df[\"Gr_Liv_Area\"].values, df[\"SalePrice\"].values)\n\ngrid = np.linspace(df[\"Gr_Liv_Area\"].min(), df[\"Gr_Liv_Area\"].max(), 200)\npred = pw.predict(grid)\n\nplt.figure()\nplt.scatter(df[\"Gr_Liv_Area\"], df[\"SalePrice\"], s=8, alpha=0.2)\nplt.plot(grid, pred, linewidth=2)\nplt.axvline(2000, linestyle=\"--\")\nplt.title(\"Piecewise Linear Fit at Knot = 2000\")\nplt.xlabel(\"Gr_Liv_Area\"); plt.ylabel(\"SalePrice\")\nplt.tight_layout()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GAM— Piecewise, LOESS, and GAM Splines - Python Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-python.html#mars-commented-out-for-reference-only",
    "href": "generalized-additive-models-python.html#mars-commented-out-for-reference-only",
    "title": "5  GAM— Piecewise, LOESS, and GAM Splines - Python Version",
    "section": "5.4 4. MARS (Commented Out — For Reference Only)",
    "text": "5.4 4. MARS (Commented Out — For Reference Only)\n# The MARS implementation (py-earth) is not actively maintained and may fail to install on modern Python.\n# If you wish to try it, install from conda-forge:\n#     conda install -c conda-forge sklearn-contrib-py-earth\n#\n# Example (commented out):\n#\n# from pyearth import Earth\n#\n# mars = Pipeline([(\"prep\", pre), (\"model\", Earth(max_degree=2))]).fit(X_train, y_train)\n# print(mars.named_steps[\"model\"].summary())\n#\n# xs = np.linspace(X[\"Garage_Area\"].min(), X[\"Garage_Area\"].max(), 200)\n# yhat = mars.predict(pd.DataFrame({\"Garage_Area\": xs}))\n#\n# plt.figure()\n# plt.scatter(X_train[\"Garage_Area\"], y_train, s=8, alpha=0.2)\n# plt.plot(xs, yhat, linewidth=2)\n# plt.title(\"MARS with Garage_Area (Commented Out Example)\")\n# plt.xlabel(\"Garage_Area\"); plt.ylabel(\"SalePrice\")\n# plt.tight_layout()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GAM— Piecewise, LOESS, and GAM Splines - Python Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-python.html#loess-visualization",
    "href": "generalized-additive-models-python.html#loess-visualization",
    "title": "5  GAM— Piecewise, LOESS, and GAM Splines - Python Version",
    "section": "5.5 5. LOESS Visualization",
    "text": "5.5 5. LOESS Visualization\n\n\nCode\nfig = plt.figure()\nplt.scatter(df[\"Gr_Liv_Area\"], df[\"SalePrice\"], s=8, alpha=0.2)\nlow = sm.nonparametric.lowess(df[\"SalePrice\"], df[\"Gr_Liv_Area\"], frac=0.4, return_sorted=True)\nplt.plot(low[:,0], low[:,1], linewidth=2)\nplt.title(\"LOESS Smoother (Visual Aid)\")\nplt.xlabel(\"Gr_Liv_Area\"); plt.ylabel(\"SalePrice\")\nplt.tight_layout()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GAM— Piecewise, LOESS, and GAM Splines - Python Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-python.html#gam-and-spline-approaches-modern-alternatives",
    "href": "generalized-additive-models-python.html#gam-and-spline-approaches-modern-alternatives",
    "title": "5  GAM— Piecewise, LOESS, and GAM Splines - Python Version",
    "section": "5.6 6. GAM and Spline Approaches (Modern Alternatives)",
    "text": "5.6 6. GAM and Spline Approaches (Modern Alternatives)\n\n\nCode\nfrom pygam import LinearGAM, s\ngam1 = LinearGAM(s(0)).fit(X_train[[\"Garage_Area\"]].values, y_train)\nprint(gam1.summary())\ngam2 = LinearGAM(s(0) + s(1)).fit(X_train[[\"Garage_Area\",\"Gr_Liv_Area\"]].values, y_train)\nprint(gam2.summary())\nnum_train = X_train[num_cols].values\ngam3 = LinearGAM().fit(num_train, y_train)\nprint(gam3.summary())\n\n\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                     11.7399\nLink Function:                     IdentityLink Log Likelihood:                                -23263.1874\nNumber of Samples:                         1022 AIC:                                            46551.8545\n                                                AICc:                                           46552.2017\n                                                GCV:                                       3130152370.8794\n                                                Scale:                                     3065497547.8147\n                                                Pseudo R-Squared:                                   0.4883\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P &gt; x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                20           11.7         1.11e-16     ***         \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\nNone\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                     20.8593\nLink Function:                     IdentityLink Log Likelihood:                                -22770.8323\nNumber of Samples:                         1022 AIC:                                            45585.3832\n                                                AICc:                                           45586.3834\n                                                GCV:                                       1965630736.4285\n                                                Scale:                                      1893550067.068\n                                                Pseudo R-Squared:                                   0.6868\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P &gt; x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                20           12.6         1.11e-16     ***         \ns(1)                              [0.6]                20           8.3          1.11e-16     ***         \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\nNone\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                     89.0836\nLink Function:                     IdentityLink Log Likelihood:                                 -21995.395\nNumber of Samples:                         1022 AIC:                                            44170.9574\n                                                AICc:                                           44188.5855\n                                                GCV:                                       1050008766.0212\n                                                Scale:                                      886662002.9642\n                                                Pseudo R-Squared:                                   0.8633\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P &gt; x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                20           8.7          6.58e-07     ***         \ns(1)                              [0.6]                20           12.8         1.11e-16     ***         \ns(2)                              [0.6]                20           11.0         5.15e-01                 \ns(3)                              [0.6]                20           8.4          1.11e-16     ***         \ns(4)                              [0.6]                20           8.6          1.11e-16     ***         \ns(5)                              [0.6]                20           10.1         2.17e-03     **          \ns(6)                              [0.6]                20           3.2          9.76e-02     .           \ns(7)                              [0.6]                20           2.2          4.19e-01                 \ns(8)                              [0.6]                20           2.9          3.16e-10     ***         \ns(9)                              [0.6]                20           9.0          1.11e-16     ***         \ns(10)                             [0.6]                20           5.7          1.11e-16     ***         \ns(11)                             [0.6]                20           6.5          3.78e-01                 \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\nNone",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GAM— Piecewise, LOESS, and GAM Splines - Python Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-python.html#regression-spline-example-fallback",
    "href": "generalized-additive-models-python.html#regression-spline-example-fallback",
    "title": "5  GAM— Piecewise, LOESS, and GAM Splines - Python Version",
    "section": "5.7 7. Regression Spline Example (Fallback)",
    "text": "5.7 7. Regression Spline Example (Fallback)\n\n\nCode\nfrom patsy import dmatrix\n\ndesign_tr = dmatrix(\"bs(Garage_Area, df=6) + bs(Gr_Liv_Area, df=6)\", data=X_train, return_type=\"dataframe\")\ndesign_te = dmatrix(\"bs(Garage_Area, df=6) + bs(Gr_Liv_Area, df=6)\", data=X_test, return_type=\"dataframe\")\n\nridge = Ridge(alpha=1.0).fit(design_tr, y_train)\npred_spline = ridge.predict(design_te)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GAM— Piecewise, LOESS, and GAM Splines - Python Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-python.html#model-comparison-piecewise-vs-gam-vs-spline",
    "href": "generalized-additive-models-python.html#model-comparison-piecewise-vs-gam-vs-spline",
    "title": "5  GAM— Piecewise, LOESS, and GAM Splines - Python Version",
    "section": "5.8 8. Model Comparison (Piecewise vs GAM vs Spline)",
    "text": "5.8 8. Model Comparison (Piecewise vs GAM vs Spline)\n\n\nCode\ndef rmse(y, yhat):\n    try:\n        return mean_squared_error(y, yhat, squared=False)\n    except TypeError:\n        return np.sqrt(mean_squared_error(y, yhat))\n\ndef mse(y, yhat): return mean_squared_error(y, yhat)\ndef rsq(y, yhat): return r2_score(y, yhat)\n\np_piece = pw.predict(X_test[\"Gr_Liv_Area\"].values)\n\ntry:\n    p_gam = gam3.predict(X_test[num_cols].values)\nexcept Exception:\n    p_gam = np.full_like(y_test, np.nan, dtype=float)\n\np_spline = pred_spline\n\ncmp = pd.DataFrame({\n    \"Model\": [\"Piecewise\", \"GAM (pyGAM)\" if not np.isnan(p_gam).all() else \"GAM (not available)\", \"Regression Spline\"],\n    \"RMSE\": [rmse(y_test, p_piece), rmse(y_test, p_gam) if not np.isnan(p_gam).all() else np.nan, rmse(y_test, p_spline)],\n    \"MSE\": [mse(y_test, p_piece), mse(y_test, p_gam) if not np.isnan(p_gam).all() else np.nan, mse(y_test, p_spline)],\n    \"R2\": [rsq(y_test, p_piece), rsq(y_test, p_gam) if not np.isnan(p_gam).all() else np.nan, rsq(y_test, p_spline)]\n}).sort_values(\"R2\", ascending=False)\n\ncmp\n\n\n\n\n\n\n\n\n\nModel\nRMSE\nMSE\nR2\n\n\n\n\n1\nGAM (pyGAM)\n40805.867162\n1.665119e+09\n0.766556\n\n\n0\nPiecewise\n58108.537553\n3.376602e+09\n0.526612\n\n\n2\nRegression Spline\n60289.000524\n3.634764e+09\n0.490419",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GAM— Piecewise, LOESS, and GAM Splines - Python Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-python.html#predicted-vs-actual-comparison",
    "href": "generalized-additive-models-python.html#predicted-vs-actual-comparison",
    "title": "5  GAM— Piecewise, LOESS, and GAM Splines - Python Version",
    "section": "5.9 9. Predicted vs Actual Comparison",
    "text": "5.9 9. Predicted vs Actual Comparison\n\n\nCode\nplt.figure()\nplt.scatter(y_test, p_piece, s=8, alpha=0.4, label=\"Piecewise\")\nif not np.isnan(p_gam).all():\n    plt.scatter(y_test, p_gam, s=8, alpha=0.4, label=\"GAM\")\nplt.scatter(y_test, p_spline, s=8, alpha=0.4, label=\"Spline\")\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle=\"--\")\nplt.legend()\nplt.title(\"Predicted vs Actual (Test Set)\")\nplt.xlabel(\"Actual SalePrice\"); plt.ylabel(\"Predicted SalePrice\")\nplt.tight_layout()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GAM— Piecewise, LOESS, and GAM Splines - Python Version</span>"
    ]
  },
  {
    "objectID": "generalized-additive-models-python.html#interpretation-summary",
    "href": "generalized-additive-models-python.html#interpretation-summary",
    "title": "5  GAM— Piecewise, LOESS, and GAM Splines - Python Version",
    "section": "5.10 10. Interpretation Summary",
    "text": "5.10 10. Interpretation Summary\n\nPiecewise: adds simple thresholds, easy to explain.\n\nGAMs: provide smooth, interpretable nonlinearities.\n\nSplines: flexible approximations that behave like local polynomials.\n\nMARS (optional): similar conceptually but less maintained in Python; consider using R’s earth instead.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GAM— Piecewise, LOESS, and GAM Splines - Python Version</span>"
    ]
  }
]